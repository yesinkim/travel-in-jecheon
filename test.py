import sys
import types
import torch.nn as nn
import traceback

# ==============================================================================
# [ê¸´ê¸‰ íŒ¨ì¹˜] êµ¬ë²„ì „ PEFTì™€ ì‹ ë²„ì „ Transformers í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
# ==============================================================================
try:
    from transformers import modeling_layers
except ImportError:
    mock_module = types.ModuleType("transformers.modeling_layers")
    class MockGradientCheckpointingLayer(nn.Module):
        def __init__(self, *args, **kwargs):
            super().__init__()
            self.gradient_checkpointing = False 
    mock_module.GradientCheckpointingLayer = MockGradientCheckpointingLayer
    sys.modules["transformers.modeling_layers"] = mock_module
    print("ğŸ©¹ [ê¸´ê¸‰ íŒ¨ì¹˜ ì ìš©] transformers.modeling_layers ëª¨í‚¹ ì™„ë£Œ")

# ==============================================================================
# Imports
# ==============================================================================
import os
import json
import pandas as pd
import torch
from typing import List, Dict, TypedDict
from tqdm import tqdm
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
# LangChain ì„ë² ë”© import ìˆ˜ì •
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    
from langchain_huggingface import HuggingFacePipeline
from langchain_core.documents import Document
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoConfig
from langgraph.graph import StateGraph, END

# ì™¸ë¶€ í‰ê°€ ëª¨ë“ˆ Import
try:
    from src.evaluation.comprehensive_evaluator import ComprehensiveRAGEvaluator
    print("âœ… ì™¸ë¶€ í‰ê°€ ëª¨ë“ˆ(comprehensive_evaluator) ë¡œë“œ ì„±ê³µ")
except ImportError:
    try:
        from comprehensive_evaluator import ComprehensiveRAGEvaluator
        print("âœ… ì™¸ë¶€ í‰ê°€ ëª¨ë“ˆ(comprehensive_evaluator) ë¡œë“œ ì„±ê³µ (í˜„ì¬ ê²½ë¡œ)")
    except ImportError:
        print("âŒ ì˜¤ë¥˜: 'comprehensive_evaluator.py' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        # sys.exit(1) # ì—ëŸ¬ ë‚˜ë„ ì¼ë‹¨ ì§„í–‰í•˜ë„ë¡ ì£¼ì„ ì²˜ë¦¬

# ==============================================================================
# ì„¤ì • (Configuration)
# ==============================================================================
load_dotenv()
# ëª¨ë¸ ì´ë¦„ì´ ì •í™•í•´ì•¼ í•©ë‹ˆë‹¤.
SELECTED_MODEL = "Rag-jecheon"  

CONFIG = {
    "NAME": SELECTED_MODEL,
    "DOC_PATH": "data/chunks/documents.jsonl", # ê²½ë¡œ í™•ì¸ í•„ìˆ˜
    "TEST_PATH": "data/processed/test.jsonl",  # ê²½ë¡œ í™•ì¸ í•„ìˆ˜
    "VECTOR_DB_PATH": "data/faiss_index",
    "OUTPUT_CSV": f"{SELECTED_MODEL}_model_result.csv",
    "GOOGLE_API_KEY": os.getenv("GOOGLE_API_KEY"),
}

# ==============================================================================
# í•¨ìˆ˜ ì •ì˜
# ==============================================================================
def load_or_create_vectorstore():
    print("ğŸ”§ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embeddings = HuggingFaceEmbeddings(
        model_name="intfloat/multilingual-e5-large-instruct",
        model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )

    if os.path.exists(CONFIG["VECTOR_DB_PATH"]):
        print(f"ğŸ“‚ ì €ì¥ëœ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ: {CONFIG['VECTOR_DB_PATH']}")
        try:
            vectorstore = FAISS.load_local(CONFIG["VECTOR_DB_PATH"], embeddings, allow_dangerous_deserialization=True)
            return vectorstore
        except Exception as e:
            print(f"âš ï¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    print(f"ğŸ“„ ë¬¸ì„œ ë¡œë“œ ë° ìƒì„±: {CONFIG['DOC_PATH']}")
    documents = []
    if os.path.exists(CONFIG['DOC_PATH']):
        with open(CONFIG["DOC_PATH"], 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                doc = Document(
                    page_content=data['content'],
                    metadata={'doc_id': data['doc_id'], 'title': data['title']}
                )
                documents.append(doc)
        
        vectorstore = FAISS.from_documents(documents, embeddings)
        vectorstore.save_local(CONFIG["VECTOR_DB_PATH"])
        return vectorstore
    else:
        print("âŒ ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return None

# LangGraph State
class RAGState(TypedDict):
    question: str
    context: str
    answer: str

# ëª¨ë¸ ì„¤ì •
MODEL_CONFIGS = {
    "Gemma-2-9B": {
        "name": "google/gemma-2-9b-it",
        "dtype": torch.bfloat16,
    },
    "Kanana-1.5-8B": {
        "name": "kakaocorp/kanana-1.5-8b-instruct-2505",
        "dtype": torch.float16,
    },
    "Rag-jecheon": {
        "name": "bailando/kanana-jecheon",
        "dtype": torch.bfloat16, 
    }
}

def create_rag_app(vectorstore, model_key):
    # 1. LLM ë¡œë“œ
    config_dict = MODEL_CONFIGS.get(model_key, MODEL_CONFIGS["Kanana-1.5-8B"])
    model_name = config_dict['name']
    
    print(f"ğŸ¤– LLM ë¡œë“œ ì¤‘: {model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    if "model_type" in config_dict:
        print(f"âœ… [DEBUG] ëª¨ë¸ íƒ€ì…ì„ '{config_dict['model_type']}' (ìœ¼)ë¡œ ê°•ì œ ì„¤ì •í•©ë‹ˆë‹¤.")
        config = AutoConfig.from_pretrained(
        model_name,
        model_type=config_dict['model_type'] # <<< ì´ ë¼ì¸ì„ ì¶”ê°€/ìˆ˜ì •!
    )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            config=config, # ìˆ˜ì •ëœ config ê°ì²´ë¥¼ ì‚¬ìš©!
            torch_dtype=config_dict['dtype'],
            device_map="auto"
        )
    else:
        # 'model_type'ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
        print("âœ… [DEBUG] ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=config_dict['dtype'],
            device_map="auto"
        )
    # === ìˆ˜ì • ë¡œì§ ë ===

    print("âœ… LLM ë¡œë“œ ì„±ê³µ!")

    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        return_full_text=False,
        do_sample=True,
        temperature=0.1,
    )
    llm = HuggingFacePipeline(pipeline=pipe)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # 2. ë…¸ë“œ ì •ì˜ (ì´ ë¶€ë¶„ì´ í•¨ìˆ˜ ë°–ìœ¼ë¡œ ë‚˜ì˜¤ì§€ ì•Šê³  ë‚´ë¶€ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤)
    def retrieve_node(state: RAGState):
        docs = retriever.invoke(state["question"])
        return {"context": docs}

    def generate_node(state: RAGState):
        # [ì¤‘ìš”] stateì—ì„œ ë³€ìˆ˜ë¥¼ êº¼ë‚´ì•¼ NameErrorê°€ ì•ˆ ë‚©ë‹ˆë‹¤.
        question = state["question"]
        context = state["context"]
        
#         prompt = f"""ë‹¹ì‹ ì€ ì œì²œì‹œ ê´€ê´‘ ì•ˆë‚´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
# ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

# ë¬¸ì„œ ë‚´ìš©:
# {context}

# ì§ˆë¬¸: {question}

# ë‹µë³€:"""

        from langchain import hub

        prompt = hub.pull("rlm/rag-prompt")
        rag_chain = prompt | llm
        response = rag_chain.invoke({"question": question, "context": context})
        return {"answer": response}

    # 3. ê·¸ë˜í”„ ìƒì„±
    workflow = StateGraph(RAGState)
    workflow.add_node("retrieve", retrieve_node)
    workflow.add_node("generate", generate_node)
    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", END)
    
    # [ì¤‘ìš”] ì»´íŒŒì¼ëœ ì•±ì„ ë°˜ë“œì‹œ ë¦¬í„´í•´ì•¼ í•©ë‹ˆë‹¤!
    return workflow.compile()

# ==============================================================================
# ë©”ì¸ ì‹¤í–‰
# ==============================================================================
def main():
    print(f"ğŸš€ ì‹¤í–‰ ì‹œì‘ (Model: {SELECTED_MODEL})")
    
    # 1. VectorStore
    vectorstore = load_or_create_vectorstore()
    if vectorstore is None:
        print("âŒ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨")
        return

    # 2. Evaluator ì¤€ë¹„
    try:
        print("âš–ï¸ Evaluator ì´ˆê¸°í™” ì¤‘...")
        doc_map = {}
        if os.path.exists(CONFIG["DOC_PATH"]):
            with open(CONFIG["DOC_PATH"], 'r', encoding='utf-8') as f:
                for line in f:
                    d = json.loads(line)
                    doc_map[d['doc_id']] = d
        
        evaluator = ComprehensiveRAGEvaluator(gemini_model="gemini-2.5-pro")
    except Exception as e:
        print(f"âš ï¸ Evaluator ì´ˆê¸°í™” ì¤‘ ê²½ê³ : {e}")
        evaluator = None

    # 3. RAG App ìƒì„±
    app = create_rag_app(vectorstore, SELECTED_MODEL)
    if app is None:
        print("âŒ RAG ì•± ìƒì„± ì‹¤íŒ¨ (app is None)")
        return

    # 4. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    if not os.path.exists(CONFIG['TEST_PATH']):
        print(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì—†ìŒ: {CONFIG['TEST_PATH']}")
        return

    test_data = []
    with open(CONFIG['TEST_PATH'], 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                test_data.append(json.loads(line))
    
    print(f"ğŸ“Š ì´ {len(test_data)}ê°œ í•­ëª© í‰ê°€ ì‹œì‘...")
    results = []
    
    for idx, item in enumerate(tqdm(test_data)):
        question = item['question']
        ground_truth = item['answer']
        
        try:
            # A. ì‹¤í–‰
            output = app.invoke({"question": question})
            generated_answer = output["answer"]
            
            # B. ë¬¸ì„œ ì •ë¦¬
            retrieved_docs_dicts = []
            if "retrieved_docs" in output:
                retrieved_docs_dicts = [{"doc_id": d.metadata['doc_id'], "title": d.metadata['title']} for d in output["retrieved_docs"]]
            
            # C. í‰ê°€
            metrics = {}
            if evaluator:
                try:
                    metrics = evaluator.evaluate_single_response(
                        question=question,
                        response=generated_answer,
                        ground_truth=ground_truth,
                        retrieved_docs=retrieved_docs_dicts,
                        documents=doc_map
                    )
                except Exception as eval_e:
                    print(f" í‰ê°€ ì—ëŸ¬: {eval_e}")
            
            # D. ì €ì¥
            row = {
                "model_name": SELECTED_MODEL,
                "question": question,
                "ground_truth": ground_truth,
                "generated_answer": generated_answer,
                "retrieved_doc_ids": [d['doc_id'] for d in retrieved_docs_dicts],
                "rougeL": metrics.get("rougeL", 0),
                "bert_f1": metrics.get("bert_f1", 0),
                "judge_accuracy": metrics.get("accuracy"),
                "judge_helpfulness": metrics.get("helpfulness"),
                "judge_relevance": metrics.get("relevance"),
                "judge_depth": metrics.get("depth")
            }
            results.append(row)
            
        except Exception as e:
            print(f"\nâŒ Error at item {idx}: {e}")
            traceback.print_exc()
            continue

    if results:
        df = pd.DataFrame(results)
        df.to_csv(CONFIG["OUTPUT_CSV"], index=False, encoding='utf-8-sig')
        print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {CONFIG['OUTPUT_CSV']}")
        try:
            print(df[["rougeL", "bert_f1", "judge_helpfulness"]].mean())
        except:
            pass
    else:
        print("âš ï¸ ê²°ê³¼ ì—†ìŒ")

if __name__ == "__main__":
    main()