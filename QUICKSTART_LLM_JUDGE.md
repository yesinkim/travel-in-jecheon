# LLM-as-a-Judge ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## ğŸ¯ 3ë‹¨ê³„ë¡œ RAG í‰ê°€í•˜ê¸°

### 1ï¸âƒ£ API Key ì„¤ì • (í•œ ë²ˆë§Œ)

```bash
# .env íŒŒì¼ì— ì¶”ê°€
echo "GOOGLE_API_KEY=your-key-here" >> .env
```

### 2ï¸âƒ£ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì¤€ë¹„

**í•„ìš”í•œ ê²ƒ:**
- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ (ì§ˆë¬¸, ì •ë‹µ ë¬¸ì„œ, ì •ë‹µ í¬í•¨)
- ëª¨ë¸ ì¶”ë¡  ê²°ê³¼ (ìƒì„±ëœ ë‹µë³€)

```bash
# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ Judge í˜•ì‹ìœ¼ë¡œ ë³€í™˜
python scripts/prepare_predictions_for_judge.py \
  --dataset data/processed/test_data.jsonl \
  --predictions outputs/model_predictions.jsonl \
  --output outputs/for_judge.jsonl
```

### 3ï¸âƒ£ í‰ê°€ ì‹¤í–‰

```bash
# LLM Judgeë¡œ í‰ê°€
python scripts/evaluate_with_llm_judge.py \
  --predictions outputs/for_judge.jsonl \
  --output outputs/judge_results.jsonl
```

**ì¶œë ¥:**
```
============================================================
LLM-AS-A-JUDGE EVALUATION RESULTS (Bench-RAG)
============================================================

ğŸ“Š Aggregate Metrics:
  Accuracy Rate:     85.00%    â† í™˜ê° ì—†ëŠ” ë‹µë³€ ë¹„ìœ¨
  Avg Helpfulness:   8.50/10   â† ì–¼ë§ˆë‚˜ ë„ì›€ë˜ëŠ”ì§€
  Avg Relevance:     9.20/10   â† ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±
  Avg Depth:         7.80/10   â† ë‹µë³€ ìƒì„¸ë„

ğŸ“ˆ Evaluation Stats:
  Successfully Evaluated: 30
  Failed Evaluations:     0
============================================================
```

---

## ğŸ“Š Baseline vs Fine-tuned ë¹„êµ

```bash
# 1. Baseline í‰ê°€
python scripts/prepare_predictions_for_judge.py \
  --dataset data/processed/test_data.jsonl \
  --predictions outputs/baseline_predictions.jsonl \
  --output outputs/baseline_for_judge.jsonl

python scripts/evaluate_with_llm_judge.py \
  --predictions outputs/baseline_for_judge.jsonl \
  --output outputs/baseline_results.jsonl

# 2. Fine-tuned í‰ê°€
python scripts/prepare_predictions_for_judge.py \
  --dataset data/processed/test_data.jsonl \
  --predictions outputs/finetuned_predictions.jsonl \
  --output outputs/finetuned_for_judge.jsonl

python scripts/evaluate_with_llm_judge.py \
  --predictions outputs/finetuned_for_judge.jsonl \
  --output outputs/finetuned_results.jsonl
```

**ê²°ê³¼ ë¹„êµ:**
```python
import json

# Load results
with open('outputs/baseline_results_aggregate.json') as f:
    baseline = json.load(f)
with open('outputs/finetuned_results_aggregate.json') as f:
    finetuned = json.load(f)

# Print comparison
print(f"Accuracy:    {baseline['accuracy_rate']:.1%} â†’ {finetuned['accuracy_rate']:.1%}")
print(f"Helpfulness: {baseline['avg_helpfulness']:.1f} â†’ {finetuned['avg_helpfulness']:.1f}")
print(f"Relevance:   {baseline['avg_relevance']:.1f} â†’ {finetuned['avg_relevance']:.1f}")
print(f"Depth:       {baseline['avg_depth']:.1f} â†’ {finetuned['avg_depth']:.1f}")
```

---

## ğŸ’¡ ì£¼ìš” í¬ì¸íŠ¸

### âœ… ì¥ì 
- **í™˜ê° íƒì§€**: ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš© ì¶”ê°€í–ˆëŠ”ì§€ ìë™ìœ¼ë¡œ í™•ì¸
- **ì¸ê°„ í‰ê°€ì— ê°€ê¹Œì›€**: ROUGEë³´ë‹¤ ì‹¤ì œ í’ˆì§ˆì„ ì˜ ë°˜ì˜
- **ìƒì„¸í•œ ì„¤ëª…**: ì™œ ê·¸ëŸ° ì ìˆ˜ë¥¼ ë°›ì•˜ëŠ”ì§€ ì„¤ëª… ì œê³µ

### âš ï¸ ì£¼ì˜ì‚¬í•­
- **ë¹„ìš©**: ìƒ˜í”Œ 1ê°œë‹¹ 4ë²ˆ API í˜¸ì¶œ (í•˜ì§€ë§Œ gemini-flashëŠ” ê±°ì˜ ë¬´ë£Œ)
- **ì‹œê°„**: 30ê°œ í‰ê°€ = ì•½ 2-3ë¶„ ì†Œìš”
- **API Key í•„ìš”**: `.env`ì— `GOOGLE_API_KEY` ì„¤ì •

### ğŸ¯ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

**ê°œë°œ ì¤‘ (ë¹ ë¥¸ í™•ì¸):**
```bash
# ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
python scripts/evaluate_with_llm_judge.py \
  --predictions outputs/for_judge.jsonl \
  --output outputs/test.jsonl \
  --limit 5
```

**ìµœì¢… í‰ê°€ (ë¦¬í¬íŠ¸ìš©):**
```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€
python scripts/evaluate_with_llm_judge.py \
  --predictions outputs/for_judge.jsonl \
  --output outputs/final_results.jsonl
```

---

## ğŸ“„ ë°ì´í„° í˜•ì‹

### ì…ë ¥ (for_judge.jsonl)
```json
{
  "filename": "ì œì²œì‹œê´€ê´‘ì •ë³´ì±…ì.pdf",
  "content": "ì˜ë¦¼ì§€ëŠ” ì œì²œì‹œ ì†¡í•™ë©´ ì˜ë¦¼ëŒ€ë¡œ 47ê¸¸ 7ì— ìœ„ì¹˜...",
  "question": "ì˜ë¦¼ì§€ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?",
  "response": "ì œì²œì‹œ ì†¡í•™ë©´ ì˜ë¦¼ëŒ€ë¡œ 47ê¸¸ 7ì— ìœ„ì¹˜í•©ë‹ˆë‹¤."
}
```

### ì¶œë ¥ (judge_results.jsonl)
```json
{
  "filename": "ì œì²œì‹œê´€ê´‘ì •ë³´ì±…ì.pdf",
  "question": "ì˜ë¦¼ì§€ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?",
  "response": "ì œì²œì‹œ ì†¡í•™ë©´ ì˜ë¦¼ëŒ€ë¡œ 47ê¸¸ 7ì— ìœ„ì¹˜í•©ë‹ˆë‹¤.",
  "accuracy": true,
  "accuracy_explanation": "ë¬¸ì„œ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•¨",
  "helpfulness": 9,
  "helpfulness_explanation": "ì •í™•í•œ ì£¼ì†Œë¥¼ ì œê³µí•˜ì—¬ ë§¤ìš° ë„ì›€ë¨",
  "relevance": 10,
  "relevance_explanation": "ì§ˆë¬¸ì— ì™„ë²½í•˜ê²Œ ë‹µë³€í•¨",
  "depth": 7,
  "depth_explanation": "í•„ìš”í•œ ì •ë³´ë¥¼ ê°„ê²°í•˜ê²Œ ì œê³µ"
}
```

---

## ğŸš¨ Troubleshooting

**ë¬¸ì œ: API Key ì—ëŸ¬**
```
ValueError: GOOGLE_API_KEY environment variable not set
```
â†’ `.env` íŒŒì¼ì— `GOOGLE_API_KEY=your-key` ì¶”ê°€

**ë¬¸ì œ: Rate Limit**
```
429 Resource Exhausted
```
â†’ `--limit 10` ì˜µì…˜ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°

**ë¬¸ì œ: JSON íŒŒì‹± ì—ëŸ¬**
â†’ ë” ì•ˆì •ì ì¸ ëª¨ë¸ ì‚¬ìš©: `--model gemini-1.5-flash`

---

**ìƒì„¸ ê°€ì´ë“œ:** `docs/LLM_JUDGE_GUIDE.md` ì°¸ê³ 
**ì‘ì„±ì¼:** 2025-11-18
