{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kanana RAG Fine-tuning Notebook\n",
    "\n",
    "This notebook fine-tunes the Kanana 8B instruct model on RAG tasks using the Jecheon tourism dataset.\n",
    "\n",
    "**Model:** kakaocorp/kanana-1.5-8b-instruct-2505\n",
    "\n",
    "**Training Data Format:**\n",
    "```\n",
    "[Instruction]\n",
    "당신은 제천시 관광 안내 전문가입니다.\n",
    "제공된 여러 문서 중에서 질문과 관련된 문서를 찾아, 그 문서의 내용을 바탕으로 정확하고 친절하게 답변해주세요.\n",
    "\n",
    "Information:\n",
    "{content1}\n",
    "Information:\n",
    "{content2}\n",
    "Question: {question}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.23.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (7.1.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu128)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting click>=8.0.1 (from wandb)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting pydantic<3 (from wandb)\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.45.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3->wandb)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3->wandb)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Downloading wandb-0.23.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m129.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m140.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.45.0-py2.py3-none-any.whl (404 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, typing-inspection, smmap, sentry-sdk, pydantic-core, pyarrow, protobuf, propcache, multidict, frozenlist, dill, click, annotated-types, aiohappyeyeballs, yarl, pydantic, pandas, multiprocess, gitdb, aiosignal, gitpython, aiohttp, wandb, bitsandbytes, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/27\u001b[0m [datasets]/27\u001b[0m [datasets]tes]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 bitsandbytes-0.48.2 click-8.3.1 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 gitdb-4.0.12 gitpython-3.1.45 multidict-6.7.0 multiprocess-0.70.18 pandas-2.3.3 propcache-0.4.1 protobuf-6.33.1 pyarrow-22.0.0 pydantic-2.12.4 pydantic-core-2.41.5 pytz-2025.2 sentry-sdk-2.45.0 smmap-5.0.2 typing-inspection-0.4.2 tzdata-2025.2 wandb-0.23.0 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install transformers peft datasets wandb bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA version: 12.8\n",
      "cuDNN version: 91002\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"NVIDIA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myesinkim\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/goodganglabs/notebooks/wandb/run-20251119_050719-f5cjgqp2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yesinkim/kanana-rag-finetuning/runs/f5cjgqp2' target=\"_blank\">kanana-1.5-8b-instruct-rag</a></strong> to <a href='https://wandb.ai/yesinkim/kanana-rag-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yesinkim/kanana-rag-finetuning' target=\"_blank\">https://wandb.ai/yesinkim/kanana-rag-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yesinkim/kanana-rag-finetuning/runs/f5cjgqp2' target=\"_blank\">https://wandb.ai/yesinkim/kanana-rag-finetuning/runs/f5cjgqp2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/yesinkim/kanana-rag-finetuning/runs/f5cjgqp2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7d78271b0a10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login to Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Login to wandb (you'll be prompted for your API key)\n",
    "wandb.login()\n",
    "\n",
    "# Initialize wandb project\n",
    "wandb.init(\n",
    "    project=\"kanana-rag-finetuning\",\n",
    "    name=\"kanana-1.5-8b-instruct-rag\",\n",
    "    config={\n",
    "        \"model\": \"kakaocorp/kanana-1.5-8b-instruct-2505\",\n",
    "        \"task\": \"RAG fine-tuning\",\n",
    "        \"dataset\": \"Jecheon Tourism\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kakaocorp/kanana-1.5-8b-instruct-2505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2011d111b71a45a9b7f647ce742c8d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load base model and tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"kakaocorp/kanana-1.5-8b-instruct-2505\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 105 training examples\n",
      "Loaded 10 validation examples\n",
      "\n",
      "First training example:\n",
      "{\n",
      "  \"documents\": [\n",
      "    {\n",
      "      \"doc_id\": \"doc_002\",\n",
      "      \"title\": \"제천 시티투어\",\n",
      "      \"filename\": \"doc_002_제천 시티투어.txt\",\n",
      "      \"content\": \"**어디를 가야할 지 잘 모르시겠다면?**\\n시티투어에 몸을 맡기기만 해도 힐링 완성!\\n\\n-   **편리한 버스투어**\\n-   **당일치기 교통약자**\\n-   **문화관광 해설사 동행해설**\\n-   **가성비 & 자율중식**\\n\\n| 상품소개 | 제천의 주요 관광지를 모아 합리적인 가격에 둘러보는 단체 관광형 상품 |\\n| :--- | :--- |\\n| **요금안내** | 30,000원 \\\\| 1인 기준 (최소 출발 인원 10명) |\\n| **예약안내** | 제천시티투어 공식 홈페이지 [citytour.jecheon.go.kr] / (사)제천시관광협의회 [043. 647. 2121] |\\n| **주요혜택** | · **시티투어버스 대\n"
     ]
    }
   ],
   "source": [
    "# Load RAG training and validation data\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "data_path = \"../data/processed/train.jsonl\"\n",
    "val_data_path = \"../data/processed/val.jsonl\"\n",
    "\n",
    "# Load training JSONL data\n",
    "data_list = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data_list.append(json.loads(line))\n",
    "\n",
    "# Load validation JSONL data\n",
    "val_data_list = []\n",
    "with open(val_data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        val_data_list.append(json.loads(line))\n",
    "        \n",
    "\n",
    "print(f\"Loaded {len(data_list)} training examples\")\n",
    "print(f\"Loaded {len(val_data_list)} validation examples\")\n",
    "print(\"\\nFirst training example:\")\n",
    "print(json.dumps(data_list[0], ensure_ascii=False, indent=2)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted 105 training examples\n",
      "Formatted 10 validation examples\n",
      "\n",
      "Example formatted prompt:\n",
      "당신은 제천시 관광 안내 전문가입니다.\n",
      "제공된 여러 문서 중에서 질문과 관련된 문서를 찾아, 그 문서의 내용을 바탕으로 정확하고 친절하게 답변해주세요.\n",
      "\n",
      "답변 시 주의사항:\n",
      "1. 관련 문서의 내용만을 바탕으로 답변하세요\n",
      "2. 문서에 정보가 없으면 \"제공된 정보에는 해당 내용이 없습니다\"라고 답변하세요\n",
      "3. 추측하거나 문서 외부 지식을 사용하지 마세요\n",
      "4. 간결하고 이해하기 쉽게 답변하세요\n",
      "\n",
      "Information:\n",
      "**어디를 가야할 지 잘 모르시겠다면?**\n",
      "시티투어에 몸을 맡기기만 해도 힐링 완성!\n",
      "\n",
      "-   **편리한 버스투어**\n",
      "-   **당일치기 교통약자**\n",
      "-   **문화관광 해설사 동행해설**\n",
      "-   **가성비 & 자율중식**\n",
      "\n",
      "| 상품소개 | 제천의 주요 관광지를 모아 합리적인 가격에 둘러보는 단체 관광형 상품 |\n",
      "| :--- | :--- |\n",
      "| **요금안내** | 30,000원 \\| 1인 기준 (최소 출발 인원 10명) |\n",
      "| **예약안내** | 제천시티투어 공식 홈페이지 \n",
      "\n",
      "Answer: 개별 여행은 자유롭게 일정을 짤 수 있는 반면, 제천 시티투어는 정해진 주요 관광지 코스(청풍호반 케이블카, 옥순봉 출렁다리 등)를 편리한 버스로 이동하며 문화관광 해설사의 해설을 들을 수 있다는 차이점이 있습니다. 또한 최소 10명 이상일 때 출발하는 단체 관광 상품입니다.\n"
     ]
    }
   ],
   "source": [
    "# Convert to requested RAG format with instruction\n",
    "def format_rag_data(example):\n",
    "    \"\"\"Convert RAG data to the requested format with instruction:\n",
    "    [Instruction]\n",
    "    당신은 제천시 관광 안내 전문가입니다...\n",
    "    \n",
    "    Information:\n",
    "    {content1}\n",
    "    Information:\n",
    "    {content2}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    instruction = \"\"\"당신은 제천시 관광 안내 전문가입니다.\n",
    "제공된 여러 문서 중에서 질문과 관련된 문서를 찾아, 그 문서의 내용을 바탕으로 정확하고 친절하게 답변해주세요.\n",
    "\n",
    "답변 시 주의사항:\n",
    "1. 관련 문서의 내용만을 바탕으로 답변하세요\n",
    "2. 문서에 정보가 없으면 \"제공된 정보에는 해당 내용이 없습니다\"라고 답변하세요\n",
    "3. 추측하거나 문서 외부 지식을 사용하지 마세요\n",
    "4. 간결하고 이해하기 쉽게 답변하세요\"\"\"\n",
    "    \n",
    "    documents = example['documents']\n",
    "    question = example['question']\n",
    "    answer = example['answer']\n",
    "    \n",
    "    # Build information sections\n",
    "    info_sections = []\n",
    "    for doc in documents:\n",
    "        info_sections.append(f\"Information:\\n{doc['content']}\")\n",
    "    \n",
    "    # Combine: instruction + information sections + question\n",
    "    prompt = instruction + \"\\n\\n\"\n",
    "    prompt += \"\\n\\n\".join(info_sections)\n",
    "    prompt += f\"\\n\\nQuestion: {question}\"\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "\n",
    "# Apply formatting to both train and validation\n",
    "formatted_data = []\n",
    "for example in data_list:\n",
    "    formatted_data.append(format_rag_data(example))\n",
    "\n",
    "formatted_val_data = []\n",
    "for example in val_data_list:\n",
    "    formatted_val_data.append(format_rag_data(example))\n",
    "\n",
    "print(f\"Formatted {len(formatted_data)} training examples\")\n",
    "print(f\"Formatted {len(formatted_val_data)} validation examples\")\n",
    "print(\"\\nExample formatted prompt:\")\n",
    "print(formatted_data[0]['prompt'][:500])\n",
    "print(f\"\\nAnswer: {formatted_data[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a259df1362d2408f9c5c7e3d248557c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327743a708174018b7acbb73d429d975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 105\n",
      "Validation dataset size: 10\n",
      "\n",
      "Dataset features: {'prompt': Value('string'), 'answer': Value('string'), 'text': Value('string')}\n",
      "\n",
      "First training example:\n",
      "당신은 제천시 관광 안내 전문가입니다.\n",
      "제공된 여러 문서 중에서 질문과 관련된 문서를 찾아, 그 문서의 내용을 바탕으로 정확하고 친절하게 답변해주세요.\n",
      "\n",
      "답변 시 주의사항:\n",
      "1. 관련 문서의 내용만을 바탕으로 답변하세요\n",
      "2. 문서에 정보가 없으면 \"제공된 정보에는 해당 내용이 없습니다\"라고 답변하세요\n",
      "3. 추측하거나 문서 외부 지식을 사용하지 마세요\n",
      "4. 간결하고 이해하기 쉽게 답변하세요\n",
      "\n",
      "Information:\n",
      "**어디를 가야할 지 잘 모르시겠다면?**\n",
      "시티투어에 몸을 맡기기만 해도 힐링 완성!\n",
      "\n",
      "-   **편리한 버스투어**\n",
      "-   **당일치기 교통약자**\n",
      "-   **문화관광 해설사 동행해설**\n",
      "-   **가성비 & 자율중식**\n",
      "\n",
      "| 상품소개 | 제천의 주요 관광지를 모아 합리적인 가격에 둘러보는 단체 관광형 상품 |\n",
      "| :--- | :--- |\n",
      "| **요금안내** | 30,000원 \\| 1인 기준 (최소 출발 인원 10명) |\n",
      "| **예약안내** | 제천시티투어 공식 홈페이지 \n"
     ]
    }
   ],
   "source": [
    "# Create training and validation datasets with proper format\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format prompts for training\"\"\"\n",
    "    prompts = examples[\"prompt\"]\n",
    "    answers = examples[\"answer\"]\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    texts = []\n",
    "    for prompt, answer in zip(prompts, answers):\n",
    "        # Combine prompt and answer with EOS token\n",
    "        text = f\"{prompt}\\n\\nAnswer: {answer}{EOS_TOKEN}\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Create datasets\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "val_dataset = Dataset.from_list(formatted_val_data)\n",
    "val_dataset = val_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"Training dataset size: {len(dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"\\nDataset features: {dataset.features}\")\n",
    "print(f\"\\nFirst training example:\")\n",
    "print(dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f2312cab9c4675a7edb27257210337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a43547cf5ee4db8a8af61fcb921e406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training dataset size: 105\n",
      "Tokenized validation dataset size: 10\n",
      "Features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64'))}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # 전체 텍스트와 프롬프트(질문까지)를 각각 준비\n",
    "    prompts = [p + \"\\n\\nAnswer: \" for p in examples[\"prompt\"]]\n",
    "    texts = examples[\"text\"] # Cell 8에서 이미 Answer까지 합쳐진 텍스트\n",
    "    \n",
    "    # 전체 텍스트 토크나이징\n",
    "    model_inputs = tokenizer(\n",
    "        texts, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=2048, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    labels = model_inputs[\"input_ids\"].clone()\n",
    "    \n",
    "    # 패딩 토큰 마스킹 (-100)\n",
    "    if tokenizer.pad_token_id is not None:\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        \n",
    "    # --- 여기가 핵심 수정 부분입니다 ---\n",
    "    # Prompt(질문) 부분의 길이를 구해서 그 부분만큼 labels를 -100으로 가림\n",
    "    \n",
    "    # 1. Prompt 부분만 따로 토크나이징하여 길이 계산 (패딩 없이)\n",
    "    prompt_tokens = tokenizer(\n",
    "        prompts, \n",
    "        truncation=True, \n",
    "        max_length=2048, \n",
    "        add_special_tokens=False # 앞부분만 잴 것이므로\n",
    "    )\n",
    "    \n",
    "    for i, prompt_ids in enumerate(prompt_tokens[\"input_ids\"]):\n",
    "        # 전체 길이보다 프롬프트가 길면 잘리기 때문에 min 처리\n",
    "        mask_len = len(prompt_ids)\n",
    "        if mask_len > labels.shape[1]:\n",
    "            mask_len = labels.shape[1]\n",
    "            \n",
    "        # 정답(Answer)이 나오기 전까지는 Loss 계산 안함\n",
    "        labels[i, :mask_len] = -100\n",
    "        \n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# dataset.map 부분은 그대로 두되, prompt 컬럼이 필요하므로 remove_columns 주의\n",
    "# 아래 코드로 map 실행\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"text\", \"prompt\", \"answer\"] # 처리 후 삭제는 OK\n",
    ")\n",
    "\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\", \"prompt\", \"answer\"]\n",
    ")\n",
    "\n",
    "print(f\"Tokenized training dataset size: {len(tokenized_dataset)}\")\n",
    "print(f\"Tokenized validation dataset size: {len(tokenized_val_dataset)}\")\n",
    "print(f\"Features: {tokenized_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 8,037,101,568 || trainable%: 0.0848\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA for parameter-efficient fine-tuning\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,                    # LoRA rank\n",
    "    lora_alpha=16,          # LoRA alpha (2024 best practice: alpha = 2 × rank)\n",
    "    lora_dropout=0.15,      # Dropout probability (increased for small dataset)\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\"            # Added output projection for better performance\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Log LoRA config to wandb\n",
    "wandb.config.update({\n",
    "    \"lora_r\": lora_config.r,\n",
    "    \"lora_alpha\": lora_config.lora_alpha,\n",
    "    \"lora_dropout\": lora_config.lora_dropout,\n",
    "    \"target_modules\": lora_config.target_modules\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-SXM4-80GB. Max memory = 79.254 GB.\n",
      "14.994 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "# Log to wandb\n",
    "wandb.config.update({\n",
    "    \"gpu_name\": gpu_stats.name,\n",
    "    \"gpu_max_memory_gb\": max_memory,\n",
    "    \"gpu_start_memory_gb\": start_gpu_memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured (2024 Best Practices):\n",
      "  Epochs: 5 (with early stopping)\n",
      "  Batch size: 2\n",
      "  Gradient accumulation: 4\n",
      "  Effective batch size: 8\n",
      "  Learning rate: 5e-05\n",
      "  Warmup steps: 50\n",
      "  Gradient clipping: 1.0\n",
      "  Early stopping: enabled (metric: eval_loss)\n",
      "  Evaluation strategy: IntervalStrategy.STEPS\n",
      "  Total steps: ~65\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output settings\n",
    "    output_dir=\"./outputs/kanana-rag\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # [중요] 메모리 절약을 위해 추가\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=50,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Optimization\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "\n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"wandb\",\n",
    "\n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    # [핵심 수정] evaluation_strategy -> eval_strategy 로 변경\n",
    "    eval_strategy=\"steps\", \n",
    "    \n",
    "    eval_steps=5,\n",
    "    per_device_eval_batch_size=2,\n",
    "    \n",
    "    # [중요] Early Stopping 설정 (eval_strategy와 짝꿍)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "\n",
    "    # Other\n",
    "    seed=1234,\n",
    "    data_seed=1234,\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Training arguments configured (2024 Best Practices):\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs} (with early stopping)\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"  Gradient clipping: {training_args.max_grad_norm}\")\n",
    "print(f\"  Early stopping: enabled (metric: {training_args.metric_for_best_model})\")\n",
    "print(f\"  Evaluation strategy: {training_args.eval_strategy}\")\n",
    "print(f\"  Total steps: ~{len(tokenized_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized successfully!\n",
      "Training samples: 105\n",
      "Validation samples: 10\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer with validation dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,  # Add validation dataset\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"Training samples: {len(tokenized_dataset)}\")\n",
    "print(f\"Validation samples: {len(tokenized_val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Checkpointing 호환성 설정 완료!\n"
     ]
    }
   ],
   "source": [
    "model.enable_input_require_grads()\n",
    "\n",
    "# 혹시 모르니 다시 한 번 명시적으로 켜줍니다\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"Gradient Checkpointing 호환성 설정 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 07:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.626700</td>\n",
       "      <td>1.406286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.615800</td>\n",
       "      <td>1.400312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.686100</td>\n",
       "      <td>1.390589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.581900</td>\n",
       "      <td>1.378505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.624500</td>\n",
       "      <td>1.360354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.676300</td>\n",
       "      <td>1.338566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.588600</td>\n",
       "      <td>1.313651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.428100</td>\n",
       "      <td>1.281587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.447800</td>\n",
       "      <td>1.247121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.386200</td>\n",
       "      <td>1.204997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.408300</td>\n",
       "      <td>1.165189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.298400</td>\n",
       "      <td>1.135070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.278600</td>\n",
       "      <td>1.111996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.208100</td>\n",
       "      <td>1.103130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training completed!\n",
      "Training loss: 1.4897\n",
      "Training time: 426.27s\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 개수: 105\n",
      "예상 전체 스텝 수: 65\n"
     ]
    }
   ],
   "source": [
    "# 이 코드를 실행해서 데이터 개수를 확인해보세요\n",
    "print(f\"학습 데이터 개수: {len(tokenized_dataset)}\")\n",
    "print(f\"예상 전체 스텝 수: {len(tokenized_dataset) // 8 * 5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final GPU memory: 23.742 GB\n",
      "Peak memory used: 8.748000000000001 GB\n"
     ]
    }
   ],
   "source": [
    "# Check final GPU memory usage\n",
    "final_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"\\nFinal GPU memory: {final_gpu_memory} GB\")\n",
    "print(f\"Peak memory used: {final_gpu_memory - start_gpu_memory} GB\")\n",
    "\n",
    "# Log final stats to wandb\n",
    "wandb.log({\n",
    "    \"final_gpu_memory_gb\": final_gpu_memory,\n",
    "    \"peak_memory_used_gb\": final_gpu_memory - start_gpu_memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./outputs/kanana-rag-final...\n",
      "Model saved successfully!\n",
      "\n",
      "Model location: ./outputs/kanana-rag-final\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./outputs/kanana-rag-final\"\n",
    "\n",
    "print(f\"Saving model to {output_dir}...\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"\\nModel location: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing inference...\n",
      "\n",
      "Test Prompt:\n",
      "당신은 제천시 관광 안내 전문가입니다.\n",
      "제공된 여러 문서 중에서 질문과 관련된 문서를 찾아, 그 문서의 내용을 바탕으로 정확하고 친절하게 답변해주세요.\n",
      "\n",
      "답변 시 주의사항:\n",
      "1. 관련 문서의 내용만을 바탕으로 답변하세요\n",
      "2. 문서에 정보가 없으면 \"제공된 정보에는 해당 내용이 없습니다\"라고 답변하세요\n",
      "3. 추측하거나 문서 외부 지식을 사용하지 마세요\n",
      "4. 간결하고 이해하기 쉽게 답변하세요\n",
      "\n",
      "Information:\n",
      "**어디를 가야할 지 잘 모르시겠다면?**\n",
      "시티투어에 몸을 맡기기만 해도 힐링 완성!\n",
      "\n",
      "-   **편리한 버스투어**\n",
      "-\n",
      "\n",
      "...\n",
      "\n",
      "Generated Response:\n",
      " 그리고 어떤 경우에 시티투어를 이용하는 것이 더 적합한가요?\n",
      "\n",
      "Answer: 제천에서 개별적으로 여행을 할 경우에는 직접 교통편을 찾고, 관광지에 대한 정보를 직접 알아야 합니다. 반면에 시티투어를 이용하면 버스를 타고 여러 관광지를 둘러볼 수 있어 시간과 노력을 절약할 수 있습니다. 또한 문화관광 해설사의 동행 해설이 제공되어 관광지에 대한 정보를 더 자세히 얻을 수 있습니다. 시티투어는 당일치기로 제천을 여행할 계획이 있고, 특별히 교통편을 직접 알아보는 것이 번거롭거나 관광지에 대한 해설이 필요한 경우에 더 적합합니다. 또한 10명 이상의 단체가 함께 여행할 계획이 있다면 시티투어를 이용하는 것이 경제적으로도 더 이득일 수 있습니다. 하지만 소규모\n",
      "\n",
      "Expected Answer:\n",
      "개별 여행은 자유롭게 일정을 짤 수 있는 반면, 제천 시티투어는 정해진 주요 관광지 코스(청풍호반 케이블카, 옥순봉 출렁다리 등)를 편리한 버스로 이동하며 문화관광 해설사의 해설을 들을 수 있다는 차이점이 있습니다. 또한 최소 10명 이상일 때 출발하는 단체 관광 상품입니다.\n"
     ]
    }
   ],
   "source": [
    "# Test inference with a sample\n",
    "print(\"Testing inference...\\n\")\n",
    "\n",
    "# Get a test example\n",
    "test_prompt = formatted_data[0]['prompt']\n",
    "expected_answer = formatted_data[0]['answer']\n",
    "\n",
    "print(\"Test Prompt:\")\n",
    "print(test_prompt[:300])\n",
    "print(\"\\n...\")\n",
    "\n",
    "# Tokenize and generate\n",
    "model.eval()\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(generated_text[len(test_prompt):])\n",
    "\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(expected_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Generation-Only Metrics\n",
    "\n",
    "Now let's evaluate the fine-tuned model using generation-only metrics (ROUGE, BERTScore, Exact Match).\n",
    "\n",
    "**No document retrieval metrics needed** - we only compare generated answers vs ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Install evaluation dependencies\n",
    "!pip install -q rouge-score bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluator initialized\n",
      "  Available metrics: ROUGE, BERTScore, Exact Match\n"
     ]
    }
   ],
   "source": [
    "# Import evaluation metrics\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from src.evaluation.metrics import create_evaluator\n",
    "\n",
    "# Create evaluator (no k_values needed for generation-only)\n",
    "evaluator = create_evaluator()\n",
    "\n",
    "print(\"✓ Evaluator initialized\")\n",
    "print(\"  Available metrics: ROUGE, BERTScore, Exact Match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on test set...\n",
      "============================================================\n",
      "  Generated 10/30 predictions...\n",
      "  Generated 20/30 predictions...\n",
      "  Generated 30/30 predictions...\n",
      "\n",
      "✓ Generated 30 predictions\n",
      "\n",
      "============================================================\n",
      "Example prediction:\n",
      "============================================================\n",
      "Question: 개별적으로 제천을 여행하는 것과 시티투어를 이용하는 것의 차이점은 무엇인가요?\n",
      "\n",
      "Generated: 그리고 시티투어를 이용할 때 권장되는 여행 일정은 어떤가요?\n",
      "\n",
      "Answer: 제천 시티투어는 단체 관광객을 위해 합리적인 가격에 주요 관광지를 둘러볼 수 있게 하는 상품입니다. 개별적으로 여행하는 것은 목적에 따라 자유롭게 일정을 계획할 수 있다는 장점이 있습니다. 시티투어는 교통약자에게도 편리하게 제천을 구경할 수 있는 기회를 주며, 문화관광 해설사를 동...\n",
      "\n",
      "Reference: 개별 여행은 자유롭게 일정을 짤 수 있는 반면, 제천 시티투어는 정해진 주요 관광지 코스(청풍호반 케이블카, 옥순봉 출렁다리 등)를 편리한 버스로 이동하며 문화관광 해설사의 해설을 들을 수 있다는 차이점이 있습니다. 또한 최소 10명 이상일 때 출발하는 단체 관광 상품입니다....\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on test set\n",
    "print(\"Generating predictions on test set...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_samples = []\n",
    "with open(\"../data/processed/val.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        test_samples.append(json.loads(line))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, sample in enumerate(test_samples):\n",
    "        # Format prompt\n",
    "        formatted = format_rag_data(sample)\n",
    "        prompt = formatted['prompt']\n",
    "        \n",
    "        # Generate answer\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Extract generated text\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = generated[len(prompt):].strip()\n",
    "        \n",
    "        # Remove \"Answer:\" prefix if present\n",
    "        if answer.startswith(\"Answer:\"):\n",
    "            answer = answer[7:].strip()\n",
    "        \n",
    "        predictions.append({\n",
    "            'answer': answer\n",
    "        })\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Generated {i + 1}/{len(test_samples)} predictions...\")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(predictions)} predictions\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example prediction:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Question: {test_samples[0]['question']}\")\n",
    "print(f\"\\nGenerated: {predictions[0]['answer'][:200]}...\")\n",
    "print(f\"\\nReference: {test_samples[0]['answer'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run generation-only evaluation (no docs needed!)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Running generation-only evaluation...\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Evaluate - only needs 'answer' field in predictions and dataset\n",
    "results = evaluator.evaluate_generation_only(\n",
    "    dataset=test_samples,\n",
    "    model_predictions=predictions\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(evaluator.format_results_generation_only(results))\n",
    "\n",
    "# Calculate train and validation loss\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Calculating train and validation loss...\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Get train loss\n",
    "train_loss = trainer.evaluate(tokenized_dataset)\n",
    "print(f\"Train Loss: {train_loss['eval_loss']:.4f}\")\n",
    "\n",
    "# Get validation loss\n",
    "val_loss = trainer.evaluate(tokenized_val_dataset)\n",
    "print(f\"Validation Loss: {val_loss['eval_loss']:.4f}\")\n",
    "\n",
    "# Log all metrics to wandb\n",
    "wandb.log({\n",
    "    \"eval/rouge1\": results['rouge1'],\n",
    "    \"eval/rouge2\": results['rouge2'],\n",
    "    \"eval/rougeL\": results['rougeL'],\n",
    "    \"eval/bert_f1\": results['bert_f1'],\n",
    "    \"eval/bert_precision\": results['bert_precision'],\n",
    "    \"eval/bert_recall\": results['bert_recall'],\n",
    "    \"eval/exact_match\": results['exact_match'],\n",
    "    \"eval/num_samples\": results['num_samples'],\n",
    "    \"eval/train_loss\": train_loss['eval_loss'],\n",
    "    \"eval/val_loss\": val_loss['eval_loss']\n",
    "})\n",
    "\n",
    "print(\"\\n✓ Results logged to WandB\")\n",
    "print(f\"\\nMetrics Summary:\")\n",
    "print(f\"  Train Loss: {train_loss['eval_loss']:.4f}\")\n",
    "print(f\"  Validation Loss: {val_loss['eval_loss']:.4f}\")\n",
    "print(f\"  ROUGE-L: {results['rougeL']:.4f}\")\n",
    "print(f\"  BERTScore F1: {results['bert_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed examples table for wandb\n",
    "examples_data = []\n",
    "\n",
    "for i in range(min(5, len(test_samples))):\n",
    "    examples_data.append([\n",
    "        test_samples[i]['question'],\n",
    "        test_samples[i]['answer'],\n",
    "        predictions[i]['answer'],\n",
    "        test_samples[i].get('question_type', 'unknown')\n",
    "    ])\n",
    "\n",
    "examples_table = wandb.Table(\n",
    "    columns=[\"Question\", \"Reference Answer\", \"Generated Answer\", \"Question Type\"],\n",
    "    data=examples_data\n",
    ")\n",
    "\n",
    "wandb.log({\"eval/detailed_examples\": examples_table})\n",
    "\n",
    "print(\"✓ Example predictions logged to WandB\")\n",
    "print(\"\\nView your results at: https://wandb.ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close wandb run\n",
    "wandb.finish()\n",
    "print(\"WandB run finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. ✅ Loads Kanana 1.5 8B instruct model (kakaocorp/kanana-1.5-8b-instruct-2505)\n",
    "2. ✅ Formats RAG training data with instruction and information format:\n",
    "   ```\n",
    "   [Instruction: 제천시 관광 안내 전문가 역할]\n",
    "   \n",
    "   Information:\n",
    "   {content1}\n",
    "   Information:\n",
    "   {content2}\n",
    "   Question: {question}\n",
    "   \n",
    "   Answer: {answer}\n",
    "   ```\n",
    "3. ✅ Applies LoRA for efficient fine-tuning\n",
    "4. ✅ Tracks training with Weights & Biases\n",
    "5. ✅ Saves the fine-tuned model\n",
    "6. ✅ Tests inference on sample data\n",
    "\n",
    "### Key Features:\n",
    "- System instruction guides the model to be a Jecheon tourism expert\n",
    "- Model learns to find relevant documents and answer based on them\n",
    "- Model learns to say \"no information available\" when appropriate\n",
    "- Training format prevents hallucination\n",
    "\n",
    "### Next Steps:\n",
    "- Run full evaluation on test set\n",
    "- Compare baseline vs fine-tuned performance\n",
    "- Upload model to Hugging Face Hub\n",
    "- Generate report with metrics and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# 1. 허깅페이스 로그인 (토큰 필요)\n",
    "# Write 권한이 있는 토큰을 입력하세요: https://huggingface.co/settings/tokens\n",
    "login()\n",
    "\n",
    "# 2. 모델과 토크나이저 업로드\n",
    "# \"본인계정명/원하는모델이름\" 형식으로 적어주세요\n",
    "repo_id = \"bailando/kanana-rag-jecheon-v1\" \n",
    "\n",
    "print(f\"Uploading to {repo_id}...\")\n",
    "\n",
    "# Trainer에 있는 모델을 바로 업로드\n",
    "trainer.push_to_hub(repo_id)\n",
    "tokenizer.push_to_hub(repo_id)\n",
    "\n",
    "print(\"업로드 완료! Hugging Face에서 확인하세요.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
