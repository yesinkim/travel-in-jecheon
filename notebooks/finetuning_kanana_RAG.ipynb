{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Kanana RAG Fine-tuning Notebook\n\nThis notebook fine-tunes the Kanana 8B instruct model on RAG tasks using the Jecheon tourism dataset.\n\n**Model:** kakaocorp/kanana-1.5-8b-instruct-2505\n\n**Training Data Format:**\n```\n[Instruction]\n당신은 제천시 관광 안내 전문가입니다.\n제공된 여러 문서 중에서 질문과 관련된 문서를 찾아, 그 문서의 내용을 바탕으로 정확하고 친절하게 답변해주세요.\n\nInformation:\n{content1}\nInformation:\n{content2}\nQuestion: {question}\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install transformers peft datasets wandb bitsandbytes accelerate bert-score"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"NVIDIA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to Weights & Biases\nimport wandb\n\n# Login to wandb (you'll be prompted for your API key)\nwandb.login()\n\n# Initialize wandb project\nwandb.init(\n    project=\"kanana-rag-finetuning\",\n    name=\"kanana-1.5-8b-instruct-rag\",\n    config={\n        \"model\": \"kakaocorp/kanana-1.5-8b-instruct-2505\",\n        \"task\": \"RAG fine-tuning\",\n        \"dataset\": \"Jecheon Tourism\"\n    }\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load base model and tokenizer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"kakaocorp/kanana-1.5-8b-instruct-2505\"\n\nprint(f\"Loading model: {model_name}\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\"Model loaded successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAG training data\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "data_path = \"/home/user/goodganglabs/data/processed/training_data.jsonl\"\n",
    "\n",
    "# Load JSONL data\n",
    "data_list = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data_list.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data_list)} training examples\")\n",
    "print(\"\\nFirst example:\")\n",
    "print(json.dumps(data_list[0], ensure_ascii=False, indent=2)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert to requested RAG format with instruction\ndef format_rag_data(example):\n    \"\"\"Convert RAG data to the requested format with instruction:\n    [Instruction]\n    당신은 제천시 관광 안내 전문가입니다...\n    \n    Information:\n    {content1}\n    Information:\n    {content2}\n    Question: {question}\n    \"\"\"\n    instruction = \"\"\"당신은 제천시 관광 안내 전문가입니다.\n제공된 여러 문서 중에서 질문과 관련된 문서를 찾아, 그 문서의 내용을 바탕으로 정확하고 친절하게 답변해주세요.\n\n답변 시 주의사항:\n1. 관련 문서의 내용만을 바탕으로 답변하세요\n2. 문서에 정보가 없으면 \"제공된 정보에는 해당 내용이 없습니다\"라고 답변하세요\n3. 추측하거나 문서 외부 지식을 사용하지 마세요\n4. 간결하고 이해하기 쉽게 답변하세요\"\"\"\n    \n    documents = example['documents']\n    question = example['question']\n    answer = example['answer']\n    \n    # Build information sections\n    info_sections = []\n    for doc in documents:\n        info_sections.append(f\"Information:\\n{doc['content']}\")\n    \n    # Combine: instruction + information sections + question\n    prompt = instruction + \"\\n\\n\"\n    prompt += \"\\n\\n\".join(info_sections)\n    prompt += f\"\\n\\nQuestion: {question}\"\n    \n    return {\n        \"prompt\": prompt,\n        \"answer\": answer\n    }\n\n# Apply formatting\nformatted_data = []\nfor example in data_list:\n    formatted_data.append(format_rag_data(example))\n\nprint(f\"Formatted {len(formatted_data)} examples\")\nprint(\"\\nExample formatted prompt:\")\nprint(formatted_data[0]['prompt'][:500])\nprint(f\"\\nAnswer: {formatted_data[0]['answer']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset with proper format\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format prompts for training\"\"\"\n",
    "    prompts = examples[\"prompt\"]\n",
    "    answers = examples[\"answer\"]\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    texts = []\n",
    "    for prompt, answer in zip(prompts, answers):\n",
    "        # Combine prompt and answer with EOS token\n",
    "        text = f\"{prompt}\\n\\nAnswer: {answer}{EOS_TOKEN}\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"\\nDataset features: {dataset.features}\")\n",
    "print(f\"\\nFirst training example:\")\n",
    "print(dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"text\", \"prompt\", \"answer\"]\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset size: {len(tokenized_dataset)}\")\n",
    "print(f\"Features: {tokenized_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Split dataset into train and validation\nfrom datasets import DatasetDict\n\n# Split dataset (80% train, 20% validation)\ndataset_split = dataset.train_test_split(test_size=0.2, seed=1234)\n\ntrain_dataset = dataset_split['train']\nval_dataset = dataset_split['test']\n\nprint(f\"Train size: {len(train_dataset)}\")\nprint(f\"Validation size: {len(val_dataset)}\")\n\n# Log to wandb\nwandb.config.update({\n    \"train_size\": len(train_dataset),\n    \"val_size\": len(val_dataset),\n    \"train_val_split\": \"80/20\"\n})",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Define compute_metrics function with BERTScore for validation\nfrom bert_score import score\nimport numpy as np\n\ndef compute_metrics(eval_preds):\n    \"\"\"\n    Compute BERTScore during validation.\n    \n    This function is called automatically by the Trainer during validation.\n    It computes BERTScore (Precision, Recall, F1) between generated and reference answers.\n    \n    Args:\n        eval_preds: EvalPrediction object with predictions and label_ids\n    \n    Returns:\n        Dict of metric names and values (logged to WandB automatically)\n    \"\"\"\n    predictions, labels = eval_preds\n    \n    # Decode predictions and references\n    # Remove padding tokens (-100) from labels\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    # Decode to text\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"Answer:\")\n    def extract_answer(text):\n        if \"Answer:\" in text:\n            return text.split(\"Answer:\")[-1].strip()\n        return text.strip()\n    \n    decoded_preds = [extract_answer(pred) for pred in decoded_preds]\n    decoded_labels = [extract_answer(label) for label in decoded_labels]\n    \n    # Compute BERTScore (Korean language)\n    P, R, F1 = score(\n        decoded_preds, \n        decoded_labels, \n        lang=\"ko\",  # Korean language\n        model_type=\"bert-base-multilingual-cased\",\n        verbose=False\n    )\n    \n    # Return metrics (will be prefixed with \"eval_\" automatically)\n    # These will appear in WandB as: eval_bert_precision, eval_bert_recall, eval_bert_f1\n    return {\n        \"bert_precision\": P.mean().item(),\n        \"bert_recall\": R.mean().item(),\n        \"bert_f1\": F1.mean().item(),\n    }\n\nprint(\"✓ compute_metrics function defined\")\nprint(\"  Metrics: BERTScore (Precision, Recall, F1)\")\nprint(\"  Language: Korean (ko)\")\nprint(\"  Model: bert-base-multilingual-cased\")\nprint(\"\\n  During validation:\")\nprint(\"    - eval_loss (automatic)\")\nprint(\"    - eval_bert_precision\")\nprint(\"    - eval_bert_recall\")\nprint(\"    - eval_bert_f1 ← used for best model selection\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Tokenize both train and validation datasets\nprint(\"Tokenizing train and validation datasets...\")\nprint(\"=\" * 60)\n\n# Tokenize training set\ntokenized_train = train_dataset.map(\n    tokenize_function, \n    batched=True, \n    remove_columns=[\"text\", \"prompt\", \"answer\"]\n)\n\n# Tokenize validation set\ntokenized_val = val_dataset.map(\n    tokenize_function, \n    batched=True, \n    remove_columns=[\"text\", \"prompt\", \"answer\"]\n)\n\nprint(f\"✓ Tokenized train dataset: {len(tokenized_train)} samples\")\nprint(f\"✓ Tokenized validation dataset: {len(tokenized_val)} samples\")\nprint(f\"\\nTrain features: {tokenized_train.features}\")\nprint(f\"Val features: {tokenized_val.features}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for parameter-efficient fine-tuning\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,                    # LoRA rank\n",
    "    lora_alpha=32,          # LoRA alpha\n",
    "    lora_dropout=0.1,       # Dropout probability\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Log LoRA config to wandb\n",
    "wandb.config.update({\n",
    "    \"lora_r\": lora_config.r,\n",
    "    \"lora_alpha\": lora_config.lora_alpha,\n",
    "    \"lora_dropout\": lora_config.lora_dropout,\n",
    "    \"target_modules\": lora_config.target_modules\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure training arguments with validation and BERTScore\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    # Output settings\n    output_dir=\"./outputs/kanana-rag\",\n    overwrite_output_dir=True,\n    \n    # Training hyperparameters\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,  # Added for validation\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    warmup_steps=10,\n    \n    # Optimization\n    bf16=True,\n    optim=\"adamw_torch\",\n    lr_scheduler_type=\"linear\",\n    \n    # Logging\n    logging_steps=5,\n    logging_dir=\"./logs\",\n    report_to=\"wandb\",\n    \n    # Saving with best model selection\n    save_strategy=\"steps\",\n    save_steps=50,\n    save_total_limit=3,\n    load_best_model_at_end=True,  # Added: Load best model at end\n    \n    # Validation - Option 3: BERTScore for best model, loss also monitored\n    evaluation_strategy=\"steps\",  # Changed from \"no\" to \"steps\"\n    eval_steps=50,  # Evaluate every 50 steps\n    metric_for_best_model=\"eval_bert_f1\",  # Use BERTScore F1 for best model\n    greater_is_better=True,  # Higher F1 is better\n    \n    # Other\n    seed=1234,\n    data_seed=1234,\n    remove_unused_columns=True,\n)\n\nprint(\"Training arguments configured with validation:\")\nprint(\"=\" * 60)\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Train batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Eval batch size: {training_args.per_device_eval_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\nprint(f\"\\n  Validation settings:\")\nprint(f\"    Strategy: {training_args.evaluation_strategy}\")\nprint(f\"    Eval steps: {training_args.eval_steps}\")\nprint(f\"    Metric for best model: {training_args.metric_for_best_model}\")\nprint(f\"    Load best at end: {training_args.load_best_model_at_end}\")\nprint(f\"\\n  Metrics tracked:\")\nprint(f\"    - eval_loss (automatic, cross-entropy)\")\nprint(f\"    - eval_bert_precision\")\nprint(f\"    - eval_bert_recall\")\nprint(f\"    - eval_bert_f1 ← Best model selection criterion\")\nprint(\"=\" * 60)\n\n# Log training config to wandb\nwandb.config.update({\n    \"num_train_epochs\": training_args.num_train_epochs,\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"per_device_eval_batch_size\": training_args.per_device_eval_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"weight_decay\": training_args.weight_decay,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"evaluation_strategy\": training_args.evaluation_strategy,\n    \"eval_steps\": training_args.eval_steps,\n    \"metric_for_best_model\": training_args.metric_for_best_model,\n})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Trainer with validation dataset and compute_metrics\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,  # Changed from tokenized_dataset\n    eval_dataset=tokenized_val,      # Added validation dataset\n    compute_metrics=compute_metrics,  # Added BERTScore computation\n)\n\nprint(\"Trainer initialized with validation!\")\nprint(\"=\" * 60)\nprint(f\"  Train dataset: {len(tokenized_train)} samples\")\nprint(f\"  Validation dataset: {len(tokenized_val)} samples\")\nprint(f\"  Compute metrics: BERTScore enabled\")\nprint(f\"\\n  During training:\")\nprint(f\"    - Every {training_args.eval_steps} steps:\")\nprint(f\"      → Run validation on {len(tokenized_val)} samples\")\nprint(f\"      → Compute eval_loss\")\nprint(f\"      → Compute BERTScore metrics\")\nprint(f\"      → Save checkpoint if eval_bert_f1 improved\")\nprint(f\"    - At end: Load best checkpoint (highest eval_bert_f1)\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final GPU memory usage\n",
    "final_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"\\nFinal GPU memory: {final_gpu_memory} GB\")\n",
    "print(f\"Peak memory used: {final_gpu_memory - start_gpu_memory} GB\")\n",
    "\n",
    "# Log final stats to wandb\n",
    "wandb.log({\n",
    "    \"final_gpu_memory_gb\": final_gpu_memory,\n",
    "    \"peak_memory_used_gb\": final_gpu_memory - start_gpu_memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./outputs/kanana-rag-final\"\n",
    "\n",
    "print(f\"Saving model to {output_dir}...\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"\\nModel location: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with a sample\n",
    "print(\"Testing inference...\\n\")\n",
    "\n",
    "# Get a test example\n",
    "test_prompt = formatted_data[0]['prompt']\n",
    "expected_answer = formatted_data[0]['answer']\n",
    "\n",
    "print(\"Test Prompt:\")\n",
    "print(test_prompt[:300])\n",
    "print(\"\\n...\")\n",
    "\n",
    "# Tokenize and generate\n",
    "model.eval()\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(generated_text[len(test_prompt):])\n",
    "\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(expected_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation with Generation-Only Metrics\n\nNow let's evaluate the fine-tuned model using generation-only metrics (ROUGE, BERTScore, Exact Match).\n\n**No document retrieval metrics needed** - we only compare generated answers vs ground truth.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Install evaluation dependencies\n!pip install -q rouge-score bert-score",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Import evaluation metrics\nimport sys\nsys.path.insert(0, '/home/user/goodganglabs')\n\nfrom src.evaluation.metrics import create_evaluator\n\n# Create evaluator (no k_values needed for generation-only)\nevaluator = create_evaluator()\n\nprint(\"✓ Evaluator initialized\")\nprint(\"  Available metrics: ROUGE, BERTScore, Exact Match\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Generate predictions on test set\nprint(\"Generating predictions on test set...\")\nprint(\"=\" * 60)\n\n# Use first 30 samples for evaluation (or load separate test set)\ntest_samples = data_list[:30]  \npredictions = []\n\nmodel.eval()\nwith torch.no_grad():\n    for i, sample in enumerate(test_samples):\n        # Format prompt\n        formatted = format_rag_data(sample)\n        prompt = formatted['prompt']\n        \n        # Generate answer\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        \n        # Extract generated text\n        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        answer = generated[len(prompt):].strip()\n        \n        # Remove \"Answer:\" prefix if present\n        if answer.startswith(\"Answer:\"):\n            answer = answer[7:].strip()\n        \n        predictions.append({\n            'answer': answer\n        })\n        \n        if (i + 1) % 10 == 0:\n            print(f\"  Generated {i + 1}/{len(test_samples)} predictions...\")\n\nprint(f\"\\n✓ Generated {len(predictions)} predictions\")\n\n# Show example\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Example prediction:\")\nprint(\"=\" * 60)\nprint(f\"Question: {test_samples[0]['question']}\")\nprint(f\"\\nGenerated: {predictions[0]['answer'][:200]}...\")\nprint(f\"\\nReference: {test_samples[0]['answer'][:200]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Run generation-only evaluation (no docs needed!)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Running generation-only evaluation...\")\nprint(\"=\" * 60 + \"\\n\")\n\n# Evaluate - only needs 'answer' field in predictions and dataset\nresults = evaluator.evaluate_generation_only(\n    dataset=test_samples,\n    model_predictions=predictions\n)\n\n# Display results\nprint(evaluator.format_results_generation_only(results))\n\n# Log to wandb\nwandb.log({\n    \"eval/rouge1\": results['rouge1'],\n    \"eval/rouge2\": results['rouge2'],\n    \"eval/rougeL\": results['rougeL'],\n    \"eval/bert_f1\": results['bert_f1'],\n    \"eval/bert_precision\": results['bert_precision'],\n    \"eval/bert_recall\": results['bert_recall'],\n    \"eval/exact_match\": results['exact_match'],\n    \"eval/num_samples\": results['num_samples']\n})\n\nprint(\"\\n✓ Results logged to WandB\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create detailed examples table for wandb\nexamples_data = []\n\nfor i in range(min(5, len(test_samples))):\n    examples_data.append([\n        test_samples[i]['question'],\n        test_samples[i]['answer'],\n        predictions[i]['answer'],\n        test_samples[i].get('question_type', 'unknown')\n    ])\n\nexamples_table = wandb.Table(\n    columns=[\"Question\", \"Reference Answer\", \"Generated Answer\", \"Question Type\"],\n    data=examples_data\n)\n\nwandb.log({\"eval/detailed_examples\": examples_table})\n\nprint(\"✓ Example predictions logged to WandB\")\nprint(\"\\nView your results at: https://wandb.ai\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "## Summary\n\nThis notebook fine-tunes Kanana 1.5 8B on RAG tasks with **validation and BERTScore tracking**.\n\n### What this notebook does:\n\n1. ✅ **Data Preparation**\n   - Loads Kanana 1.5 8B instruct model (kakaocorp/kanana-1.5-8b-instruct-2505)\n   - Formats RAG training data with instruction format\n   - **Splits dataset: 80% train, 20% validation**\n\n2. ✅ **Training Configuration**\n   - Applies LoRA for efficient fine-tuning (r=8, alpha=32)\n   - **Validation every 50 steps**\n   - **Tracks both eval_loss AND BERTScore metrics**\n\n3. ✅ **Best Model Selection (Option 3)**\n   - **Primary criterion: `eval_bert_f1` (BERTScore F1)**\n   - Also monitors: `eval_loss`, `eval_bert_precision`, `eval_bert_recall`\n   - Automatically loads best checkpoint at end of training\n\n4. ✅ **Experiment Tracking**\n   - Weights & Biases integration\n   - All metrics logged automatically during training\n   - Model checkpoints saved when BERTScore improves\n\n5. ✅ **Evaluation**\n   - Post-training evaluation with ROUGE, BERTScore, Exact Match\n   - Comparison examples logged to WandB\n\n### Training Data Format:\n```\n[Instruction: 제천시 관광 안내 전문가 역할]\n\nInformation:\n{content1}\nInformation:\n{content2}\nQuestion: {question}\n\nAnswer: {answer}\n```\n\n### Key Features:\n- **System instruction** guides the model to be a Jecheon tourism expert\n- **Multi-document format** teaches model to find relevant info\n- **Validation with BERTScore** ensures quality during training\n- **Best model selection** based on semantic similarity (BERTScore F1)\n- **Loss also tracked** for analysis in WandB\n\n### Metrics Tracked During Training:\n| Metric | Description | Purpose |\n|--------|-------------|---------|\n| `eval_loss` | Cross-entropy loss | General training health |\n| `eval_bert_precision` | BERTScore precision | Answer accuracy |\n| `eval_bert_recall` | BERTScore recall | Answer completeness |\n| `eval_bert_f1` | **BERTScore F1** | **Best model selection** ⭐ |\n\n### Why Option 3 (BERTScore for best model)?\n- **BERTScore F1** directly measures answer quality (semantic similarity)\n- **eval_loss** can be low but answers poor (overfitting to form, not content)\n- **Both metrics visible** in WandB for comprehensive analysis\n- Best of both worlds: quality-based selection + loss monitoring\n\n### Next Steps:\n- Run full evaluation on test set\n- Compare baseline vs fine-tuned performance\n- Upload model to Hugging Face Hub\n- Generate report with metrics and examples",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook:\n1. ✅ Loads Kanana 1.5 8B instruct model (kakaocorp/kanana-1.5-8b-instruct-2505)\n2. ✅ Formats RAG training data with instruction and information format:\n   ```\n   [Instruction: 제천시 관광 안내 전문가 역할]\n   \n   Information:\n   {content1}\n   Information:\n   {content2}\n   Question: {question}\n   \n   Answer: {answer}\n   ```\n3. ✅ Applies LoRA for efficient fine-tuning\n4. ✅ Tracks training with Weights & Biases\n5. ✅ Saves the fine-tuned model\n6. ✅ Tests inference on sample data\n\n### Key Features:\n- System instruction guides the model to be a Jecheon tourism expert\n- Model learns to find relevant documents and answer based on them\n- Model learns to say \"no information available\" when appropriate\n- Training format prevents hallucination\n\n### Next Steps:\n- Run full evaluation on test set\n- Compare baseline vs fine-tuned performance\n- Upload model to Hugging Face Hub\n- Generate report with metrics and examples"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}