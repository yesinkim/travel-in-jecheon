{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Kanana RAG Fine-tuning Notebook\n\nThis notebook fine-tunes the Kanana 8B instruct model on RAG tasks using the Jecheon tourism dataset.\n\n**Model:** kakaocorp/kanana-1.5-8b-instruct-2505\n\n**Training Data Format:**\n```\n[Instruction]\n당신은 제천시 관광 안내 전문가입니다.\n제공된 여러 문서 중에서 질문과 관련된 문서를 찾아, 그 문서의 내용을 바탕으로 정확하고 친절하게 답변해주세요.\n\nInformation:\n{content1}\nInformation:\n{content2}\nQuestion: {question}\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers peft datasets wandb bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"NVIDIA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to Weights & Biases\nimport wandb\n\n# Login to wandb (you'll be prompted for your API key)\nwandb.login()\n\n# Initialize wandb project\nwandb.init(\n    project=\"kanana-rag-finetuning\",\n    name=\"kanana-1.5-8b-instruct-rag\",\n    config={\n        \"model\": \"kakaocorp/kanana-1.5-8b-instruct-2505\",\n        \"task\": \"RAG fine-tuning\",\n        \"dataset\": \"Jecheon Tourism\"\n    }\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load base model and tokenizer with QLoRA (4bit quantization)\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel_name = \"kakaocorp/kanana-1.5-8b-instruct-2505\"\n\n# Configure 4bit quantization for QLoRA\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                      # Enable 4bit loading\n    bnb_4bit_use_double_quant=True,         # Double quantization for better compression\n    bnb_4bit_quant_type=\"nf4\",              # NF4 quantization type (best for fine-tuning)\n    bnb_4bit_compute_dtype=torch.bfloat16   # Compute in bfloat16 for stability\n)\n\nprint(f\"Loading model with QLoRA: {model_name}\")\nprint(f\"Quantization: 4bit NF4 with double quantization\")\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,  # Apply 4bit quantization\n    trust_remote_code=True,\n    device_map=\"auto\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\"Model loaded successfully with QLoRA!\")\nprint(f\"Model dtype: {base_model.dtype}\")\nprint(f\"Model device: {base_model.device}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAG training data\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "data_path = \"/home/user/goodganglabs/data/processed/training_data.jsonl\"\n",
    "\n",
    "# Load JSONL data\n",
    "data_list = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data_list.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data_list)} training examples\")\n",
    "print(\"\\nFirst example:\")\n",
    "print(json.dumps(data_list[0], ensure_ascii=False, indent=2)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert to requested RAG format with instruction\ndef format_rag_data(example):\n    \"\"\"Convert RAG data to the requested format with instruction:\n    [Instruction]\n    당신은 제천시 관광 안내 전문가입니다...\n    \n    Information:\n    {content1}\n    Information:\n    {content2}\n    Question: {question}\n    \"\"\"\n    instruction = \"\"\"당신은 제천시 관광 안내 전문가입니다.\n제공된 여러 문서 중에서 질문과 관련된 문서를 찾아, 그 문서의 내용을 바탕으로 정확하고 친절하게 답변해주세요.\n\n답변 시 주의사항:\n1. 관련 문서의 내용만을 바탕으로 답변하세요\n2. 문서에 정보가 없으면 \"제공된 정보에는 해당 내용이 없습니다\"라고 답변하세요\n3. 추측하거나 문서 외부 지식을 사용하지 마세요\n4. 간결하고 이해하기 쉽게 답변하세요\"\"\"\n    \n    documents = example['documents']\n    question = example['question']\n    answer = example['answer']\n    \n    # Build information sections\n    info_sections = []\n    for doc in documents:\n        info_sections.append(f\"Information:\\n{doc['content']}\")\n    \n    # Combine: instruction + information sections + question\n    prompt = instruction + \"\\n\\n\"\n    prompt += \"\\n\\n\".join(info_sections)\n    prompt += f\"\\n\\nQuestion: {question}\"\n    \n    return {\n        \"prompt\": prompt,\n        \"answer\": answer\n    }\n\n# Apply formatting\nformatted_data = []\nfor example in data_list:\n    formatted_data.append(format_rag_data(example))\n\nprint(f\"Formatted {len(formatted_data)} examples\")\nprint(\"\\nExample formatted prompt:\")\nprint(formatted_data[0]['prompt'][:500])\nprint(f\"\\nAnswer: {formatted_data[0]['answer']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset with proper format\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format prompts for training\"\"\"\n",
    "    prompts = examples[\"prompt\"]\n",
    "    answers = examples[\"answer\"]\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    texts = []\n",
    "    for prompt, answer in zip(prompts, answers):\n",
    "        # Combine prompt and answer with EOS token\n",
    "        text = f\"{prompt}\\n\\nAnswer: {answer}{EOS_TOKEN}\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"\\nDataset features: {dataset.features}\")\n",
    "print(f\"\\nFirst training example:\")\n",
    "print(dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tokenize dataset with reduced max_length for memory efficiency\ndef tokenize_function(examples):\n    tokens = tokenizer(\n        examples[\"text\"], \n        padding=\"max_length\",\n        truncation=True,\n        max_length=1024,  # Reduced from 2048 to save memory\n        return_tensors=\"pt\"\n    )\n    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n    return tokens\n\ntokenized_dataset = dataset.map(\n    tokenize_function, \n    batched=True, \n    remove_columns=[\"text\", \"prompt\", \"answer\"]\n)\n\nprint(f\"Tokenized dataset size: {len(tokenized_dataset)}\")\nprint(f\"Features: {tokenized_dataset.features}\")\nprint(f\"Max sequence length: 1024 tokens\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure LoRA for parameter-efficient fine-tuning with QLoRA optimizations\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# Prepare model for k-bit training (QLoRA requirement)\nprint(\"Preparing model for k-bit training...\")\nbase_model = prepare_model_for_kbit_training(base_model)\n\n# Enable gradient checkpointing to save memory\nbase_model.gradient_checkpointing_enable()\nprint(\"Gradient checkpointing enabled\")\n\n# Configure LoRA\nlora_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",\n    r=16,                    # LoRA rank (increased from 8 for better performance)\n    lora_alpha=32,          # LoRA alpha (scaling factor)\n    lora_dropout=0.1,       # Dropout probability\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\"           # Added o_proj for better coverage\n    ],\n    bias=\"none\"\n)\n\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()\n\n# Log LoRA config to wandb\nwandb.config.update({\n    \"quantization\": \"4bit NF4\",\n    \"lora_r\": lora_config.r,\n    \"lora_alpha\": lora_config.lora_alpha,\n    \"lora_dropout\": lora_config.lora_dropout,\n    \"target_modules\": lora_config.target_modules,\n    \"gradient_checkpointing\": True\n})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "# Log to wandb\n",
    "wandb.config.update({\n",
    "    \"gpu_name\": gpu_stats.name,\n",
    "    \"gpu_max_memory_gb\": max_memory,\n",
    "    \"gpu_start_memory_gb\": start_gpu_memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure training arguments with memory-efficient settings\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    # Output settings\n    output_dir=\"./outputs/kanana-rag\",\n    overwrite_output_dir=True,\n    \n    # Training hyperparameters\n    num_train_epochs=3,\n    per_device_train_batch_size=2,      # Can reduce to 1 if still OOM\n    gradient_accumulation_steps=4,      # Effective batch size = 2*4 = 8\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    warmup_steps=10,\n    max_grad_norm=0.3,                  # Gradient clipping for stability\n    \n    # Memory optimization\n    gradient_checkpointing=True,        # Critical for memory savings\n    \n    # Optimization\n    bf16=True,\n    optim=\"paged_adamw_8bit\",          # Changed to 8bit optimizer for QLoRA\n    lr_scheduler_type=\"linear\",\n    \n    # Logging\n    logging_steps=5,\n    logging_dir=\"./logs\",\n    report_to=\"wandb\",  # Enable wandb reporting\n    \n    # Saving\n    save_strategy=\"steps\",\n    save_steps=50,\n    save_total_limit=3,\n    \n    # Evaluation\n    evaluation_strategy=\"no\",  # No validation set in this example\n    \n    # Other\n    seed=1234,\n    data_seed=1234,\n    remove_unused_columns=True,\n)\n\nprint(\"Training arguments configured:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\nprint(f\"  Optimizer: {training_args.optim}\")\nprint(f\"  Gradient checkpointing: {training_args.gradient_checkpointing}\")\nprint(f\"  Total steps: ~{len(tokenized_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final GPU memory usage\n",
    "final_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"\\nFinal GPU memory: {final_gpu_memory} GB\")\n",
    "print(f\"Peak memory used: {final_gpu_memory - start_gpu_memory} GB\")\n",
    "\n",
    "# Log final stats to wandb\n",
    "wandb.log({\n",
    "    \"final_gpu_memory_gb\": final_gpu_memory,\n",
    "    \"peak_memory_used_gb\": final_gpu_memory - start_gpu_memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./outputs/kanana-rag-final\"\n",
    "\n",
    "print(f\"Saving model to {output_dir}...\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"\\nModel location: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with a sample\n",
    "print(\"Testing inference...\\n\")\n",
    "\n",
    "# Get a test example\n",
    "test_prompt = formatted_data[0]['prompt']\n",
    "expected_answer = formatted_data[0]['answer']\n",
    "\n",
    "print(\"Test Prompt:\")\n",
    "print(test_prompt[:300])\n",
    "print(\"\\n...\")\n",
    "\n",
    "# Tokenize and generate\n",
    "model.eval()\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(generated_text[len(test_prompt):])\n",
    "\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(expected_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close wandb run\n",
    "wandb.finish()\n",
    "print(\"WandB run finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook implements **QLoRA (Quantized LoRA)** fine-tuning for memory efficiency:\n\n### Key Features:\n\n1. ✅ **QLoRA (4bit quantization)**\n   - NF4 quantization type\n   - Double quantization enabled\n   - Compute in bfloat16 for stability\n   - **Memory savings: ~75% compared to full precision**\n\n2. ✅ **Memory optimization techniques**\n   - Gradient checkpointing enabled\n   - Paged AdamW 8bit optimizer\n   - Reduced max sequence length (1024 tokens)\n   - Gradient clipping (max_grad_norm=0.3)\n\n3. ✅ **LoRA configuration**\n   - Rank: 16\n   - Alpha: 32\n   - Target modules: q_proj, k_proj, v_proj, o_proj\n   - Dropout: 0.1\n\n4. ✅ **Training format**\n   ```\n   [Instruction: 제천시 관광 안내 전문가 역할]\n   \n   Information:\n   {content1}\n   Information:\n   {content2}\n   Question: {question}\n   \n   Answer: {answer}\n   ```\n\n### Expected Memory Usage (A100 40GB):\n\n- Model (4bit): ~2-3 GB\n- Optimizer states: ~1-2 GB\n- Gradients: ~1-2 GB\n- Activations (with checkpointing): ~3-5 GB\n- **Total: ~8-12 GB** (well within 40GB limit)\n\n### Changes from Original:\n\n| Setting | Before | After | Impact |\n|---------|--------|-------|--------|\n| Quantization | bfloat16 | 4bit NF4 | -75% memory |\n| Max length | 2048 | 1024 | -50% activation memory |\n| Optimizer | adamw_torch | paged_adamw_8bit | -50% optimizer memory |\n| Gradient checkpoint | ❌ | ✅ | -40% activation memory |\n| LoRA rank | 8 | 16 | +better performance |\n| Target modules | 3 | 4 | +better coverage |\n\n### Why This Should Work Now:\n\n**Before (caused OOM):**\n- 8B model × 2 bytes (bf16) = 16 GB base\n- + optimizer states (32 GB)\n- + gradients (16 GB)\n- + activations (10-15 GB)\n- **Total: 70-80 GB** ❌ Exceeds A100 capacity\n\n**After (QLoRA):**\n- 8B model × 0.5 bytes (4bit) = 4 GB base\n- + optimizer states (2 GB, 8bit)\n- + gradients (2 GB, LoRA only)\n- + activations (4-6 GB, checkpointing)\n- **Total: 12-14 GB** ✅ Fits comfortably\n\n### Next Steps:\n- Run training on A100\n- Monitor GPU memory usage\n- Evaluate fine-tuned model\n- Compare baseline vs fine-tuned performance\n- Upload model to Hugging Face Hub"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}