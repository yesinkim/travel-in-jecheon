{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Kanana RAG Fine-tuning Notebook\n\nThis notebook fine-tunes the Kanana 8B instruct model on RAG tasks using the Jecheon tourism dataset.\n\n**Model:** kakaocorp/kanana-1.5-8b-instruct-2505\n\n**Training Data Format:**\n```\n[Instruction]\në‹¹ì‹ ì€ ì œì²œì‹œ ê´€ê´‘ ì•ˆë‚´ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.\nì œê³µëœ ì—¬ëŸ¬ ë¬¸ì„œ ì¤‘ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì•„, ê·¸ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\nInformation:\n{content1}\nInformation:\n{content2}\nQuestion: {question}\n```\n\n**Evaluation Metrics:**\n\n*Automatic Metrics (during training):*\n- **Perplexity**: Model confidence (lower is better)\n- **Exact Match (EM)**: Exact answer matching accuracy\n- **ROUGE-L**: Text overlap score\n- **BERTScore**: Semantic similarity with Korean BERT\n- **Token F1**: Token-level overlap\n\n*LLM-as-a-Judge Metrics (post-training):*\n- **Accuracy** (true/false): No hallucinations or extra details\n- **Helpfulness** (1-10): How helpful is the answer\n- **Relevance** (1-10): Does it fully address the question\n- **Depth** (1-10): Level of detail in the response\n\nAll metrics are tracked in Weights & Biases for each training run."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install transformers peft datasets wandb bitsandbytes accelerate evaluate rouge_score bert_score anthropic"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"NVIDIA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to Weights & Biases\nimport wandb\n\n# Login to wandb (you'll be prompted for your API key)\nwandb.login()\n\n# Initialize wandb project\nwandb.init(\n    project=\"kanana-rag-finetuning\",\n    name=\"kanana-1.5-8b-instruct-rag\",\n    config={\n        \"model\": \"kakaocorp/kanana-1.5-8b-instruct-2505\",\n        \"task\": \"RAG fine-tuning\",\n        \"dataset\": \"Jecheon Tourism\"\n    }\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load base model and tokenizer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"kakaocorp/kanana-1.5-8b-instruct-2505\"\n\nprint(f\"Loading model: {model_name}\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\"Model loaded successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAG training data\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "data_path = \"/home/user/goodganglabs/data/processed/training_data.jsonl\"\n",
    "\n",
    "# Load JSONL data\n",
    "data_list = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data_list.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data_list)} training examples\")\n",
    "print(\"\\nFirst example:\")\n",
    "print(json.dumps(data_list[0], ensure_ascii=False, indent=2)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert to requested RAG format with instruction\ndef format_rag_data(example):\n    \"\"\"Convert RAG data to the requested format with instruction:\n    [Instruction]\n    ë‹¹ì‹ ì€ ì œì²œì‹œ ê´€ê´‘ ì•ˆë‚´ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤...\n    \n    Information:\n    {content1}\n    Information:\n    {content2}\n    Question: {question}\n    \"\"\"\n    instruction = \"\"\"ë‹¹ì‹ ì€ ì œì²œì‹œ ê´€ê´‘ ì•ˆë‚´ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.\nì œê³µëœ ì—¬ëŸ¬ ë¬¸ì„œ ì¤‘ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì•„, ê·¸ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\në‹µë³€ ì‹œ ì£¼ì˜ì‚¬í•­:\n1. ê´€ë ¨ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”\n2. ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìœ¼ë©´ \"ì œê³µëœ ì •ë³´ì—ëŠ” í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”\n3. ì¶”ì¸¡í•˜ê±°ë‚˜ ë¬¸ì„œ ì™¸ë¶€ ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”\n4. ê°„ê²°í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•˜ì„¸ìš”\"\"\"\n    \n    documents = example['documents']\n    question = example['question']\n    answer = example['answer']\n    \n    # Build information sections\n    info_sections = []\n    for doc in documents:\n        info_sections.append(f\"Information:\\n{doc['content']}\")\n    \n    # Combine: instruction + information sections + question\n    prompt = instruction + \"\\n\\n\"\n    prompt += \"\\n\\n\".join(info_sections)\n    prompt += f\"\\n\\nQuestion: {question}\"\n    \n    return {\n        \"prompt\": prompt,\n        \"answer\": answer\n    }\n\n# Apply formatting\nformatted_data = []\nfor example in data_list:\n    formatted_data.append(format_rag_data(example))\n\nprint(f\"Formatted {len(formatted_data)} examples\")\nprint(\"\\nExample formatted prompt:\")\nprint(formatted_data[0]['prompt'][:500])\nprint(f\"\\nAnswer: {formatted_data[0]['answer']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset with proper format\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format prompts for training\"\"\"\n",
    "    prompts = examples[\"prompt\"]\n",
    "    answers = examples[\"answer\"]\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    texts = []\n",
    "    for prompt, answer in zip(prompts, answers):\n",
    "        # Combine prompt and answer with EOS token\n",
    "        text = f\"{prompt}\\n\\nAnswer: {answer}{EOS_TOKEN}\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"\\nDataset features: {dataset.features}\")\n",
    "print(f\"\\nFirst training example:\")\n",
    "print(dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"text\", \"prompt\", \"answer\"]\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset size: {len(tokenized_dataset)}\")\n",
    "print(f\"Features: {tokenized_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Define compute_metrics function for evaluation\ndef compute_metrics(eval_preds):\n    \"\"\"\n    Compute evaluation metrics during training.\n    \n    Metrics:\n    - Perplexity: Language modeling quality\n    - EM (Exact Match): Exact answer matching accuracy\n    - ROUGE-L: Text overlap (longest common subsequence)\n    - BERTScore: Semantic similarity (Korean model)\n    - F1 Score: Token-level overlap\n    \"\"\"\n    predictions, labels = eval_preds\n    \n    # Decode predictions and labels\n    # Replace -100 in labels (padding tokens)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Clean up whitespace\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [label.strip() for label in decoded_labels]\n    \n    # 1. Exact Match (EM)\n    def normalize_text(text):\n        \"\"\"Normalize text for EM calculation\"\"\"\n        import re\n        # Remove extra whitespace\n        text = re.sub(r'\\s+', ' ', text.strip())\n        # Lowercase (optional, adjust based on needs)\n        # text = text.lower()\n        return text\n    \n    exact_matches = [\n        1 if normalize_text(pred) == normalize_text(label) else 0\n        for pred, label in zip(decoded_preds, decoded_labels)\n    ]\n    em_score = np.mean(exact_matches)\n    \n    # 2. ROUGE Score\n    rouge_result = rouge.compute(\n        predictions=decoded_preds,\n        references=decoded_labels,\n        rouge_types=[\"rougeL\"]\n    )\n    \n    # 3. BERTScore (Korean model)\n    bertscore_result = bertscore_metric.compute(\n        predictions=decoded_preds,\n        references=decoded_labels,\n        lang=\"ko\",\n        model_type=\"klue/roberta-base\",  # Korean BERT model\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )\n    \n    # 4. F1 Score (token-level)\n    def f1_score(pred, label):\n        pred_tokens = set(pred.split())\n        label_tokens = set(label.split())\n        \n        if len(pred_tokens) == 0 or len(label_tokens) == 0:\n            return 0.0\n        \n        common = pred_tokens & label_tokens\n        if len(common) == 0:\n            return 0.0\n        \n        precision = len(common) / len(pred_tokens)\n        recall = len(common) / len(label_tokens)\n        \n        return 2 * (precision * recall) / (precision + recall)\n    \n    f1_scores = [f1_score(pred, label) for pred, label in zip(decoded_preds, decoded_labels)]\n    avg_f1 = np.mean(f1_scores)\n    \n    # Compile results\n    result = {\n        \"exact_match\": em_score,\n        \"rouge_l\": rouge_result[\"rougeL\"],\n        \"bertscore_precision\": np.mean(bertscore_result[\"precision\"]),\n        \"bertscore_recall\": np.mean(bertscore_result[\"recall\"]),\n        \"bertscore_f1\": np.mean(bertscore_result[\"f1\"]),\n        \"token_f1\": avg_f1,\n    }\n    \n    # Note: Perplexity is automatically computed from eval_loss by Trainer\n    \n    return result\n\nprint(\"compute_metrics function defined!\")\nprint(\"Metrics will be logged to wandb during training.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load evaluation metrics\nfrom evaluate import load\nimport numpy as np\nimport torch\n\n# Load metrics\nrouge = load('rouge')\nbertscore_metric = load('bertscore')\n\nprint(\"Loaded evaluation metrics:\")\nprint(\"  - Exact Match (EM)\")\nprint(\"  - ROUGE (ROUGE-L)\")\nprint(\"  - BERTScore (Korean semantic similarity)\")\nprint(\"  - F1 Score (token overlap)\")\nprint(\"  - Perplexity (from loss)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Split dataset into train and validation (80/20 split)\nfrom datasets import DatasetDict\n\n# Split tokenized dataset\nsplit_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=1234)\n\ntrain_dataset = split_dataset['train']\neval_dataset = split_dataset['test']\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(eval_dataset)}\")\nprint(f\"Train/Eval ratio: {len(train_dataset)/len(eval_dataset):.2f}\")\n\n# Keep reference to original formatted data for metric computation\nformatted_data_dict = {\n    \"train\": formatted_data[:len(train_dataset)],\n    \"eval\": formatted_data[len(train_dataset):]\n}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for parameter-efficient fine-tuning\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,                    # LoRA rank\n",
    "    lora_alpha=32,          # LoRA alpha\n",
    "    lora_dropout=0.1,       # Dropout probability\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Log LoRA config to wandb\n",
    "wandb.config.update({\n",
    "    \"lora_r\": lora_config.r,\n",
    "    \"lora_alpha\": lora_config.lora_alpha,\n",
    "    \"lora_dropout\": lora_config.lora_dropout,\n",
    "    \"target_modules\": lora_config.target_modules\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "# Log to wandb\n",
    "wandb.config.update({\n",
    "    \"gpu_name\": gpu_stats.name,\n",
    "    \"gpu_max_memory_gb\": max_memory,\n",
    "    \"gpu_start_memory_gb\": start_gpu_memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure training arguments with evaluation and wandb integration\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    # Output settings\n    output_dir=\"./outputs/kanana-rag\",\n    overwrite_output_dir=True,\n    \n    # Training hyperparameters\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    warmup_steps=10,\n    \n    # Optimization\n    bf16=True,\n    optim=\"adamw_torch\",\n    lr_scheduler_type=\"linear\",\n    \n    # Logging\n    logging_steps=5,\n    logging_dir=\"./logs\",\n    report_to=\"wandb\",  # Enable wandb reporting\n    \n    # Saving\n    save_strategy=\"steps\",\n    save_steps=50,\n    save_total_limit=3,\n    load_best_model_at_end=True,  # Load best model at end\n    \n    # Evaluation with metrics\n    evaluation_strategy=\"steps\",  # Evaluate during training\n    eval_steps=25,  # Evaluate every 25 steps\n    metric_for_best_model=\"bertscore_f1\",  # Use BERTScore F1 for best model\n    greater_is_better=True,  # Higher BERTScore is better\n    \n    # Other\n    seed=1234,\n    data_seed=1234,\n    remove_unused_columns=True,\n    predict_with_generate=False,  # Use logits for faster evaluation\n)\n\nprint(\"Training arguments configured:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\nprint(f\"  Evaluation strategy: {training_args.evaluation_strategy} (every {training_args.eval_steps} steps)\")\nprint(f\"  Best model metric: {training_args.metric_for_best_model}\")\nprint(f\"\\nMetrics tracked:\")\nprint(f\"  - Perplexity (from eval_loss)\")\nprint(f\"  - Exact Match (EM)\")\nprint(f\"  - ROUGE-L\")\nprint(f\"  - BERTScore (P/R/F1)\")\nprint(f\"  - Token F1\")\nprint(f\"\\nAll metrics will be logged to WandB!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Trainer with evaluation\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\nprint(\"Trainer initialized successfully!\")\nprint(f\"\\nTraining setup:\")\nprint(f\"  Train samples: {len(train_dataset)}\")\nprint(f\"  Eval samples: {len(eval_dataset)}\")\nprint(f\"  Metrics: Perplexity, EM, ROUGE-L, BERTScore, Token F1\")\nprint(f\"  Best model selection: {training_args.metric_for_best_model}\")\nprint(f\"\\nâš ï¸ Note: BERTScore evaluation may take some time due to Korean BERT model inference.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final GPU memory usage\n",
    "final_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"\\nFinal GPU memory: {final_gpu_memory} GB\")\n",
    "print(f\"Peak memory used: {final_gpu_memory - start_gpu_memory} GB\")\n",
    "\n",
    "# Log final stats to wandb\n",
    "wandb.log({\n",
    "    \"final_gpu_memory_gb\": final_gpu_memory,\n",
    "    \"peak_memory_used_gb\": final_gpu_memory - start_gpu_memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./outputs/kanana-rag-final\"\n",
    "\n",
    "print(f\"Saving model to {output_dir}...\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"\\nModel location: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with a sample\n",
    "print(\"Testing inference...\\n\")\n",
    "\n",
    "# Get a test example\n",
    "test_prompt = formatted_data[0]['prompt']\n",
    "expected_answer = formatted_data[0]['answer']\n",
    "\n",
    "print(\"Test Prompt:\")\n",
    "print(test_prompt[:300])\n",
    "print(\"\\n...\")\n",
    "\n",
    "# Tokenize and generate\n",
    "model.eval()\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(generated_text[len(test_prompt):])\n",
    "\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(expected_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close wandb run\n",
    "wandb.finish()\n",
    "print(\"WandB run finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# EXAMPLE: Run LLM-as-a-Judge evaluation on test samples\n# Uncomment and run this cell after training is complete\n\n\"\"\"\n# Step 1: Set your Anthropic API key\napi_key = os.getenv(\"ANTHROPIC_API_KEY\")\nif not api_key:\n    print(\"âš ï¸  Please set ANTHROPIC_API_KEY environment variable\")\n    print(\"   export ANTHROPIC_API_KEY='your-api-key'\")\nelse:\n    # Step 2: Generate responses for evaluation (10 samples)\n    print(\"Generating responses for evaluation...\")\n    eval_samples = generate_responses_for_evaluation(\n        model=trainer.model,\n        tokenizer=tokenizer,\n        test_samples=formatted_data,\n        max_samples=10\n    )\n    \n    # Step 3: Evaluate with LLM judge\n    print(\"\\nEvaluating with Claude as judge...\")\n    judge_results = evaluate_batch_with_llm_judge(\n        samples=eval_samples,\n        api_key=api_key,\n        judge_model=\"claude-3-5-sonnet-20241022\"\n    )\n    \n    # Step 4: Log results to WandB\n    print(\"\\nLogging results to WandB...\")\n    metrics = log_llm_judge_to_wandb(judge_results)\n    \n    # Step 5: Save results to file\n    output_path = \"./outputs/llm_judge_results.json\"\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(judge_results, f, ensure_ascii=False, indent=2)\n    print(f\"\\nðŸ’¾ Results saved to: {output_path}\")\n\"\"\"\n\nprint(\"LLM-as-a-Judge evaluation example loaded!\")\nprint(\"\\nUncomment the code above to run evaluation after training.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Analyze and log LLM judge results to WandB\n\ndef analyze_llm_judge_results(results: List[Dict]) -> Dict:\n    \"\"\"\n    Analyze LLM judge evaluation results\n    \n    Returns:\n        Dictionary with aggregated metrics\n    \"\"\"\n    # Extract scores\n    accuracy_scores = []\n    helpfulness_scores = []\n    relevance_scores = []\n    depth_scores = []\n    \n    for result in results:\n        # Accuracy (true/false â†’ 1/0)\n        if 'accuracy' in result and 'accuracy' in result['accuracy']:\n            acc = result['accuracy']['accuracy']\n            if isinstance(acc, bool):\n                accuracy_scores.append(1 if acc else 0)\n            elif isinstance(acc, str):\n                accuracy_scores.append(1 if acc.lower() == 'true' else 0)\n        \n        # Helpfulness (1-10)\n        if 'helpfulness' in result and 'helpfulness' in result['helpfulness']:\n            helpfulness_scores.append(int(result['helpfulness']['helpfulness']))\n        \n        # Relevance (1-10)\n        if 'relevance' in result and 'relevance' in result['relevance']:\n            relevance_scores.append(int(result['relevance']['relevance']))\n        \n        # Depth (1-10)\n        if 'depth' in result and 'depth' in result['depth']:\n            depth_scores.append(int(result['depth']['depth']))\n    \n    # Calculate averages\n    metrics = {\n        \"llm_judge_accuracy\": np.mean(accuracy_scores) if accuracy_scores else 0,\n        \"llm_judge_helpfulness\": np.mean(helpfulness_scores) if helpfulness_scores else 0,\n        \"llm_judge_relevance\": np.mean(relevance_scores) if relevance_scores else 0,\n        \"llm_judge_depth\": np.mean(depth_scores) if depth_scores else 0,\n        \"num_samples_evaluated\": len(results)\n    }\n    \n    return metrics\n\ndef log_llm_judge_to_wandb(results: List[Dict], run_name: str = None):\n    \"\"\"\n    Log LLM judge evaluation results to WandB\n    \n    Args:\n        results: LLM judge evaluation results\n        run_name: Optional run name for WandB\n    \"\"\"\n    # Analyze results\n    metrics = analyze_llm_judge_results(results)\n    \n    # Log to WandB\n    wandb.log({\n        \"eval/llm_judge_accuracy\": metrics[\"llm_judge_accuracy\"],\n        \"eval/llm_judge_helpfulness\": metrics[\"llm_judge_helpfulness\"],\n        \"eval/llm_judge_relevance\": metrics[\"llm_judge_relevance\"],\n        \"eval/llm_judge_depth\": metrics[\"llm_judge_depth\"],\n    })\n    \n    # Create summary table\n    summary_table = wandb.Table(\n        columns=[\"Metric\", \"Score\", \"Scale\"],\n        data=[\n            [\"Accuracy\", f\"{metrics['llm_judge_accuracy']:.2%}\", \"0-100%\"],\n            [\"Helpfulness\", f\"{metrics['llm_judge_helpfulness']:.1f}\", \"1-10\"],\n            [\"Relevance\", f\"{metrics['llm_judge_relevance']:.1f}\", \"1-10\"],\n            [\"Depth\", f\"{metrics['llm_judge_depth']:.1f}\", \"1-10\"],\n        ]\n    )\n    wandb.log({\"llm_judge_summary\": summary_table})\n    \n    # Create detailed results table\n    detailed_data = []\n    for i, result in enumerate(results[:20]):  # First 20 samples\n        detailed_data.append([\n            i + 1,\n            result.get('question', '')[:50] + \"...\",\n            result.get('response', '')[:100] + \"...\",\n            result.get('accuracy', {}).get('accuracy', 'N/A'),\n            result.get('helpfulness', {}).get('helpfulness', 'N/A'),\n            result.get('relevance', {}).get('relevance', 'N/A'),\n            result.get('depth', {}).get('depth', 'N/A'),\n        ])\n    \n    detailed_table = wandb.Table(\n        columns=[\"#\", \"Question\", \"Response\", \"Accuracy\", \"Helpfulness\", \"Relevance\", \"Depth\"],\n        data=detailed_data\n    )\n    wandb.log({\"llm_judge_detailed_results\": detailed_table})\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"LLM Judge Evaluation Results\")\n    print(\"=\"*60)\n    print(f\"Accuracy:    {metrics['llm_judge_accuracy']:.2%} (no hallucinations)\")\n    print(f\"Helpfulness: {metrics['llm_judge_helpfulness']:.1f}/10\")\n    print(f\"Relevance:   {metrics['llm_judge_relevance']:.1f}/10\")\n    print(f\"Depth:       {metrics['llm_judge_depth']:.1f}/10\")\n    print(f\"\\nEvaluated {metrics['num_samples_evaluated']} samples\")\n    print(\"=\"*60)\n    \n    return metrics\n\nprint(\"LLM judge analysis and logging functions loaded!\")\nprint(\"\\nTo use:\")\nprint(\"  metrics = log_llm_judge_to_wandb(results)\")\nprint(\"\\nThis will log all 4 metrics to WandB dashboard\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example: Evaluate test samples with LLM-as-a-Judge\n# This cell demonstrates how to use LLM judge evaluation\n\ndef generate_responses_for_evaluation(\n    model,\n    tokenizer,\n    test_samples: List[Dict],\n    max_samples: int = 10\n) -> List[Dict]:\n    \"\"\"\n    Generate responses for test samples\n    \n    Args:\n        model: Fine-tuned model\n        tokenizer: Tokenizer\n        test_samples: List of test samples with 'prompt', 'answer', 'documents'\n        max_samples: Maximum number of samples to evaluate\n        \n    Returns:\n        List of samples ready for LLM judge evaluation\n    \"\"\"\n    model.eval()\n    evaluation_samples = []\n    \n    for i, sample in enumerate(test_samples[:max_samples]):\n        print(f\"Generating response {i+1}/{min(max_samples, len(test_samples))}...\")\n        \n        # Get the prompt (instruction + information + question)\n        prompt = sample['prompt']\n        \n        # Get the correct context (first document marked as correct)\n        context = \"\"\n        for doc in sample.get('documents', []):\n            if doc.get('is_correct', False):\n                context = doc['content']\n                break\n        \n        # If no correct document found, use first document\n        if not context and sample.get('documents'):\n            context = sample['documents'][0]['content']\n        \n        # Generate response\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=200,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        \n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Extract only the answer part (after \"Answer:\")\n        if \"Answer:\" in generated_text:\n            response = generated_text.split(\"Answer:\")[-1].strip()\n        else:\n            response = generated_text[len(prompt):].strip()\n        \n        # Extract question from prompt\n        if \"Question:\" in prompt:\n            question = prompt.split(\"Question:\")[-1].strip()\n        else:\n            question = \"Unknown question\"\n        \n        evaluation_samples.append({\n            \"question\": question,\n            \"context\": context,\n            \"response\": response,\n            \"expected_answer\": sample.get('answer', ''),\n            \"filename\": sample.get('documents', [{}])[0].get('filename', 'document')\n        })\n    \n    return evaluation_samples\n\nprint(\"Response generation function loaded!\")\nprint(\"\\nTo use:\")\nprint(\"  1. Generate responses: samples = generate_responses_for_evaluation(model, tokenizer, formatted_data)\")\nprint(\"  2. Evaluate with judge: results = evaluate_batch_with_llm_judge(samples, api_key)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# LLM-as-a-Judge Evaluation\n# Using GPT-4 or Claude to evaluate model responses on 4 metrics\n# Based on the paper's evaluation methodology\n\nimport json\nfrom typing import Dict, List, Tuple\nimport anthropic\nimport os\n\n# Evaluation prompts from the paper\nEVALUATION_PROMPTS = {\n    \"accuracy\": \"\"\"Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below, based solely on a piece of information extracted from a file provided below. Your evaluation should consider the accuracy of the response.\n\nYou will check whether the response contains extra details not found in the piece of information provided. If extra details are found, accuracy is false. Otherwise, accuracy is true. Take note that if the response partially addresses the question, but did not provide extra details not found in the piece of information provided, the response will still be considered accurate (hence accuracy = true).\n\nBegin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the accuracy with true or false by strictly following this JSON format:\n{\n\"accuracy_explanation\": <provide an explanation on accuracy, whether extra details outside the content were found.>,\n\"accuracy\": <true/false>\n}\"\"\",\n    \n    \"helpfulness\": \"\"\"Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below, based solely on a piece of information extracted from a file provided below. Your evaluation should consider the helpfulness of the response.\n\nYou will check whether the AI assistant is helpful in answering the question based on the response.\n\nBegin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the helpfulness on a scale of 1 to 10 by strictly following this JSON format:\n{\n\"helpfulness_explanation\": <provide an explanation on helpfulness>,\n\"helpfulness\": <score>\n}\"\"\",\n    \n    \"relevance\": \"\"\"Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below, based solely on a piece of information extracted from a file provided below. Your evaluation should consider the relevance of the response.\n\nYou will check the relevance of the response by evaluating whether the response fully addresses the question.\n\nBegin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the relevance on a scale of 1 to 10 by strictly following this JSON format:\n{\n\"relevance_explanation\": <provide an explanation on relevance>,\n\"relevance\": <score>\n}\"\"\",\n    \n    \"depth\": \"\"\"Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below, based solely on a piece of information extracted from a file provided below. Your evaluation should consider the depth of the response.\n\nYou will check the depth of the response by evaluating the level of detail of the response in answering the question.\n\nBegin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the depth on a scale of 1 to 10 by strictly following this JSON format:\n{\n\"depth_explanation\": <provide an explanation on depth>,\n\"depth\": <score>\n}\"\"\"\n}\n\ndef create_user_message(question: str, context: str, response: str, filename: str = \"document\") -> str:\n    \"\"\"Create user message for LLM judge evaluation\"\"\"\n    return f\"\"\"[The Start of Provided Information Extracted from a File]\nFilename: {filename}\nInformation: {context}\n[The End of Provided Information]\n\n[Question]\n{question}\n\n[The Start of Assistant's Response]\n{response}\n[The End of Assistant's Response]\"\"\"\n\ndef evaluate_with_llm_judge(\n    question: str,\n    context: str,\n    response: str,\n    metric: str,\n    client,\n    model: str = \"claude-3-5-sonnet-20241022\"\n) -> Dict:\n    \"\"\"\n    Evaluate a single response using LLM-as-a-Judge\n    \n    Args:\n        question: The question asked\n        context: The relevant document context\n        response: Model's response to evaluate\n        metric: One of 'accuracy', 'helpfulness', 'relevance', 'depth'\n        client: Anthropic client instance\n        model: Judge model to use\n        \n    Returns:\n        Dictionary with evaluation results\n    \"\"\"\n    system_prompt = EVALUATION_PROMPTS[metric]\n    user_message = create_user_message(question, context, response)\n    \n    # Call Claude as judge\n    message = client.messages.create(\n        model=model,\n        max_tokens=1024,\n        system=system_prompt,\n        messages=[\n            {\"role\": \"user\", \"content\": user_message}\n        ]\n    )\n    \n    # Parse JSON response\n    try:\n        result = json.loads(message.content[0].text)\n        return result\n    except json.JSONDecodeError:\n        # Fallback: extract JSON from text\n        text = message.content[0].text\n        # Try to find JSON in the response\n        import re\n        json_match = re.search(r'\\{[^}]+\\}', text, re.DOTALL)\n        if json_match:\n            return json.loads(json_match.group())\n        else:\n            return {\"error\": \"Failed to parse JSON\", \"raw_response\": text}\n\ndef evaluate_batch_with_llm_judge(\n    samples: List[Dict],\n    api_key: str,\n    judge_model: str = \"claude-3-5-sonnet-20241022\"\n) -> List[Dict]:\n    \"\"\"\n    Evaluate multiple samples with LLM judge\n    \n    Args:\n        samples: List of dicts with 'question', 'context', 'response'\n        api_key: Anthropic API key\n        judge_model: Claude model to use as judge\n        \n    Returns:\n        List of evaluation results\n    \"\"\"\n    client = anthropic.Anthropic(api_key=api_key)\n    \n    results = []\n    metrics = ['accuracy', 'helpfulness', 'relevance', 'depth']\n    \n    for i, sample in enumerate(samples):\n        print(f\"\\nEvaluating sample {i+1}/{len(samples)}...\")\n        sample_result = {\n            \"question\": sample[\"question\"],\n            \"context\": sample[\"context\"],\n            \"response\": sample[\"response\"]\n        }\n        \n        # Evaluate on all 4 metrics\n        for metric in metrics:\n            print(f\"  - {metric}...\", end=\" \")\n            try:\n                eval_result = evaluate_with_llm_judge(\n                    question=sample[\"question\"],\n                    context=sample[\"context\"],\n                    response=sample[\"response\"],\n                    metric=metric,\n                    client=client,\n                    model=judge_model\n                )\n                sample_result[metric] = eval_result\n                print(\"âœ“\")\n            except Exception as e:\n                print(f\"âœ— Error: {e}\")\n                sample_result[metric] = {\"error\": str(e)}\n        \n        results.append(sample_result)\n    \n    return results\n\nprint(\"LLM-as-a-Judge evaluation functions loaded!\")\nprint(\"\\nAvailable metrics:\")\nprint(\"  1. Accuracy (true/false) - No hallucinations\")\nprint(\"  2. Helpfulness (1-10) - How helpful is the answer\")\nprint(\"  3. Relevance (1-10) - Does it address the question\")\nprint(\"  4. Depth (1-10) - Level of detail provided\")\nprint(\"\\nUsage:\")\nprint(\"  Set ANTHROPIC_API_KEY environment variable\")\nprint(\"  Then run evaluate_batch_with_llm_judge(samples, api_key)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook:\n1. âœ… Loads Kanana 1.5 8B instruct model (kakaocorp/kanana-1.5-8b-instruct-2505)\n2. âœ… Formats RAG training data with instruction and information format\n3. âœ… Splits data into train/validation (80/20)\n4. âœ… Applies LoRA for efficient fine-tuning\n5. âœ… **Automatic evaluation metrics (during training):**\n   - **Perplexity**: Language modeling quality (from eval_loss)\n   - **Exact Match (EM)**: Exact answer matching accuracy\n   - **ROUGE-L**: Text overlap with longest common subsequence\n   - **BERTScore**: Semantic similarity using Korean BERT (klue/roberta-base)\n   - **Token F1**: Token-level overlap score\n6. âœ… **LLM-as-a-Judge evaluation (post-training):**\n   - **Accuracy** (true/false): No hallucinations or extra details\n   - **Helpfulness** (1-10): How helpful is the answer\n   - **Relevance** (1-10): Does it fully address the question\n   - **Depth** (1-10): Level of detail in the response\n7. âœ… Tracks all metrics in Weights & Biases\n8. âœ… Saves best model based on BERTScore F1\n9. âœ… Tests inference on sample data\n\n### Key Features:\n- **System instruction** guides the model to be a Jecheon tourism expert\n- Model learns to find relevant documents and answer based on them\n- Model learns to say \"no information available\" when appropriate\n- **Semantic evaluation** with BERTScore catches paraphrased answers\n- **LLM-as-a-Judge** provides qualitative assessment using Claude\n- **WandB integration** for real-time metric tracking across runs\n- Training format prevents hallucination\n\n### Automatic Evaluation Strategy:\n- Evaluate every 25 steps during training\n- Track 7 metrics: eval_loss, perplexity, exact_match, rouge_l, bertscore_f1, bertscore_precision, bertscore_recall, token_f1\n- Best model selected by BERTScore F1 (semantic similarity)\n- All metrics logged to WandB for comparison across runs\n\n### LLM-as-a-Judge Evaluation:\n- Run after training on test samples\n- Uses Claude 3.5 Sonnet as impartial judge\n- Evaluates on 4 dimensions based on research paper methodology\n- Results logged to WandB with summary and detailed tables\n- Provides qualitative insights beyond automatic metrics\n\n### Usage:\n1. **Training**: Run cells sequentially to train the model\n2. **LLM Judge**: Uncomment and run LLM judge evaluation cells\n3. **Set API key**: `export ANTHROPIC_API_KEY='your-key'`\n4. **View results**: Check WandB dashboard for all metrics\n\n### Next Steps:\n- Run full evaluation on test set with both automatic and LLM judge metrics\n- Compare baseline vs fine-tuned performance\n- Upload model to Hugging Face Hub\n- Generate comprehensive report with quantitative and qualitative analysis"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}