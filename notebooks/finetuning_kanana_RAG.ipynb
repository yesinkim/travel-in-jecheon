{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Kanana RAG Fine-tuning Notebook\n\nThis notebook fine-tunes the Kanana 8B instruct model on RAG tasks using the Jecheon tourism dataset.\n\n**Model:** kakaocorp/kanana-1.5-8b-instruct-2505\n\n**Training Data Format:**\n```\n[Instruction]\n당신은 제천시 관광 안내 전문가입니다.\n제공된 여러 문서 중에서 질문과 관련된 문서를 찾아, 그 문서의 내용을 바탕으로 정확하고 친절하게 답변해주세요.\n\nInformation:\n{content1}\nInformation:\n{content2}\nQuestion: {question}\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install transformers peft datasets wandb bitsandbytes accelerate\n!pip install rouge-score bert-score evaluate scikit-learn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"NVIDIA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to Weights & Biases\nimport wandb\n\n# Login to wandb (you'll be prompted for your API key)\nwandb.login()\n\n# Initialize wandb project\nwandb.init(\n    project=\"kanana-rag-finetuning\",\n    name=\"kanana-1.5-8b-instruct-rag\",\n    config={\n        \"model\": \"kakaocorp/kanana-1.5-8b-instruct-2505\",\n        \"task\": \"RAG fine-tuning\",\n        \"dataset\": \"Jecheon Tourism\"\n    }\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load base model and tokenizer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"kakaocorp/kanana-1.5-8b-instruct-2505\"\n\nprint(f\"Loading model: {model_name}\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\"Model loaded successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAG training data\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "data_path = \"/home/user/goodganglabs/data/processed/training_data.jsonl\"\n",
    "\n",
    "# Load JSONL data\n",
    "data_list = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data_list.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data_list)} training examples\")\n",
    "print(\"\\nFirst example:\")\n",
    "print(json.dumps(data_list[0], ensure_ascii=False, indent=2)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Convert to requested RAG format with instruction\ndef format_rag_data(example):\n    \"\"\"Convert RAG data to the requested format with instruction:\n    [Instruction]\n    당신은 제천시 관광 안내 전문가입니다...\n    \n    Information:\n    {content1}\n    Information:\n    {content2}\n    Question: {question}\n    \"\"\"\n    instruction = \"\"\"당신은 제천시 관광 안내 전문가입니다.\n제공된 여러 문서 중에서 질문과 관련된 문서를 찾아, 그 문서의 내용을 바탕으로 정확하고 친절하게 답변해주세요.\n\n답변 시 주의사항:\n1. 관련 문서의 내용만을 바탕으로 답변하세요\n2. 문서에 정보가 없으면 \"제공된 정보에는 해당 내용이 없습니다\"라고 답변하세요\n3. 추측하거나 문서 외부 지식을 사용하지 마세요\n4. 간결하고 이해하기 쉽게 답변하세요\"\"\"\n    \n    documents = example['documents']\n    question = example['question']\n    answer = example['answer']\n    \n    # Build information sections\n    info_sections = []\n    for doc in documents:\n        info_sections.append(f\"Information:\\n{doc['content']}\")\n    \n    # Combine: instruction + information sections + question\n    prompt = instruction + \"\\n\\n\"\n    prompt += \"\\n\\n\".join(info_sections)\n    prompt += f\"\\n\\nQuestion: {question}\"\n    \n    return {\n        \"prompt\": prompt,\n        \"answer\": answer,\n        \"question\": question,\n        \"documents\": documents,\n        \"correct_doc_id\": example.get('correct_doc_id', None)\n    }\n\n# Apply formatting to TRAIN data only\nformatted_train_data = []\nfor example in train_data:\n    formatted_train_data.append(format_rag_data(example))\n\n# Also format test data (but don't use for training)\nformatted_test_data = []\nfor example in test_data:\n    formatted_test_data.append(format_rag_data(example))\n\nprint(f\"Formatted {len(formatted_train_data)} training examples\")\nprint(f\"Formatted {len(formatted_test_data)} test examples\")\nprint(\"\\nExample formatted prompt:\")\nprint(formatted_train_data[0]['prompt'][:500])\nprint(f\"\\nAnswer: {formatted_train_data[0]['answer']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create training and evaluation datasets\ndef formatting_prompts_func(examples):\n    \"\"\"Format prompts for training\"\"\"\n    prompts = examples[\"prompt\"]\n    answers = examples[\"answer\"]\n    \n    EOS_TOKEN = tokenizer.eos_token\n    \n    texts = []\n    for prompt, answer in zip(prompts, answers):\n        # Combine prompt and answer with EOS token\n        text = f\"{prompt}\\n\\nAnswer: {answer}{EOS_TOKEN}\"\n        texts.append(text)\n    \n    return {\"text\": texts}\n\n# Create train dataset\ntrain_dataset = Dataset.from_list(formatted_train_data)\ntrain_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n\n# Create eval dataset (keep original structure for metrics computation)\neval_dataset = Dataset.from_list(formatted_test_data)\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Eval dataset size: {len(eval_dataset)}\")\nprint(f\"\\nTrain dataset features: {train_dataset.features}\")\nprint(f\"Eval dataset features: {eval_dataset.features}\")\nprint(f\"\\nFirst training example:\")\nprint(train_dataset[0]['text'][:500])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tokenize training dataset only\ndef tokenize_function(examples):\n    tokens = tokenizer(\n        examples[\"text\"], \n        padding=\"max_length\",\n        truncation=True,\n        max_length=2048,\n        return_tensors=\"pt\"\n    )\n    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n    return tokens\n\ntokenized_train_dataset = train_dataset.map(\n    tokenize_function, \n    batched=True, \n    remove_columns=[\"text\", \"prompt\", \"answer\", \"question\", \"documents\", \"correct_doc_id\"]\n)\n\nprint(f\"Tokenized train dataset size: {len(tokenized_train_dataset)}\")\nprint(f\"Features: {tokenized_train_dataset.features}\")"
  },
  {
   "cell_type": "code",
   "source": "# Define RAG-specific evaluation metrics\nfrom rouge_score import rouge_scorer\nfrom bert_score import score as bert_score\nimport numpy as np\nfrom typing import List, Dict\n\nclass RAGMetrics:\n    \"\"\"Compute RAG-specific metrics\"\"\"\n    \n    def __init__(self):\n        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n    \n    def compute_context_precision(self, documents: List[Dict], correct_doc_id: str, k: int = 3) -> float:\n        \"\"\"\n        Compute if the correct document appears in top-k retrieved documents.\n        Returns 1.0 if found in top-k, 0.0 otherwise.\n        \"\"\"\n        if not correct_doc_id:\n            return 0.0\n        \n        # Check if correct doc is in the provided documents (top-k)\n        doc_ids = [doc['doc_id'] for doc in documents[:k]]\n        return 1.0 if correct_doc_id in doc_ids else 0.0\n    \n    def compute_context_recall(self, documents: List[Dict], correct_doc_id: str) -> float:\n        \"\"\"\n        Compute if the correct document was retrieved at all.\n        Returns 1.0 if found, 0.0 otherwise.\n        \"\"\"\n        if not correct_doc_id:\n            return 0.0\n        \n        doc_ids = [doc['doc_id'] for doc in documents]\n        return 1.0 if correct_doc_id in doc_ids else 0.0\n    \n    def compute_rouge_l(self, prediction: str, reference: str) -> float:\n        \"\"\"Compute ROUGE-L F1 score\"\"\"\n        scores = self.rouge_scorer.score(reference, prediction)\n        return scores['rougeL'].fmeasure\n    \n    def compute_bert_score(self, predictions: List[str], references: List[str]) -> float:\n        \"\"\"Compute BERTScore F1 (batch computation for efficiency)\"\"\"\n        if not predictions or not references:\n            return 0.0\n        \n        # Use Korean BERT model for better Korean language support\n        P, R, F1 = bert_score(\n            predictions, \n            references, \n            lang='ko',\n            verbose=False,\n            device='cuda' if torch.cuda.is_available() else 'cpu'\n        )\n        return F1.mean().item()\n    \n    def compute_answer_relevance(self, answer: str, question: str) -> float:\n        \"\"\"\n        Compute answer relevance using simple heuristics:\n        - Length check (not too short, not too long)\n        - Contains key terms from question\n        \"\"\"\n        if not answer or len(answer.strip()) < 10:\n            return 0.0\n        \n        # Check if answer contains question keywords (simple approach)\n        question_words = set(question.replace('?', '').split())\n        answer_words = set(answer.split())\n        \n        # Remove common stop words (basic Korean stop words)\n        stop_words = {'은', '는', '이', '가', '을', '를', '의', '에', '에서', '로', '으로', '와', '과', '도', '만'}\n        question_words -= stop_words\n        answer_words -= stop_words\n        \n        if not question_words:\n            return 1.0\n        \n        # Compute overlap ratio\n        overlap = len(question_words & answer_words) / len(question_words)\n        return min(overlap, 1.0)\n\n# Initialize metrics calculator\nrag_metrics = RAGMetrics()\n\nprint(\"RAG Metrics initialized successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"text\", \"prompt\", \"answer\"]\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset size: {len(tokenized_dataset)}\")\n",
    "print(f\"Features: {tokenized_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for parameter-efficient fine-tuning\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,                    # LoRA rank\n",
    "    lora_alpha=32,          # LoRA alpha\n",
    "    lora_dropout=0.1,       # Dropout probability\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Log LoRA config to wandb\n",
    "wandb.config.update({\n",
    "    \"lora_r\": lora_config.r,\n",
    "    \"lora_alpha\": lora_config.lora_alpha,\n",
    "    \"lora_dropout\": lora_config.lora_dropout,\n",
    "    \"target_modules\": lora_config.target_modules\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "# Log to wandb\n",
    "wandb.config.update({\n",
    "    \"gpu_name\": gpu_stats.name,\n",
    "    \"gpu_max_memory_gb\": max_memory,\n",
    "    \"gpu_start_memory_gb\": start_gpu_memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments with wandb integration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output settings\n",
    "    output_dir=\"./outputs/kanana-rag\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=10,\n",
    "    \n",
    "    # Optimization\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"wandb\",  # Enable wandb reporting\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"no\",  # No validation set in this example\n",
    "    \n",
    "    # Other\n",
    "    seed=1234,\n",
    "    data_seed=1234,\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Total steps: ~{len(tokenized_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final GPU memory usage\n",
    "final_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"\\nFinal GPU memory: {final_gpu_memory} GB\")\n",
    "print(f\"Peak memory used: {final_gpu_memory - start_gpu_memory} GB\")\n",
    "\n",
    "# Log final stats to wandb\n",
    "wandb.log({\n",
    "    \"final_gpu_memory_gb\": final_gpu_memory,\n",
    "    \"peak_memory_used_gb\": final_gpu_memory - start_gpu_memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./outputs/kanana-rag-final\"\n",
    "\n",
    "print(f\"Saving model to {output_dir}...\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"\\nModel location: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with a sample\n",
    "print(\"Testing inference...\\n\")\n",
    "\n",
    "# Get a test example\n",
    "test_prompt = formatted_data[0]['prompt']\n",
    "expected_answer = formatted_data[0]['answer']\n",
    "\n",
    "print(\"Test Prompt:\")\n",
    "print(test_prompt[:300])\n",
    "print(\"\\n...\")\n",
    "\n",
    "# Tokenize and generate\n",
    "model.eval()\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(generated_text[len(test_prompt):])\n",
    "\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(expected_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close wandb run\n",
    "wandb.finish()\n",
    "print(\"WandB run finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook:\n1. ✅ Loads Kanana 1.5 8B instruct model (kakaocorp/kanana-1.5-8b-instruct-2505)\n2. ✅ Formats RAG training data with instruction and information format:\n   ```\n   [Instruction: 제천시 관광 안내 전문가 역할]\n   \n   Information:\n   {content1}\n   Information:\n   {content2}\n   Question: {question}\n   \n   Answer: {answer}\n   ```\n3. ✅ Applies LoRA for efficient fine-tuning\n4. ✅ Tracks training with Weights & Biases\n5. ✅ Saves the fine-tuned model\n6. ✅ Tests inference on sample data\n\n### Key Features:\n- System instruction guides the model to be a Jecheon tourism expert\n- Model learns to find relevant documents and answer based on them\n- Model learns to say \"no information available\" when appropriate\n- Training format prevents hallucination\n\n### Next Steps:\n- Run full evaluation on test set\n- Compare baseline vs fine-tuned performance\n- Upload model to Hugging Face Hub\n- Generate report with metrics and examples"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}