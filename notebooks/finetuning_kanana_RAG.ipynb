{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Kanana RAG Fine-tuning Notebook\n\nThis notebook fine-tunes the Kanana 8B instruct model on RAG tasks using the Jecheon tourism dataset.\n\n**Model:** kakaocorp/kanana-1.5-8b-instruct-2505\n\n**Training Data Format:**\n```\n[Instruction]\n당신은 제천시 관광 안내 전문가입니다.\n제공된 여러 문서 중에서 질문과 관련된 문서를 찾아, 그 문서의 내용을 바탕으로 정확하고 친절하게 답변해주세요.\n\nInformation:\n{content1}\nInformation:\n{content2}\nQuestion: {question}\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers peft datasets wandb bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"NVIDIA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to Weights & Biases\nimport wandb\n\n# Login to wandb (you'll be prompted for your API key)\nwandb.login()\n\n# Initialize wandb project\nwandb.init(\n    project=\"kanana-rag-finetuning\",\n    name=\"kanana-1.5-8b-instruct-rag\",\n    config={\n        \"model\": \"kakaocorp/kanana-1.5-8b-instruct-2505\",\n        \"task\": \"RAG fine-tuning\",\n        \"dataset\": \"Jecheon Tourism\"\n    }\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load base model and tokenizer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"kakaocorp/kanana-1.5-8b-instruct-2505\"\n\nprint(f\"Loading model: {model_name}\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\"Model loaded successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAG training data\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "data_path = \"/home/user/goodganglabs/data/processed/training_data.jsonl\"\n",
    "\n",
    "# Load JSONL data\n",
    "data_list = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data_list.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data_list)} training examples\")\n",
    "print(\"\\nFirst example:\")\n",
    "print(json.dumps(data_list[0], ensure_ascii=False, indent=2)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert to requested RAG format with instruction\ndef format_rag_data(example):\n    \"\"\"Convert RAG data to the requested format with instruction:\n    [Instruction]\n    당신은 제천시 관광 안내 전문가입니다...\n    \n    Information:\n    {content1}\n    Information:\n    {content2}\n    Question: {question}\n    \"\"\"\n    instruction = \"\"\"당신은 제천시 관광 안내 전문가입니다.\n제공된 여러 문서 중에서 질문과 관련된 문서를 찾아, 그 문서의 내용을 바탕으로 정확하고 친절하게 답변해주세요.\n\n답변 시 주의사항:\n1. 관련 문서의 내용만을 바탕으로 답변하세요\n2. 문서에 정보가 없으면 \"제공된 정보에는 해당 내용이 없습니다\"라고 답변하세요\n3. 추측하거나 문서 외부 지식을 사용하지 마세요\n4. 간결하고 이해하기 쉽게 답변하세요\"\"\"\n    \n    documents = example['documents']\n    question = example['question']\n    answer = example['answer']\n    \n    # Build information sections\n    info_sections = []\n    for doc in documents:\n        info_sections.append(f\"Information:\\n{doc['content']}\")\n    \n    # Combine: instruction + information sections + question\n    prompt = instruction + \"\\n\\n\"\n    prompt += \"\\n\\n\".join(info_sections)\n    prompt += f\"\\n\\nQuestion: {question}\"\n    \n    return {\n        \"prompt\": prompt,\n        \"answer\": answer\n    }\n\n# Apply formatting\nformatted_data = []\nfor example in data_list:\n    formatted_data.append(format_rag_data(example))\n\nprint(f\"Formatted {len(formatted_data)} examples\")\nprint(\"\\nExample formatted prompt:\")\nprint(formatted_data[0]['prompt'][:500])\nprint(f\"\\nAnswer: {formatted_data[0]['answer']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset with proper format\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format prompts for training\"\"\"\n",
    "    prompts = examples[\"prompt\"]\n",
    "    answers = examples[\"answer\"]\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    texts = []\n",
    "    for prompt, answer in zip(prompts, answers):\n",
    "        # Combine prompt and answer with EOS token\n",
    "        text = f\"{prompt}\\n\\nAnswer: {answer}{EOS_TOKEN}\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"\\nDataset features: {dataset.features}\")\n",
    "print(f\"\\nFirst training example:\")\n",
    "print(dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"text\", \"prompt\", \"answer\"]\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset size: {len(tokenized_dataset)}\")\n",
    "print(f\"Features: {tokenized_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for parameter-efficient fine-tuning\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,                    # LoRA rank\n",
    "    lora_alpha=32,          # LoRA alpha\n",
    "    lora_dropout=0.1,       # Dropout probability\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Log LoRA config to wandb\n",
    "wandb.config.update({\n",
    "    \"lora_r\": lora_config.r,\n",
    "    \"lora_alpha\": lora_config.lora_alpha,\n",
    "    \"lora_dropout\": lora_config.lora_dropout,\n",
    "    \"target_modules\": lora_config.target_modules\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "# Log to wandb\n",
    "wandb.config.update({\n",
    "    \"gpu_name\": gpu_stats.name,\n",
    "    \"gpu_max_memory_gb\": max_memory,\n",
    "    \"gpu_start_memory_gb\": start_gpu_memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments with wandb integration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output settings\n",
    "    output_dir=\"./outputs/kanana-rag\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=10,\n",
    "    \n",
    "    # Optimization\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"wandb\",  # Enable wandb reporting\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"no\",  # No validation set in this example\n",
    "    \n",
    "    # Other\n",
    "    seed=1234,\n",
    "    data_seed=1234,\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Total steps: ~{len(tokenized_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final GPU memory usage\n",
    "final_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"\\nFinal GPU memory: {final_gpu_memory} GB\")\n",
    "print(f\"Peak memory used: {final_gpu_memory - start_gpu_memory} GB\")\n",
    "\n",
    "# Log final stats to wandb\n",
    "wandb.log({\n",
    "    \"final_gpu_memory_gb\": final_gpu_memory,\n",
    "    \"peak_memory_used_gb\": final_gpu_memory - start_gpu_memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./outputs/kanana-rag-final\"\n",
    "\n",
    "print(f\"Saving model to {output_dir}...\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"\\nModel location: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with a sample\n",
    "print(\"Testing inference...\\n\")\n",
    "\n",
    "# Get a test example\n",
    "test_prompt = formatted_data[0]['prompt']\n",
    "expected_answer = formatted_data[0]['answer']\n",
    "\n",
    "print(\"Test Prompt:\")\n",
    "print(test_prompt[:300])\n",
    "print(\"\\n...\")\n",
    "\n",
    "# Tokenize and generate\n",
    "model.eval()\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(generated_text[len(test_prompt):])\n",
    "\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(expected_answer)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Create detailed examples table for wandb\nexamples_data = []\n\nfor i in range(min(5, len(test_samples))):\n    examples_data.append([\n        test_samples[i]['question'],\n        test_samples[i]['answer'],\n        predictions[i]['answer'],\n        test_samples[i].get('question_type', 'unknown')\n    ])\n\nexamples_table = wandb.Table(\n    columns=[\"Question\", \"Reference Answer\", \"Generated Answer\", \"Question Type\"],\n    data=examples_data\n)\n\nwandb.log({\"eval/detailed_examples\": examples_table})\n\nprint(\"✓ Example predictions logged to WandB\")\nprint(\"\\nView your results at: https://wandb.ai\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Run generation-only evaluation (no docs needed!)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Running generation-only evaluation...\")\nprint(\"=\" * 60 + \"\\n\")\n\n# Evaluate - only needs 'answer' field in predictions and dataset\nresults = evaluator.evaluate_generation_only(\n    dataset=test_samples,\n    model_predictions=predictions\n)\n\n# Display results\nprint(evaluator.format_results_generation_only(results))\n\n# Log to wandb\nwandb.log({\n    \"eval/rouge1\": results['rouge1'],\n    \"eval/rouge2\": results['rouge2'],\n    \"eval/rougeL\": results['rougeL'],\n    \"eval/bert_f1\": results['bert_f1'],\n    \"eval/bert_precision\": results['bert_precision'],\n    \"eval/bert_recall\": results['bert_recall'],\n    \"eval/exact_match\": results['exact_match'],\n    \"eval/num_samples\": results['num_samples']\n})\n\nprint(\"\\n✓ Results logged to WandB\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Generate predictions on test set\nprint(\"Generating predictions on test set...\")\nprint(\"=\" * 60)\n\n# Use first 30 samples for evaluation (or load separate test set)\ntest_samples = data_list[:30]  \npredictions = []\n\nmodel.eval()\nwith torch.no_grad():\n    for i, sample in enumerate(test_samples):\n        # Format prompt\n        formatted = format_rag_data(sample)\n        prompt = formatted['prompt']\n        \n        # Generate answer\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        \n        # Extract generated text\n        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        answer = generated[len(prompt):].strip()\n        \n        # Remove \"Answer:\" prefix if present\n        if answer.startswith(\"Answer:\"):\n            answer = answer[7:].strip()\n        \n        predictions.append({\n            'answer': answer\n        })\n        \n        if (i + 1) % 10 == 0:\n            print(f\"  Generated {i + 1}/{len(test_samples)} predictions...\")\n\nprint(f\"\\n✓ Generated {len(predictions)} predictions\")\n\n# Show example\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Example prediction:\")\nprint(\"=\" * 60)\nprint(f\"Question: {test_samples[0]['question']}\")\nprint(f\"\\nGenerated: {predictions[0]['answer'][:200]}...\")\nprint(f\"\\nReference: {test_samples[0]['answer'][:200]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Import evaluation metrics\nimport sys\nsys.path.insert(0, '/home/user/goodganglabs')\n\nfrom src.evaluation.metrics import create_evaluator\n\n# Create evaluator (no k_values needed for generation-only)\nevaluator = create_evaluator()\n\nprint(\"✓ Evaluator initialized\")\nprint(\"  Available metrics: ROUGE, BERTScore, Exact Match\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Install evaluation dependencies if needed\n!pip install -q rouge-score bert-score",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation with Generation-Only Metrics\n\nNow let's evaluate the fine-tuned model using generation-only metrics (ROUGE, BERTScore, Exact Match).\n\n**No document retrieval metrics needed** - we only compare generated answers vs ground truth.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close wandb run\n",
    "wandb.finish()\n",
    "print(\"WandB run finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook:\n1. ✅ Loads Kanana 1.5 8B instruct model (kakaocorp/kanana-1.5-8b-instruct-2505)\n2. ✅ Formats RAG training data with instruction and information format:\n   ```\n   [Instruction: 제천시 관광 안내 전문가 역할]\n   \n   Information:\n   {content1}\n   Information:\n   {content2}\n   Question: {question}\n   \n   Answer: {answer}\n   ```\n3. ✅ Applies LoRA for efficient fine-tuning\n4. ✅ Tracks training with Weights & Biases\n5. ✅ Saves the fine-tuned model\n6. ✅ Tests inference on sample data\n\n### Key Features:\n- System instruction guides the model to be a Jecheon tourism expert\n- Model learns to find relevant documents and answer based on them\n- Model learns to say \"no information available\" when appropriate\n- Training format prevents hallucination\n\n### Next Steps:\n- Run full evaluation on test set\n- Compare baseline vs fine-tuned performance\n- Upload model to Hugging Face Hub\n- Generate report with metrics and examples"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}