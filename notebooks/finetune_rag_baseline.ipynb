{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Baseline Fine-tuning with Bench-RAG Evaluation\n",
    "\n",
    "This notebook fine-tunes a Korean LLM (kanana-nano-2.1b) for RAG tasks with comprehensive evaluation metrics.\n",
    "\n",
    "**Features:**\n",
    "- LoRA fine-tuning for efficient training\n",
    "- Weights & Biases (wandb) tracking\n",
    "- ROUGE and BERTScore evaluation\n",
    "- Bench-RAG evaluation system\n",
    "- Modular evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers peft datasets accelerate\n",
    "!pip install -q wandb rouge-score bert-score\n",
    "!pip install -q sentencepiece  # For tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set CUDA device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.login()\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    \"model_name\": \"kakaocorp/kanana-nano-2.1b-base\",\n",
    "    \"dataset\": \"jecheon_rag_training\",\n",
    "    \"task\": \"rag_finetuning\",\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 5,\n",
    "    \"max_steps\": 100,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 20,\n",
    "}\n",
    "\n",
    "# Start wandb run\n",
    "wandb.init(\n",
    "    project=\"goodganglabs-rag\",\n",
    "    name=\"rag-baseline-kanana-nano\",\n",
    "    config=config,\n",
    "    tags=[\"rag\", \"kanana-nano\", \"lora\", \"bench-rag\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = config[\"model_name\"]\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading model: {model_name}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side=\"left\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Set pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úì Model loaded successfully\")\n",
    "print(f\"‚úì Tokenizer vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Load training data\n",
    "data_path = project_root / \"data\" / \"processed\" / \"training_data.jsonl\"\n",
    "print(f\"Loading training data from: {data_path}\")\n",
    "\n",
    "# Read JSONL file\n",
    "data_samples = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data_samples.append(json.loads(line))\n",
    "\n",
    "print(f\"‚úì Loaded {len(data_samples)} training samples\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample training sample:\")\n",
    "print(json.dumps(data_samples[0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rag_prompt(sample):\n",
    "    \"\"\"\n",
    "    Format RAG training sample into instruction-following format\n",
    "    \"\"\"\n",
    "    # Get relevant documents\n",
    "    documents = sample.get('documents', [])\n",
    "    \n",
    "    # Find correct document(s)\n",
    "    correct_docs = [doc for doc in documents if doc.get('is_correct', False)]\n",
    "    \n",
    "    # Build context from correct documents\n",
    "    if correct_docs:\n",
    "        context_parts = []\n",
    "        for doc in correct_docs:\n",
    "            title = doc.get('title', '')\n",
    "            content = doc.get('content', '')\n",
    "            if title and content:\n",
    "                context_parts.append(f\"[{title}]\\n{content}\")\n",
    "            elif content:\n",
    "                context_parts.append(content)\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "    else:\n",
    "        # Use all documents if no correct document is marked\n",
    "        context = \"\\n\\n\".join([doc.get('content', '') for doc in documents[:3]])\n",
    "    \n",
    "    # Create instruction-following format\n",
    "    instruction = \"Ï£ºÏñ¥ÏßÑ Î¨∏ÏÑú ÎÇ¥Ïö©ÏùÑ Î∞îÌÉïÏúºÎ°ú ÏßàÎ¨∏Ïóê ÎãµÌïòÏÑ∏Ïöî.\"\n",
    "    question = sample['question']\n",
    "    answer = sample['answer']\n",
    "    \n",
    "    prompt = f\"\"\"Îã§ÏùåÏùÄ ÏßàÎ¨∏Ïóê ÎãµÌïòÍ∏∞ ÏúÑÌïú Î¨∏ÏÑúÏûÖÎãàÎã§:\n",
    "\n",
    "{context}\n",
    "\n",
    "### ÏßàÎ¨∏:\n",
    "{question}\n",
    "\n",
    "### ÎãµÎ≥Ä:\n",
    "{answer}\"\"\"\n",
    "    \n",
    "    return prompt + tokenizer.eos_token\n",
    "\n",
    "\n",
    "def preprocess_dataset(samples):\n",
    "    \"\"\"\n",
    "    Preprocess dataset for training\n",
    "    \"\"\"\n",
    "    # Format prompts\n",
    "    texts = [format_rag_prompt(sample) for sample in samples]\n",
    "    \n",
    "    # Create dataset with metadata\n",
    "    dataset_dict = {\n",
    "        'text': texts,\n",
    "        'question': [s['question'] for s in samples],\n",
    "        'answer': [s['answer'] for s in samples],\n",
    "        'question_type': [s.get('question_type', 'unknown') for s in samples],\n",
    "    }\n",
    "    \n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = preprocess_dataset(data_samples)\n",
    "print(f\"‚úì Created dataset with {len(train_dataset)} samples\")\n",
    "print(f\"\\nDataset features: {train_dataset.features}\")\n",
    "\n",
    "# Show formatted example\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example formatted prompt:\")\n",
    "print(\"=\"*60)\n",
    "print(train_dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text samples\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Tokenize dataset\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\", \"question\", \"answer\", \"question_type\"]\n",
    ")\n",
    "\n",
    "print(f\"‚úì Tokenization complete\")\n",
    "print(f\"  Tokenized features: {tokenized_dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=config[\"lora_r\"],\n",
    "    lora_alpha=config[\"lora_alpha\"],\n",
    "    lora_dropout=config[\"lora_dropout\"],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"‚úì LoRA configuration applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Setup Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation modules\n",
    "from src.evaluation.metrics import (\n",
    "    GenerationMetrics,\n",
    "    BenchRAGEvaluator,\n",
    "    create_evaluator\n",
    ")\n",
    "\n",
    "# Initialize evaluators\n",
    "generation_metrics = GenerationMetrics()\n",
    "bench_rag_evaluator = create_evaluator(k_values=[1, 3, 5])\n",
    "\n",
    "print(\"‚úì Evaluation metrics initialized\")\n",
    "print(\"  - ROUGE scores\")\n",
    "print(\"  - BERTScore\")\n",
    "print(\"  - Bench-RAG metrics (Recall@K, NDCG@K, MRR)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Custom Evaluation Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "class RAGEvaluationCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for RAG evaluation during training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eval_samples, generation_metrics, tokenizer):\n",
    "        self.eval_samples = eval_samples\n",
    "        self.generation_metrics = generation_metrics\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, model, metrics=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Called after evaluation phase\n",
    "        \"\"\"\n",
    "        if metrics is None:\n",
    "            return\n",
    "        \n",
    "        # Generate predictions for evaluation samples\n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sample in self.eval_samples[:10]:  # Evaluate on first 10 samples\n",
    "                # Format prompt (without answer)\n",
    "                documents = sample.get('documents', [])\n",
    "                correct_docs = [doc for doc in documents if doc.get('is_correct', False)]\n",
    "                \n",
    "                if correct_docs:\n",
    "                    context = correct_docs[0].get('content', '')\n",
    "                else:\n",
    "                    context = documents[0].get('content', '') if documents else ''\n",
    "                \n",
    "                prompt = f\"\"\"Îã§ÏùåÏùÄ ÏßàÎ¨∏Ïóê ÎãµÌïòÍ∏∞ ÏúÑÌïú Î¨∏ÏÑúÏûÖÎãàÎã§:\n",
    "\n",
    "{context}\n",
    "\n",
    "### ÏßàÎ¨∏:\n",
    "{sample['question']}\n",
    "\n",
    "### ÎãµÎ≥Ä:\n",
    "\"\"\"\n",
    "                \n",
    "                # Generate answer\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                generated_text = self.tokenizer.decode(\n",
    "                    outputs[0][inputs['input_ids'].shape[1]:],\n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                \n",
    "                predictions.append(generated_text.strip())\n",
    "                references.append(sample['answer'])\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        rouge_scores = []\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            scores = self.generation_metrics.rouge_scores(pred, ref)\n",
    "            rouge_scores.append(scores)\n",
    "        \n",
    "        # Average ROUGE scores\n",
    "        avg_rouge = {\n",
    "            'rouge1': np.mean([s['rouge1'] for s in rouge_scores]),\n",
    "            'rouge2': np.mean([s['rouge2'] for s in rouge_scores]),\n",
    "            'rougeL': np.mean([s['rougeL'] for s in rouge_scores]),\n",
    "        }\n",
    "        \n",
    "        # Compute BERTScore\n",
    "        bert_scores = self.generation_metrics.bert_score(predictions, references)\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log({\n",
    "            \"eval/rouge1\": avg_rouge['rouge1'],\n",
    "            \"eval/rouge2\": avg_rouge['rouge2'],\n",
    "            \"eval/rougeL\": avg_rouge['rougeL'],\n",
    "            \"eval/bert_f1\": bert_scores['bert_f1'],\n",
    "            \"eval/bert_precision\": bert_scores['bert_precision'],\n",
    "            \"eval/bert_recall\": bert_scores['bert_recall'],\n",
    "        }, step=state.global_step)\n",
    "        \n",
    "        # Log example predictions\n",
    "        if state.global_step % 40 == 0:  # Log examples every 40 steps\n",
    "            example_table = wandb.Table(\n",
    "                columns=[\"Question\", \"Reference\", \"Prediction\", \"ROUGE-L\"],\n",
    "                data=[\n",
    "                    [q, r, p, s['rougeL']]\n",
    "                    for q, r, p, s in list(zip(\n",
    "                        [s['question'] for s in self.eval_samples[:3]],\n",
    "                        references[:3],\n",
    "                        predictions[:3],\n",
    "                        rouge_scores[:3]\n",
    "                    ))\n",
    "                ]\n",
    "            )\n",
    "            wandb.log({\"eval/examples\": example_table}, step=state.global_step)\n",
    "        \n",
    "        print(f\"\\nüìä Evaluation Metrics (Step {state.global_step}):\")\n",
    "        print(f\"  ROUGE-1: {avg_rouge['rouge1']:.4f}\")\n",
    "        print(f\"  ROUGE-2: {avg_rouge['rouge2']:.4f}\")\n",
    "        print(f\"  ROUGE-L: {avg_rouge['rougeL']:.4f}\")\n",
    "        print(f\"  BERTScore F1: {bert_scores['bert_f1']:.4f}\")\n",
    "\n",
    "\n",
    "# Create callback instance\n",
    "eval_callback = RAGEvaluationCallback(\n",
    "    eval_samples=data_samples[:20],  # Use first 20 samples for evaluation\n",
    "    generation_metrics=generation_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"‚úì Custom evaluation callback created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Training arguments with wandb integration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs/rag_baseline\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    per_device_train_batch_size=config[\"batch_size\"],\n",
    "    gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    warmup_steps=config[\"warmup_steps\"],\n",
    "    max_steps=config[\"max_steps\"],\n",
    "    \n",
    "    # Optimization\n",
    "    bf16=True,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    "    logging_first_step=True,\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy=config[\"evaluation_strategy\"],\n",
    "    eval_steps=config[\"eval_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=config[\"eval_steps\"],\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Wandb integration\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"rag-baseline-kanana-nano\",\n",
    "    \n",
    "    # Misc\n",
    "    seed=1234,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(\"‚úì Training arguments configured\")\n",
    "print(f\"  Total steps: {config['max_steps']}\")\n",
    "print(f\"  Batch size: {config['batch_size']} x {config['gradient_accumulation_steps']} (accumulation)\")\n",
    "print(f\"  Effective batch size: {config['batch_size'] * config['gradient_accumulation_steps']}\")\n",
    "print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "print(f\"  Evaluation every: {config['eval_steps']} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer with custom callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset.select(range(min(20, len(tokenized_dataset)))),  # Small eval set\n",
    "    callbacks=[eval_callback],\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized with RAG evaluation callback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. GPU Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    \n",
    "    print(f\"GPU: {gpu_stats.name}\")\n",
    "    print(f\"Max memory: {max_memory} GB\")\n",
    "    print(f\"Reserved memory: {start_gpu_memory} GB\")\n",
    "    print(f\"Available memory: {max_memory - start_gpu_memory:.3f} GB\")\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        \"system/gpu_name\": gpu_stats.name,\n",
    "        \"system/gpu_memory_total_gb\": max_memory,\n",
    "        \"system/gpu_memory_reserved_gb\": start_gpu_memory,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training completed!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total training time: {trainer_stats.metrics['train_runtime']:.2f}s\")\n",
    "print(f\"Training loss: {trainer_stats.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Final Evaluation with Bench-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä Running final Bench-RAG evaluation...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Generate predictions for test set\n",
    "test_samples = data_samples[:30]  # Use first 30 for comprehensive evaluation\n",
    "predictions = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, sample in enumerate(test_samples):\n",
    "        # Format prompt\n",
    "        documents = sample.get('documents', [])\n",
    "        correct_docs = [doc for doc in documents if doc.get('is_correct', False)]\n",
    "        \n",
    "        if correct_docs:\n",
    "            context = correct_docs[0].get('content', '')\n",
    "            correct_doc_id = correct_docs[0].get('doc_id', '')\n",
    "        else:\n",
    "            context = documents[0].get('content', '') if documents else ''\n",
    "            correct_doc_id = documents[0].get('doc_id', '') if documents else ''\n",
    "        \n",
    "        prompt = f\"\"\"Îã§ÏùåÏùÄ ÏßàÎ¨∏Ïóê ÎãµÌïòÍ∏∞ ÏúÑÌïú Î¨∏ÏÑúÏûÖÎãàÎã§:\n",
    "\n",
    "{context}\n",
    "\n",
    "### ÏßàÎ¨∏:\n",
    "{sample['question']}\n",
    "\n",
    "### ÎãµÎ≥Ä:\n",
    "\"\"\"\n",
    "        \n",
    "        # Generate\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        generated = tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        # For Bench-RAG, assume retrieved docs are the correct ones\n",
    "        # (In real scenario, you'd use actual retrieval results)\n",
    "        retrieved_doc_ids = [doc['doc_id'] for doc in documents[:3]]\n",
    "        \n",
    "        predictions.append({\n",
    "            'answer': generated,\n",
    "            'retrieved_doc_ids': retrieved_doc_ids\n",
    "        })\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(test_samples)} samples...\")\n",
    "\n",
    "# Run Bench-RAG evaluation\n",
    "bench_rag_results = bench_rag_evaluator.evaluate_dataset(\n",
    "    dataset=test_samples,\n",
    "    model_predictions=predictions\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + bench_rag_evaluator.format_results(bench_rag_results))\n",
    "\n",
    "# Log to wandb\n",
    "wandb.log({f\"bench_rag/{k}\": v for k, v in bench_rag_results.items()})\n",
    "\n",
    "# Log example predictions\n",
    "examples_table = wandb.Table(\n",
    "    columns=[\"Question\", \"Reference\", \"Prediction\", \"Question Type\"],\n",
    "    data=[\n",
    "        [\n",
    "            test_samples[i]['question'],\n",
    "            test_samples[i]['answer'],\n",
    "            predictions[i]['answer'],\n",
    "            test_samples[i].get('question_type', 'unknown')\n",
    "        ]\n",
    "        for i in range(min(10, len(test_samples)))\n",
    "    ]\n",
    ")\n",
    "wandb.log({\"final_evaluation/examples\": examples_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine-tuned model\n",
    "output_dir = \"./models/rag_baseline_finetuned\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úì Model saved to: {output_dir}\")\n",
    "\n",
    "# Save to wandb\n",
    "artifact = wandb.Artifact(\n",
    "    name=\"rag-baseline-model\",\n",
    "    type=\"model\",\n",
    "    description=\"Fine-tuned RAG baseline model (kanana-nano-2.1b)\"\n",
    ")\n",
    "artifact.add_dir(output_dir)\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "print(\"‚úì Model uploaded to wandb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Finish Wandb Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ All done! Check your wandb dashboard for detailed metrics.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. ‚úÖ Fine-tuned kanana-nano-2.1b for RAG tasks using LoRA\n",
    "2. ‚úÖ Tracked training with Weights & Biases\n",
    "3. ‚úÖ Evaluated with ROUGE and BERTScore metrics\n",
    "4. ‚úÖ Implemented Bench-RAG evaluation system\n",
    "5. ‚úÖ Used modular, reusable evaluation functions\n",
    "\n",
    "**Next Steps:**\n",
    "- Compare with baseline (untrained) model\n",
    "- Analyze performance by question type\n",
    "- Upload model to Hugging Face Hub\n",
    "- Create detailed evaluation report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
