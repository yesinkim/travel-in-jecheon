{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kanana RAG Fine-tuning with Evaluation Metrics\n",
    "\n",
    "This notebook fine-tunes Kanana 8B on RAG tasks and tracks multiple RAG-specific metrics:\n",
    "\n",
    "**Metrics tracked:**\n",
    "1. **Train Loss** - Standard training loss\n",
    "2. **Context Precision@3** - Correct document in top-3 retrieved docs\n",
    "3. **Context Recall** - Correct document retrieved at all\n",
    "4. **ROUGE-L** - Lexical similarity with reference answer\n",
    "5. **BERTScore** - Semantic similarity with reference answer\n",
    "6. **Answer Relevance** - Answer relevance to question\n",
    "\n",
    "All metrics are logged to Weights & Biases during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers peft datasets wandb bitsandbytes accelerate\n",
    "!pip install -q rouge-score bert-score evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"NVIDIA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"kanana-rag-finetuning\",\n",
    "    name=\"kanana-rag-with-metrics\",\n",
    "    config={\n",
    "        \"model\": \"kakaocorp/kanana-1.5-8b-instruct-2505\",\n",
    "        \"task\": \"RAG fine-tuning with evaluation metrics\",\n",
    "        \"dataset\": \"Jecheon Tourism\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and split into train/test\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = \"/home/user/goodganglabs/data/processed/training_data.jsonl\"\n",
    "\n",
    "# Load all data\n",
    "data_list = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data_list.append(json.loads(line))\n",
    "\n",
    "# Split 80/20\n",
    "train_data, test_data = train_test_split(\n",
    "    data_list, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Total: {len(data_list)} examples\")\n",
    "print(f\"Train: {len(train_data)} examples\")\n",
    "print(f\"Test: {len(test_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_name = \"kakaocorp/kanana-1.5-8b-instruct-2505\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RAG Metrics\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score_fn\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "class RAGMetrics:\n",
    "    def __init__(self):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "    \n",
    "    def context_precision_at_k(self, documents: List[Dict], correct_doc_id: str, k: int = 3) -> float:\n",
    "        \"\"\"Correct document in top-k?\"\"\"\n",
    "        if not correct_doc_id:\n",
    "            return 0.0\n",
    "        doc_ids = [doc['doc_id'] for doc in documents[:k]]\n",
    "        return 1.0 if correct_doc_id in doc_ids else 0.0\n",
    "    \n",
    "    def context_recall(self, documents: List[Dict], correct_doc_id: str) -> float:\n",
    "        \"\"\"Correct document retrieved?\"\"\"\n",
    "        if not correct_doc_id:\n",
    "            return 0.0\n",
    "        doc_ids = [doc['doc_id'] for doc in documents]\n",
    "        return 1.0 if correct_doc_id in doc_ids else 0.0\n",
    "    \n",
    "    def rouge_l(self, prediction: str, reference: str) -> float:\n",
    "        \"\"\"ROUGE-L F1 score\"\"\"\n",
    "        scores = self.rouge_scorer.score(reference, prediction)\n",
    "        return scores['rougeL'].fmeasure\n",
    "    \n",
    "    def bert_score(self, predictions: List[str], references: List[str]) -> float:\n",
    "        \"\"\"BERTScore F1 (batched)\"\"\"\n",
    "        if not predictions or not references:\n",
    "            return 0.0\n",
    "        P, R, F1 = bert_score_fn(\n",
    "            predictions, references,\n",
    "            lang='ko', verbose=False,\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "        return F1.mean().item()\n",
    "    \n",
    "    def answer_relevance(self, answer: str, question: str) -> float:\n",
    "        \"\"\"Simple keyword overlap-based relevance\"\"\"\n",
    "        if not answer or len(answer.strip()) < 10:\n",
    "            return 0.0\n",
    "        \n",
    "        stop_words = {'은', '는', '이', '가', '을', '를', '의', '에', '에서', '로', '으로', '와', '과', '도', '만', '?'}\n",
    "        q_words = set(question.split()) - stop_words\n",
    "        a_words = set(answer.split()) - stop_words\n",
    "        \n",
    "        if not q_words:\n",
    "            return 1.0\n",
    "        \n",
    "        overlap = len(q_words & a_words) / len(q_words)\n",
    "        return min(overlap, 1.0)\n",
    "\n",
    "rag_metrics = RAGMetrics()\n",
    "print(\"RAG Metrics initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for training\n",
    "from datasets import Dataset\n",
    "\n",
    "INSTRUCTION = \"\"\"당신은 제천시 관광 안내 전문가입니다.\n",
    "제공된 여러 문서 중에서 질문과 관련된 문서를 찾아, 그 문서의 내용을 바탕으로 정확하고 친절하게 답변해주세요.\n",
    "\n",
    "답변 시 주의사항:\n",
    "1. 관련 문서의 내용만을 바탕으로 답변하세요\n",
    "2. 문서에 정보가 없으면 \"제공된 정보에는 해당 내용이 없습니다\"라고 답변하세요\n",
    "3. 추측하거나 문서 외부 지식을 사용하지 마세요\n",
    "4. 간결하고 이해하기 쉽게 답변하세요\"\"\"\n",
    "\n",
    "def format_example(example):\n",
    "    # Build prompt\n",
    "    info_sections = [f\"Information:\\n{doc['content']}\" for doc in example['documents']]\n",
    "    prompt = INSTRUCTION + \"\\n\\n\" + \"\\n\\n\".join(info_sections) + f\"\\n\\nQuestion: {example['question']}\"\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"answer\": example['answer'],\n",
    "        \"question\": example['question'],\n",
    "        \"documents\": example['documents'],\n",
    "        \"correct_doc_id\": example.get('correct_doc_id', None)\n",
    "    }\n",
    "\n",
    "# Format train and test\n",
    "train_formatted = [format_example(ex) for ex in train_data]\n",
    "test_formatted = [format_example(ex) for ex in test_data]\n",
    "\n",
    "# Create training dataset with text column\n",
    "train_texts = [f\"{ex['prompt']}\\n\\nAnswer: {ex['answer']}{tokenizer.eos_token}\" for ex in train_formatted]\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
    "\n",
    "print(f\"Train: {len(train_formatted)}, Test: {len(test_formatted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize training data\n",
    "def tokenize(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "train_dataset_tokenized = train_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(f\"Tokenized training set: {len(train_dataset_tokenized)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function (called periodically during training)\n",
    "def evaluate_rag(model, test_data, tokenizer, num_samples=20):\n",
    "    \"\"\"Evaluate model on test set and return metrics\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Sample subset for faster evaluation\n",
    "    import random\n",
    "    samples = random.sample(test_data, min(num_samples, len(test_data)))\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    context_precisions = []\n",
    "    context_recalls = []\n",
    "    answer_relevances = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        # Generate answer\n",
    "        inputs = tokenizer(sample['prompt'], return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extract answer part (after \"Answer:\")\n",
    "        if \"Answer:\" in generated:\n",
    "            pred_answer = generated.split(\"Answer:\")[-1].strip()\n",
    "        else:\n",
    "            pred_answer = generated[len(sample['prompt']):].strip()\n",
    "        \n",
    "        predictions.append(pred_answer)\n",
    "        references.append(sample['answer'])\n",
    "        \n",
    "        # Compute context metrics\n",
    "        if sample['correct_doc_id']:\n",
    "            context_precisions.append(\n",
    "                rag_metrics.context_precision_at_k(sample['documents'], sample['correct_doc_id'], k=3)\n",
    "            )\n",
    "            context_recalls.append(\n",
    "                rag_metrics.context_recall(sample['documents'], sample['correct_doc_id'])\n",
    "            )\n",
    "        \n",
    "        # Compute answer relevance\n",
    "        answer_relevances.append(\n",
    "            rag_metrics.answer_relevance(pred_answer, sample['question'])\n",
    "        )\n",
    "    \n",
    "    # Compute ROUGE-L (average over samples)\n",
    "    rouge_scores = [rag_metrics.rouge_l(p, r) for p, r in zip(predictions, references)]\n",
    "    \n",
    "    # Compute BERTScore (batched)\n",
    "    bert_score = rag_metrics.bert_score(predictions, references)\n",
    "    \n",
    "    metrics = {\n",
    "        \"eval_context_precision@3\": np.mean(context_precisions) if context_precisions else 0.0,\n",
    "        \"eval_context_recall\": np.mean(context_recalls) if context_recalls else 0.0,\n",
    "        \"eval_rouge_l\": np.mean(rouge_scores),\n",
    "        \"eval_bert_score\": bert_score,\n",
    "        \"eval_answer_relevance\": np.mean(answer_relevances)\n",
    "    }\n",
    "    \n",
    "    model.train()\n",
    "    return metrics\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom callback for periodic evaluation\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class RAGEvaluationCallback(TrainerCallback):\n",
    "    def __init__(self, test_data, tokenizer, eval_steps=50):\n",
    "        self.test_data = test_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eval_steps = eval_steps\n",
    "    \n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        # Evaluate every eval_steps\n",
    "        if state.global_step % self.eval_steps == 0 and state.global_step > 0:\n",
    "            print(f\"\\n[Step {state.global_step}] Running RAG evaluation...\")\n",
    "            metrics = evaluate_rag(model, self.test_data, self.tokenizer, num_samples=15)\n",
    "            \n",
    "            # Log to wandb\n",
    "            wandb.log({**metrics, \"step\": state.global_step})\n",
    "            \n",
    "            print(f\"Context Precision@3: {metrics['eval_context_precision@3']:.3f}\")\n",
    "            print(f\"Context Recall: {metrics['eval_context_recall']:.3f}\")\n",
    "            print(f\"ROUGE-L: {metrics['eval_rouge_l']:.3f}\")\n",
    "            print(f\"BERTScore: {metrics['eval_bert_score']:.3f}\")\n",
    "            print(f\"Answer Relevance: {metrics['eval_answer_relevance']:.3f}\\n\")\n",
    "\n",
    "eval_callback = RAGEvaluationCallback(test_formatted, tokenizer, eval_steps=50)\n",
    "print(\"Evaluation callback created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs/kanana-rag-metrics\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=10,\n",
    "    bf16=True,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"wandb\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    callbacks=[eval_callback]  # Add evaluation callback\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized with RAG evaluation callback!\")\n",
    "print(f\"Will evaluate every {eval_callback.eval_steps} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Starting training with RAG metrics tracking...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final train loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final full evaluation on test set\n",
    "print(\"Running final evaluation on full test set...\\n\")\n",
    "\n",
    "final_metrics = evaluate_rag(model, test_formatted, tokenizer, num_samples=len(test_formatted))\n",
    "\n",
    "print(\"Final Test Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Context Precision@3: {final_metrics['eval_context_precision@3']:.4f}\")\n",
    "print(f\"Context Recall:      {final_metrics['eval_context_recall']:.4f}\")\n",
    "print(f\"ROUGE-L:             {final_metrics['eval_rouge_l']:.4f}\")\n",
    "print(f\"BERTScore:           {final_metrics['eval_bert_score']:.4f}\")\n",
    "print(f\"Answer Relevance:    {final_metrics['eval_answer_relevance']:.4f}\")\n",
    "\n",
    "# Log to wandb\n",
    "wandb.log({f\"final_{k}\": v for k, v in final_metrics.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "output_dir = \"./outputs/kanana-rag-final\"\n",
    "print(f\"Saving model to {output_dir}...\")\n",
    "\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "print(\"Testing inference on sample...\\n\")\n",
    "\n",
    "test_sample = test_formatted[0]\n",
    "inputs = tokenizer(test_sample['prompt'], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Question:\", test_sample['question'])\n",
    "print(\"\\nGenerated Answer:\")\n",
    "if \"Answer:\" in generated:\n",
    "    print(generated.split(\"Answer:\")[-1].strip())\n",
    "else:\n",
    "    print(generated[len(test_sample['prompt']):].strip())\n",
    "print(\"\\nReference Answer:\")\n",
    "print(test_sample['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close wandb\n",
    "wandb.finish()\n",
    "print(\"WandB run finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "\n",
    "### Training\n",
    "- ✅ Loads Kanana 1.5 8B instruct model\n",
    "- ✅ Splits data into train (80%) / test (20%)\n",
    "- ✅ Applies LoRA for efficient fine-tuning\n",
    "- ✅ Trains with standard cross-entropy loss\n",
    "\n",
    "### Evaluation Metrics (tracked during training)\n",
    "1. **Train Loss** - Standard language modeling loss\n",
    "2. **Context Precision@3** - How often the correct document appears in top-3 retrieved docs\n",
    "3. **Context Recall** - How often the correct document is retrieved at all\n",
    "4. **ROUGE-L** - Lexical overlap between generated and reference answers\n",
    "5. **BERTScore** - Semantic similarity using Korean BERT embeddings\n",
    "6. **Answer Relevance** - Whether answer contains relevant keywords from question\n",
    "\n",
    "### Logging\n",
    "- ✅ All metrics logged to Weights & Biases\n",
    "- ✅ Evaluation runs every 50 training steps\n",
    "- ✅ Final full evaluation on complete test set\n",
    "\n",
    "### Next Steps\n",
    "- Compare baseline (pre-trained) vs fine-tuned metrics\n",
    "- Analyze failure cases\n",
    "- Upload model to Hugging Face Hub\n",
    "- Generate report with metric visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
