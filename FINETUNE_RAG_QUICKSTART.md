# Finetune-RAG for KANANA1.5-8B - Quick Start Guide

ì´ ê°€ì´ë“œëŠ” KANANA1.5-8B ëª¨ë¸ì„ Finetune-RAG ë°©ë²•ë¡ ìœ¼ë¡œ ë¹ ë¥´ê²Œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸš€ 5ë¶„ Quick Start

### 1. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# UV ì‚¬ìš© (ê¶Œì¥)
uv sync

# ë˜ëŠ” pip ì‚¬ìš©
pip install -r requirements.txt
```

### 2. ìƒ˜í”Œ ë°ì´í„°ì…‹ ì¤€ë¹„

```bash
bash scripts/prepare_sample_dataset.sh
```

### 3. í•™ìŠµ ì‹¤í–‰

```bash
bash scripts/train_kanana_finetune_rag.sh
```

### 4. ëª¨ë¸ í‰ê°€

```bash
bash scripts/evaluate_model.sh \
  --model_path models/kanana-finetune-rag \
  --dataset_path data/processed/finetune_rag_sample
```

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

### í•™ìŠµ ì‹œê°„
- **GPU**: RTX 4090
- **ë°ì´í„°**: 5 examples (ìƒ˜í”Œ)
- **ì˜ˆìƒ ì‹œê°„**: 5-10ë¶„
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ~18GB

### í‰ê°€ ë©”íŠ¸ë¦­ (ì˜ˆìƒ)

```
================================
EVALUATION METRICS
================================
Total Examples: 5
Overall Accuracy: 80.00%
Hallucination Rate: 10.00%

Answerable Questions:
  Count: 4
  Accuracy: 85.00%

Unanswerable Questions:
  Count: 1
  Accuracy (Refusal Rate): 70.00%
================================
```

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### ì‹¤ì œ ë°ì´í„°ì…‹ ì¤€ë¹„

1. **PDFì—ì„œ ë°ì´í„° ì¶”ì¶œ** (ì œì²œ ê´€ê´‘ ì •ë³´)
2. **Q&A ìŒ ìƒì„±** (150-200ê°œ ê¶Œì¥)
3. **Unanswerable ì§ˆë¬¸ ì¶”ê°€** (15% ë¹„ìœ¨)

ì˜ˆì œ:
```python
from src.data_processing.prepare_finetune_rag_dataset import (
    FinetuneRAGDatasetBuilder,
    RAGExample,
    AnswerType
)

examples = [
    RAGExample(
        question="ì˜ë¦¼ì§€ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?",
        context="ì˜ë¦¼ì§€ëŠ” ì œì²œì‹œ ì†¡í•™ë©´ ì˜ë¦¼ëŒ€ë¡œ 47ê¸¸ 7ì— ìœ„ì¹˜...",
        answer="ì˜ë¦¼ì§€ëŠ” ì œì²œì‹œ ì†¡í•™ë©´ ì˜ë¦¼ëŒ€ë¡œ 47ê¸¸ 7ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.",
        answer_type=AnswerType.ANSWERABLE,
    ),
    # ... ë” ë§ì€ ì˜ˆì œ
]

builder = FinetuneRAGDatasetBuilder(xml_format=True)
dataset = builder.build_dataset(
    examples,
    output_path="data/processed/jecheon_rag_dataset"
)
```

### í•™ìŠµ ì„¤ì • ì¡°ì •

`configs/finetune_rag_config.yaml` íŒŒì¼ ìˆ˜ì •:

```yaml
# ë” ë§ì€ ë°ì´í„°ë¡œ í•™ìŠµ ì‹œ
training:
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-4

# ë°ì´í„°ì…‹ ê²½ë¡œ ë³€ê²½
dataset:
  train_dataset_path: "data/processed/jecheon_rag_dataset"
```

### HuggingFace Hubì— ì—…ë¡œë“œ

í•™ìŠµ ì™„ë£Œ í›„:

```python
from transformers import AutoTokenizer
from peft import PeftModel, AutoModelForCausalLM

# ëª¨ë¸ ë¡œë“œ
base_model = AutoModelForCausalLM.from_pretrained("kakaocorp/kanana-1.5-8b-base")
model = PeftModel.from_pretrained(base_model, "models/kanana-finetune-rag")
tokenizer = AutoTokenizer.from_pretrained("models/kanana-finetune-rag")

# Merge adapter (ì„ íƒì‚¬í•­)
model = model.merge_and_unload()

# HuggingFace Hubì— í‘¸ì‹œ
model.push_to_hub("your-username/kanana-finetune-rag-jecheon")
tokenizer.push_to_hub("your-username/kanana-finetune-rag-jecheon")
```

## ğŸ“ ì¤‘ìš” ì‚¬í•­

### GPU ë©”ëª¨ë¦¬ ê´€ë¦¬

**ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ:**
```yaml
# configs/finetune_rag_config.yaml
training:
  per_device_train_batch_size: 1  # 2 â†’ 1ë¡œ ê°ì†Œ
  gradient_accumulation_steps: 16  # 8 â†’ 16ìœ¼ë¡œ ì¦ê°€
```

### Wandb ì„¤ì • (ì„ íƒì‚¬í•­)

í•™ìŠµ ê³¼ì •ì„ ì‹œê°í™”í•˜ë ¤ë©´:

```bash
# Wandb ë¡œê·¸ì¸
wandb login

# configs/finetune_rag_config.yamlì—ì„œ í™œì„±í™”
wandb:
  enabled: true
  project: "kanana-finetune-rag"
  name: "experiment-1"
```

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### CUDA Out of Memory
â†’ Batch size ì¤„ì´ê¸°, Sequence length ì¤„ì´ê¸° (2048 â†’ 1024)

### í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼
â†’ Flash Attention 2 í™•ì¸, DataLoader workers ì¦ê°€

### Lossê°€ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ
â†’ Learning rate ì¡°ì • (2e-4 â†’ 1e-4), Warmup ë¹„ìœ¨ ì¦ê°€

## ğŸ“š ë” ìì„¸í•œ ì •ë³´

- **ì „ì²´ ë¬¸ì„œ**: [docs/FINETUNE_RAG_README.md](docs/FINETUNE_RAG_README.md)
- **ë…¼ë¬¸**: [arXiv:2505.10792](https://arxiv.org/pdf/2505.10792)
- **KANANA ëª¨ë¸**: [HuggingFace](https://huggingface.co/kakaocorp/kanana-1.5-8b-base)

---

**Happy Fine-tuning! ğŸ‰**
