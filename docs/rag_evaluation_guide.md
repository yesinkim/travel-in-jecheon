# RAG ì‹œìŠ¤í…œ í‰ê°€ ê°€ì´ë“œ

## Overview

RAG ì‹œìŠ¤í…œì€ **Retrieval (ê²€ìƒ‰)**ê³¼ **Generation (ìƒì„±)** ë‘ ë‹¨ê³„ë¡œ êµ¬ì„±ë˜ë¯€ë¡œ, ê°ê°ì„ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

## ğŸ“Š RAG í‰ê°€ êµ¬ì¡°

```
                RAG ì‹œìŠ¤í…œ
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚
    Retrieval              Generation
    (ê²€ìƒ‰ í‰ê°€)             (ìƒì„± í‰ê°€)
        â”‚                       â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”              â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚       â”‚              â”‚       â”‚
  ì •ëŸ‰ì   ì •ì„±ì           ì •ëŸ‰ì   ì •ì„±ì 
```

---

## 1ï¸âƒ£ Retrieval í‰ê°€ (ê²€ìƒ‰ í’ˆì§ˆ)

### A. ì •ëŸ‰ì  ë©”íŠ¸ë¦­

#### 1. **Recall@K** (ì¬í˜„ìœ¨)
> ê´€ë ¨ ë¬¸ì„œ ì¤‘ ì‹¤ì œë¡œ ê²€ìƒ‰ëœ ë¹„ìœ¨

```python
# ì˜ˆì‹œ: ê´€ë ¨ ë¬¸ì„œ 5ê°œ ì¤‘ 3ê°œ ê²€ìƒ‰
# Recall@3 = 3/5 = 0.6

def recall_at_k(retrieved_docs, relevant_docs, k):
    """
    Args:
        retrieved_docs: ê²€ìƒ‰ëœ ìƒìœ„ kê°œ ë¬¸ì„œ ID
        relevant_docs: ì‹¤ì œ ê´€ë ¨ ìˆëŠ” ë¬¸ì„œ ID ì§‘í•©
    """
    retrieved_k = set(retrieved_docs[:k])
    relevant = set(relevant_docs)

    return len(retrieved_k & relevant) / len(relevant)
```

**ì‚¬ìš© ì‹œê¸°:**
- ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ í¬ê´„ì„± í‰ê°€
- "ê´€ë ¨ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ë§ì´ ì°¾ì•˜ëŠ”ê°€?"

#### 2. **Precision@K** (ì •ë°€ë„)
> ê²€ìƒ‰ëœ ë¬¸ì„œ ì¤‘ ì‹¤ì œ ê´€ë ¨ ìˆëŠ” ë¬¸ì„œ ë¹„ìœ¨

```python
# ì˜ˆì‹œ: 3ê°œ ê²€ìƒ‰ ì¤‘ 2ê°œê°€ ê´€ë ¨ ìˆìŒ
# Precision@3 = 2/3 = 0.67

def precision_at_k(retrieved_docs, relevant_docs, k):
    retrieved_k = set(retrieved_docs[:k])
    relevant = set(relevant_docs)

    return len(retrieved_k & relevant) / k
```

**ì‚¬ìš© ì‹œê¸°:**
- ê²€ìƒ‰ ì •í™•ë„ í‰ê°€
- "ê²€ìƒ‰ ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ ì •í™•í•œê°€?"

#### 3. **MRR (Mean Reciprocal Rank)** â­ ì¶”ì²œ
> ì²« ë²ˆì§¸ ê´€ë ¨ ë¬¸ì„œì˜ ìˆœìœ„ ì—­ìˆ˜ í‰ê· 

```python
# ì˜ˆì‹œ: ì²« ê´€ë ¨ ë¬¸ì„œê°€ 2ë²ˆì§¸ ìœ„ì¹˜
# RR = 1/2 = 0.5

def mean_reciprocal_rank(queries_results):
    """
    Args:
        queries_results: [
            ([retrieved_doc_ids], [relevant_doc_ids]),
            ...
        ]
    """
    reciprocal_ranks = []

    for retrieved, relevant in queries_results:
        for i, doc_id in enumerate(retrieved, 1):
            if doc_id in relevant:
                reciprocal_ranks.append(1.0 / i)
                break
        else:
            reciprocal_ranks.append(0.0)

    return sum(reciprocal_ranks) / len(reciprocal_ranks)
```

**ì‚¬ìš© ì‹œê¸°:**
- ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë¬¸ì„œê°€ ìƒìœ„ì— ìˆëŠ”ì§€ í‰ê°€
- RAGì—ì„œ ê°€ì¥ ì¤‘ìš”! (top-1ì´ ë‹µë³€ í’ˆì§ˆì— í° ì˜í–¥)

#### 4. **NDCG (Normalized Discounted Cumulative Gain)**
> ìˆœìœ„ì™€ ê´€ë ¨ì„±ì„ ëª¨ë‘ ê³ ë ¤í•œ í‰ê°€

```python
import numpy as np

def dcg_at_k(relevances, k):
    """
    Args:
        relevances: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ê´€ë ¨ì„± ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ [3, 2, 3, 0, 1, ...]
                   (0: ë¬´ê´€, 1: ì•½ê°„ ê´€ë ¨, 2: ê´€ë ¨, 3: ë§¤ìš° ê´€ë ¨)
    """
    relevances = np.array(relevances)[:k]
    if relevances.size == 0:
        return 0.0

    # DCG = sum(rel_i / log2(i+1))
    discounts = np.log2(np.arange(2, relevances.size + 2))
    return np.sum(relevances / discounts)

def ndcg_at_k(retrieved_relevances, ideal_relevances, k):
    """
    Args:
        retrieved_relevances: ì‹¤ì œ ê²€ìƒ‰ ìˆœì„œëŒ€ë¡œ ê´€ë ¨ì„± ì ìˆ˜
        ideal_relevances: ì´ìƒì ì¸ ìˆœì„œ (ê´€ë ¨ì„± ë†’ì€ ìˆœ)
    """
    dcg = dcg_at_k(retrieved_relevances, k)
    idcg = dcg_at_k(sorted(ideal_relevances, reverse=True), k)

    return dcg / idcg if idcg > 0 else 0.0
```

**ì‚¬ìš© ì‹œê¸°:**
- ê²€ìƒ‰ ìˆœìœ„ í’ˆì§ˆ ì¢…í•© í‰ê°€
- í•™ìˆ  ë…¼ë¬¸ì—ì„œ ë§ì´ ì‚¬ìš©

### B. ì •ì„±ì  í‰ê°€

#### 1. **Hit Rate (ì ì¤‘ë¥ )**
> ìƒìœ„ Kê°œ ì¤‘ ìµœì†Œ 1ê°œë¼ë„ ê´€ë ¨ ë¬¸ì„œê°€ ìˆëŠ” ë¹„ìœ¨

```python
def hit_rate(queries_results, k):
    hits = 0
    for retrieved, relevant in queries_results:
        if any(doc in relevant for doc in retrieved[:k]):
            hits += 1

    return hits / len(queries_results)
```

#### 2. **Context Relevance (ë¬¸ë§¥ ê´€ë ¨ì„±)**
> ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ì‹¤ì œë¡œ ê´€ë ¨ ìˆëŠ”ì§€ íŒë‹¨

```python
# LLMì„ ì‚¬ìš©í•œ í‰ê°€
def evaluate_context_relevance(query, context, llm):
    """
    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        context: ê²€ìƒ‰ëœ ë¬¸ì„œ
        llm: í‰ê°€ìš© LLM
    """
    prompt = f"""
    Query: {query}
    Context: {context}

    Does this context help answer the query?
    Answer with: RELEVANT or NOT_RELEVANT
    """

    response = llm.generate(prompt)
    return "RELEVANT" in response
```

---

## 2ï¸âƒ£ Generation í‰ê°€ (ìƒì„± í’ˆì§ˆ)

### A. ì •ëŸ‰ì  ë©”íŠ¸ë¦­

#### 1. **BLEU / ROUGE** (ì°¸ì¡° ë‹µë³€ì´ ìˆì„ ë•Œ)
> ì •ë‹µê³¼ ìƒì„±ëœ ë‹µë³€ì˜ ë‹¨ì–´ ì¼ì¹˜ë„

```python
from nltk.translate.bleu_score import sentence_bleu
from rouge import Rouge

def calculate_bleu(reference, generated):
    """
    Args:
        reference: ì •ë‹µ ë‹µë³€ (ë¦¬ìŠ¤íŠ¸)
        generated: ìƒì„±ëœ ë‹µë³€ (ë¬¸ìì—´)
    """
    reference_tokens = [reference.split()]
    generated_tokens = generated.split()

    return sentence_bleu(reference_tokens, generated_tokens)

def calculate_rouge(reference, generated):
    rouge = Rouge()
    scores = rouge.get_scores(generated, reference)[0]

    return {
        'rouge-1': scores['rouge-1']['f'],  # Unigram F1
        'rouge-2': scores['rouge-2']['f'],  # Bigram F1
        'rouge-l': scores['rouge-l']['f'],  # Longest common subsequence
    }
```

**í•œê³„:**
- í‘œí˜„ì´ ë‹¤ë¥´ì§€ë§Œ ì˜ë¯¸ê°€ ê°™ì€ ê²½ìš° ë‚®ì€ ì ìˆ˜
- í•œêµ­ì–´ì—ì„œëŠ” í˜•íƒœì†Œ ë¶„ì„ í•„ìš”

#### 2. **BERTScore** â­ ì¶”ì²œ (í•œêµ­ì–´)
> ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜ í‰ê°€

```python
from bert_score import score

def calculate_bertscore(references, candidates):
    """
    Args:
        references: ì •ë‹µ ë¦¬ìŠ¤íŠ¸
        candidates: ìƒì„± ë‹µë³€ ë¦¬ìŠ¤íŠ¸
    """
    P, R, F1 = score(
        candidates,
        references,
        lang="ko",  # í•œêµ­ì–´
        model_type="bert-base-multilingual-cased"
    )

    return {
        'precision': P.mean().item(),
        'recall': R.mean().item(),
        'f1': F1.mean().item()
    }
```

**ì¥ì :**
- ì˜ë¯¸ì  ìœ ì‚¬ë„ ì¸¡ì •
- í•œêµ­ì–´ ì§€ì› ìš°ìˆ˜

### B. RAG íŠ¹í™” ë©”íŠ¸ë¦­ â­â­â­

#### 1. **Faithfulness (ì¶©ì‹¤ë„)** - ê°€ì¥ ì¤‘ìš”!
> ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ì— ê¸°ë°˜í•˜ëŠ”ì§€

```python
def evaluate_faithfulness(question, context, answer, llm):
    """
    ë‹µë³€ì´ contextì—ì„œ ë‚˜ì˜¨ ì •ë³´ë§Œ ì‚¬ìš©í–ˆëŠ”ì§€ í‰ê°€

    Hallucination ë°©ì§€!
    """
    prompt = f"""
    Question: {question}
    Context: {context}
    Answer: {answer}

    Does the answer only use information from the context?
    Score from 1-5 (5 = completely faithful, 1 = hallucination)
    """

    response = llm.generate(prompt)
    # Parse score from response
    return parse_score(response)
```

**ì™œ ì¤‘ìš”í•œê°€:**
- RAGì˜ í•µì‹¬ = ê²€ìƒ‰í•œ ì •ë³´ë§Œ ì‚¬ìš©
- Hallucination (í™˜ê°) ë°©ì§€

#### 2. **Answer Relevance (ë‹µë³€ ê´€ë ¨ì„±)**
> ë‹µë³€ì´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ì ì ˆí•œì§€

```python
def evaluate_answer_relevance(question, answer, llm):
    """
    ë‹µë³€ì´ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ë‹µí•˜ëŠ”ì§€
    """
    prompt = f"""
    Question: {question}
    Answer: {answer}

    Does this answer directly address the question?
    Score from 1-5 (5 = perfect answer, 1 = irrelevant)
    """

    return parse_score(llm.generate(prompt))
```

#### 3. **Context Precision (ë¬¸ë§¥ ì •ë°€ë„)**
> ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ëª¨ë‘ ìœ ìš©í•œì§€

```python
def evaluate_context_precision(question, contexts, answer, llm):
    """
    ê²€ìƒ‰ëœ contextë“¤ì´ ë‹µë³€ì— ì‹¤ì œë¡œ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€
    """
    useful_contexts = 0

    for ctx in contexts:
        prompt = f"""
        Question: {question}
        Context: {ctx}
        Answer: {answer}

        Was this context useful for generating the answer?
        Answer: YES or NO
        """

        if "YES" in llm.generate(prompt):
            useful_contexts += 1

    return useful_contexts / len(contexts)
```

---

## 3ï¸âƒ£ ì‹¤ì „ í‰ê°€ í”„ë ˆì„ì›Œí¬

### RAGAS (RAG Assessment) â­ ì¶”ì²œ

ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” RAG í‰ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
)

# í‰ê°€ ë°ì´í„°ì…‹ ì¤€ë¹„
eval_dataset = {
    'question': [
        "ì œì²œ ì‹œí‹°íˆ¬ì–´ëŠ” ì–´ë–»ê²Œ ì˜ˆì•½í•˜ë‚˜ìš”?",
        "ì˜ë¦¼ì§€ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?",
        ...
    ],
    'answer': [
        "citytour.jecheon.go.krì—ì„œ ì˜ˆì•½í•˜ê±°ë‚˜...",
        "ì˜ë¦¼ì§€ëŠ” ì œì²œì‹œ ëª¨ì‚°ë™ì— ìœ„ì¹˜í•©ë‹ˆë‹¤...",
        ...
    ],
    'contexts': [
        ["ì œì²œ ì‹œí‹°íˆ¬ì–´\nì˜ˆì•½ì•ˆë‚´\ncitytour.jecheon.go.kr..."],
        ["ì˜ë¦¼ì§€\nìœ„ì¹˜: ì¶©ì²­ë¶ë„ ì œì²œì‹œ ëª¨ì‚°ë™..."],
        ...
    ],
    'ground_truth': [  # Optional
        "ê³µì‹ í™ˆí˜ì´ì§€ë‚˜ ì „í™”ë¡œ ì˜ˆì•½",
        "ì œì²œì‹œ ëª¨ì‚°ë™",
        ...
    ]
}

# í‰ê°€ ì‹¤í–‰
result = evaluate(
    eval_dataset,
    metrics=[
        faithfulness,          # ë‹µë³€ì´ contextì— ì¶©ì‹¤í•œê°€
        answer_relevancy,      # ë‹µë³€ì´ ì§ˆë¬¸ì— ê´€ë ¨ìˆëŠ”ê°€
        context_recall,        # í•„ìš”í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í–ˆëŠ”ê°€
        context_precision,     # ê²€ìƒ‰ëœ ì •ë³´ê°€ ìœ ìš©í•œê°€
    ],
)

print(result)
# {
#   'faithfulness': 0.92,
#   'answer_relevancy': 0.88,
#   'context_recall': 0.85,
#   'context_precision': 0.90
# }
```

---

## 4ï¸âƒ£ ì œì²œ í”„ë¡œì íŠ¸ í‰ê°€ ì „ëµ

### í‰ê°€ ë°ì´í„°ì…‹ êµ¬ì¶•

#### A. ì§ˆë¬¸-ë‹µë³€ ìŒ ìƒì„±

```python
# 1. GPT/Claudeë¡œ ì§ˆë¬¸ ìƒì„±
from openai import OpenAI

client = OpenAI()

def generate_qa_pairs(context_chunk):
    """
    PDF chunkì—ì„œ ì§ˆë¬¸-ë‹µë³€ ìŒ ìƒì„±
    """
    prompt = f"""
    ë‹¤ìŒ ì œì²œì‹œ ê´€ê´‘ ì •ë³´ë¥¼ ì½ê³ , 3ê°œì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì„ ìƒì„±í•˜ì„¸ìš”.

    ì •ë³´:
    {context_chunk}

    í˜•ì‹:
    Q1: [ì§ˆë¬¸]
    A1: [ë‹µë³€]
    Q2: [ì§ˆë¬¸]
    A2: [ë‹µë³€]
    Q3: [ì§ˆë¬¸]
    A3: [ë‹µë³€]
    """

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )

    return parse_qa_pairs(response.choices[0].message.content)

# 2. ìˆ˜ë™ìœ¼ë¡œ ê²€ì¦ ë° ì •ì œ
# 3. train/test ë¶„í•  (80/20)
```

#### B. í‰ê°€ ì§ˆë¬¸ ì˜ˆì‹œ (ì œì²œ ê´€ê´‘)

```python
eval_questions = [
    # ë‹¨ìˆœ ì‚¬ì‹¤ ì§ˆë¬¸
    "ì œì²œ ì‹œí‹°íˆ¬ì–´ ìš”ê¸ˆì€ ì–¼ë§ˆì¸ê°€ìš”?",
    "ì˜ë¦¼ì§€ ë°•ë¬¼ê´€ì€ ì–¸ì œ íœ´ë¬´ì¸ê°€ìš”?",
    "ì²­í’í˜¸ë°˜ ì¼€ì´ë¸”ì¹´ ìš´ì˜ì‹œê°„ì€?",

    # ë¹„êµ ì§ˆë¬¸
    "ì œì²œì˜ ëŒ€í‘œ ì¶•ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
    "ê°€ì¡± ì—¬í–‰ì— ì¶”ì²œí•˜ëŠ” ì½”ìŠ¤ëŠ”?",

    # ë³µí•© ì§ˆë¬¸
    "ì œì²œì—ì„œ 1ë°• 2ì¼ ì—¬í–‰ ê³„íšì„ ì„¸ì›Œì£¼ì„¸ìš”",
    "ê²¨ìš¸ì— ì œì²œì—ì„œ í•  ìˆ˜ ìˆëŠ” í™œë™ì€?",

    # ì¶”ë¡  ì§ˆë¬¸
    "ë¹„ê°€ ì˜¤ëŠ” ë‚  ì œì²œì—ì„œ ê°ˆ ë§Œí•œ ê³³ì€?",
    "ì–´ë¦°ì´ì™€ í•¨ê»˜ ê°€ê¸° ì¢‹ì€ ê´€ê´‘ì§€ëŠ”?",
]
```

### í‰ê°€ ë©”íŠ¸ë¦­ ì„ ì • (ê³¼ì œìš©)

```python
# Retrieval í‰ê°€
retrieval_metrics = {
    'MRR': mean_reciprocal_rank,      # ì²« ê´€ë ¨ ë¬¸ì„œ ìˆœìœ„
    'Recall@3': lambda: recall_at_k(k=3),  # ìƒìœ„ 3ê°œ ì¬í˜„ìœ¨
    'Hit_Rate@5': lambda: hit_rate(k=5),   # ìƒìœ„ 5ê°œ ì ì¤‘ë¥ 
}

# Generation í‰ê°€
generation_metrics = {
    'BERTScore_F1': calculate_bertscore,   # ì˜ë¯¸ ìœ ì‚¬ë„
    'BLEU': calculate_bleu,                # ë‹¨ì–´ ì¼ì¹˜ë„ (ì°¸ê³ ìš©)
}

# RAG í†µí•© í‰ê°€ (RAGAS)
rag_metrics = {
    'Faithfulness': faithfulness,          # ì¶©ì‹¤ë„ â­
    'Answer_Relevancy': answer_relevancy,  # ë‹µë³€ ê´€ë ¨ì„± â­
    'Context_Precision': context_precision,# ê²€ìƒ‰ ì •ë°€ë„
}
```

### Baseline vs Fine-tuned ë¹„êµ

```python
import pandas as pd

# í‰ê°€ ì‹¤í–‰
baseline_results = evaluate_model(baseline_model, eval_dataset)
finetuned_results = evaluate_model(finetuned_model, eval_dataset)

# ë¹„êµí‘œ ìƒì„±
comparison = pd.DataFrame({
    'Metric': list(rag_metrics.keys()),
    'Baseline': [baseline_results[m] for m in rag_metrics],
    'Fine-tuned': [finetuned_results[m] for m in rag_metrics],
    'Improvement': [
        (finetuned_results[m] - baseline_results[m]) / baseline_results[m] * 100
        for m in rag_metrics
    ]
})

print(comparison)
#          Metric  Baseline  Fine-tuned  Improvement
# 0  Faithfulness      0.72        0.89        23.6%
# 1  Answer_Rel...     0.68        0.85        25.0%
# 2  Context_Pre...    0.75        0.88        17.3%
```

---

## 5ï¸âƒ£ ì •ì„±ì  í‰ê°€ (í•„ìˆ˜!)

### A. ì˜ˆì‹œ ê¸°ë°˜ ë¹„êµ (3+ examples)

```markdown
### Example 1: ì‹œí‹°íˆ¬ì–´ ì˜ˆì•½

**ì§ˆë¬¸:** ì œì²œ ì‹œí‹°íˆ¬ì–´ëŠ” ì–´ë–»ê²Œ ì˜ˆì•½í•˜ë‚˜ìš”?

**ê²€ìƒ‰ëœ Context:**
ì œì²œ ì‹œí‹°íˆ¬ì–´
ì˜ˆì•½ì•ˆë‚´: citytour.jecheon.go.kr
ì „í™”: 043-647-2121

**Baseline ë‹µë³€:**
ì‹œí‹°íˆ¬ì–´ëŠ” 05ë²ˆìœ¼ë¡œ ì˜ˆì•½í•˜ì„¸ìš”.
â†’ âŒ Hallucination (í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì „í™”ë²ˆí˜¸ë¡œ ì˜¤ì¸)

**Fine-tuned ë‹µë³€:**
ì œì²œ ì‹œí‹°íˆ¬ì–´ëŠ” ê³µì‹ í™ˆí˜ì´ì§€(citytour.jecheon.go.kr)ë‚˜
ì „í™”(043-647-2121)ë¡œ ì˜ˆì•½í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
â†’ âœ… ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€

**í‰ê°€:**
- Faithfulness: Baseline 2/5, Fine-tuned 5/5
- Relevancy: Baseline 3/5, Fine-tuned 5/5
```

### B. ì‹¤íŒ¨ ì‚¬ë¡€ ë¶„ì„

```markdown
### Failure Case 1: ë³µí•© ì§ˆë¬¸

**ì§ˆë¬¸:** 1ë°• 2ì¼ ì œì²œ ì—¬í–‰ ì½”ìŠ¤ ì¶”ì²œí•´ì£¼ì„¸ìš”

**ë¬¸ì œ:** ì—¬ëŸ¬ chunkì—ì„œ ì •ë³´ë¥¼ ì¢…í•©í•´ì•¼ í•¨

**ê°œì„  ë°©ì•ˆ:**
- Retrieval: top_kë¥¼ 3â†’5ë¡œ ì¦ê°€
- Generation: ë” ê¸´ context window ì‚¬ìš©
- Re-ranking ì¶”ê°€
```

---

## 6ï¸âƒ£ êµ¬í˜„ ìˆœì„œ

### Step 1: í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±
```bash
python scripts/generate_eval_dataset.py
# â†’ data/eval/test_set.json
```

### Step 2: Baseline í‰ê°€
```bash
python scripts/evaluate_rag.py --model baseline
# â†’ results/baseline_evaluation.json
```

### Step 3: Fine-tuned í‰ê°€
```bash
python scripts/evaluate_rag.py --model finetuned
# â†’ results/finetuned_evaluation.json
```

### Step 4: ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
```bash
python scripts/generate_comparison_report.py
# â†’ results/comparison_report.pdf
```

---

## 7ï¸âƒ£ ì¶”ì²œ ë©”íŠ¸ë¦­ ì¡°í•©

### ìµœì†Œ êµ¬ì„± (ì‹œê°„ ë¶€ì¡± ì‹œ)
```python
metrics = {
    'Retrieval': ['MRR', 'Recall@3'],
    'Generation': ['BERTScore'],
    'RAG': ['Faithfulness', 'Answer_Relevancy'],
    'Qualitative': ['3 Examples', 'Failure Analysis']
}
```

### ê¶Œì¥ êµ¬ì„± â­
```python
metrics = {
    'Retrieval': ['MRR', 'Recall@3', 'Hit_Rate@5'],
    'Generation': ['BERTScore', 'BLEU'],
    'RAG': ['Faithfulness', 'Answer_Relevancy', 'Context_Precision'],
    'Qualitative': ['5 Examples', 'Failure Analysis', 'User Study (Optional)']
}
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **RAGAS ê³µì‹ ë¬¸ì„œ:** https://docs.ragas.io/
- **ì°¸ì¡° ë…¼ë¬¸:** "RAGAS: Automated Evaluation of RAG" (2023)
- **í•œêµ­ì–´ RAG ë²¤ì¹˜ë§ˆí¬:** https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-KO

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] í‰ê°€ ë°ì´í„°ì…‹ ìƒì„± (50-100 Q&A)
- [ ] Retrieval ë©”íŠ¸ë¦­ ê³„ì‚° (MRR, Recall@K)
- [ ] Generation ë©”íŠ¸ë¦­ ê³„ì‚° (BERTScore)
- [ ] RAG ë©”íŠ¸ë¦­ ê³„ì‚° (Faithfulness, Relevancy)
- [ ] Baseline vs Fine-tuned ë¹„êµí‘œ
- [ ] ì •ì„±ì  ì˜ˆì‹œ ë¶„ì„ (3+)
- [ ] ì‹¤íŒ¨ ì‚¬ë¡€ ë¶„ì„
- [ ] ì‹œê°í™” (ê·¸ë˜í”„, ì°¨íŠ¸)
- [ ] ë¦¬í¬íŠ¸ ì‘ì„±

---

**ë‹¤ìŒ ë‹¨ê³„: í‰ê°€ ë°ì´í„°ì…‹ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±**
