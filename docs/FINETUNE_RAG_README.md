# Finetune-RAG for KANANA1.5-8B

ì´ í”„ë¡œì íŠ¸ëŠ” ë…¼ë¬¸ **"Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation"** (arXiv:2505.10792)ì˜ ë°©ë²•ë¡ ì„ KANANA1.5-8B ëª¨ë¸ì— ì ìš©í•œ êµ¬í˜„ì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ì£¼ìš” íŠ¹ì§•](#ì£¼ìš”-íŠ¹ì§•)
- [í™˜ê²½ ìš”êµ¬ì‚¬í•­](#í™˜ê²½-ìš”êµ¬ì‚¬í•­)
- [ì„¤ì¹˜](#ì„¤ì¹˜)
- [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
- [í”„ë¡œì íŠ¸ êµ¬ì¡°](#í”„ë¡œì íŠ¸-êµ¬ì¡°)
- [ì„¤ì • íŒŒì¼](#ì„¤ì •-íŒŒì¼)
- [í‰ê°€ ë° ë¶„ì„](#í‰ê°€-ë°-ë¶„ì„)
- [ì°¸ê³  ìë£Œ](#ì°¸ê³ -ìë£Œ)

## ê°œìš”

### Finetune-RAGë€?

Finetune-RAGëŠ” RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì—ì„œ LLMì˜ í™˜ê°(hallucination) ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ fine-tuning ë°©ë²•ë¡ ì…ë‹ˆë‹¤.

**í•µì‹¬ ì•„ì´ë””ì–´:**
1. **XML ê¸°ë°˜ êµ¬ì¡°í™” ì…ë ¥**: ë¬¸ë§¥(context)ê³¼ ì§ˆë¬¸(question)ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„
2. **í™˜ê° ë°©ì§€ í•™ìŠµ**: "ë‹µë³€í•  ìˆ˜ ì—†ìŒ" ì‘ë‹µì„ í•™ìŠµí•˜ì—¬ ë¶€ì •í™•í•œ ì •ë³´ ìƒì„± ë°©ì§€
3. **ë¬¸ë§¥ ê¸°ë°˜ ë‹µë³€**: ê²€ìƒ‰ëœ ë¬¸ë§¥ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±

### KANANA1.5-8Bë€?

Kakaoì—ì„œ ê°œë°œí•œ 8B íŒŒë¼ë¯¸í„° ì´ì¤‘ì–¸ì–´(í•œ-ì˜) LLMìœ¼ë¡œ, ë‹¤ìŒ íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤:
- **í¬ê¸°**: 8.03B parameters
- **ë¬¸ë§¥ ê¸¸ì´**: 32K tokens (YaRNìœ¼ë¡œ 128Kê¹Œì§€ í™•ì¥ ê°€ëŠ¥)
- **ê°•ì **: ì½”ë”©, ìˆ˜í•™, í•¨ìˆ˜ í˜¸ì¶œ, í•œêµ­ì–´ ì²˜ë¦¬
- **ì•„í‚¤í…ì²˜**: Llama ê¸°ë°˜
- **ë¼ì´ì„¼ìŠ¤**: Apache 2.0

## ì£¼ìš” íŠ¹ì§•

### âœ¨ êµ¬í˜„ëœ ê¸°ëŠ¥

1. **QLoRA ê¸°ë°˜ íš¨ìœ¨ì  í•™ìŠµ**
   - 4-bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ê°ì†Œ
   - RTX 4090 (24GB)ì—ì„œ í•™ìŠµ ê°€ëŠ¥
   - LoRA adapterë¡œ ë¹ ë¥¸ í•™ìŠµ ë° ë°°í¬

2. **XML ê¸°ë°˜ ë°ì´í„° í¬ë§·**
   ```xml
   <document>
   <source>ì œì²œì‹œ ê´€ê´‘ì •ë³´</source>
   <context>
   ì˜ë¦¼ì§€ëŠ” ì œì²œì‹œ ì†¡í•™ë©´ì— ìœ„ì¹˜í•œ ì—­ì‚¬ì ì¸ ì €ìˆ˜ì§€ì…ë‹ˆë‹¤.
   </context>
   </document>

   <question>ì˜ë¦¼ì§€ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?</question>

   <answer>ì˜ë¦¼ì§€ëŠ” ì œì²œì‹œ ì†¡í•™ë©´ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.</answer>
   ```

3. **í™˜ê° ë°©ì§€ ë©”ì»¤ë‹ˆì¦˜**
   - Unanswerable ì§ˆë¬¸ í•™ìŠµ (15% ë¹„ìœ¨)
   - "ì œê³µëœ ì •ë³´ì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ì‘ë‹µ í•™ìŠµ
   - ë¬¸ë§¥ ê¸°ë°˜ ë‹µë³€ ê°•ì œ

4. **í¬ê´„ì ì¸ í‰ê°€**
   - ë‹µë³€ ì •í™•ë„ ì¸¡ì •
   - í™˜ê° ë¹„ìœ¨ ì¸¡ì •
   - Answerable vs Unanswerable ì„±ëŠ¥ ë¶„ë¦¬ í‰ê°€

5. **Weights & Biases í†µí•©**
   - ì‹¤ì‹œê°„ í•™ìŠµ ëª¨ë‹ˆí„°ë§
   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì 
   - ì‹¤í—˜ ë¹„êµ

## í™˜ê²½ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´

**ê¶Œì¥ í™˜ê²½:**
- **GPU**: RTX 4090 (24GB) ì´ìƒ
- **ëŒ€ì•ˆ**: RTX 3090 (24GB), A40 (48GB), A100 (40GB/80GB)
- **RAM**: 32GB ì´ìƒ
- **ì €ì¥ê³µê°„**: 50GB ì´ìƒ

**QLoRA ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­:**
- ëª¨ë¸ ë¡œë”©: ~5-6GB
- í•™ìŠµ: ~18-20GB
- ì—¬ìœ  ê³µê°„: ~4GB

### ì†Œí”„íŠ¸ì›¨ì–´

- **Python**: 3.11+
- **CUDA**: 11.8+ (GPU ì‚¬ìš© ì‹œ)
- **ìš´ì˜ì²´ì œ**: Linux (ê¶Œì¥), Windows (WSL2), macOS (CPUë§Œ)

## ì„¤ì¹˜

### 1. ì €ì¥ì†Œ í´ë¡ 

```bash
git clone https://github.com/your-username/goodganglabs.git
cd goodganglabs
```

### 2. ì˜ì¡´ì„± ì„¤ì¹˜

**Option A: uv ì‚¬ìš© (ê¶Œì¥)**
```bash
uv sync
```

**Option B: pip ì‚¬ìš©**
```bash
pip install -r requirements.txt
```

### 3. ì„¤ì¹˜ í™•ì¸

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
```

## ì‚¬ìš© ë°©ë²•

### ë¹ ë¥¸ ì‹œì‘ (Quick Start)

#### 1. ìƒ˜í”Œ ë°ì´í„°ì…‹ ì¤€ë¹„

```bash
bash scripts/prepare_sample_dataset.sh
```

ìƒì„± ìœ„ì¹˜: `data/processed/finetune_rag_sample/`

#### 2. í•™ìŠµ ì‹¤í–‰

```bash
bash scripts/train_kanana_finetune_rag.sh
```

ê¸°ë³¸ì ìœ¼ë¡œ `configs/finetune_rag_config.yaml` ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

#### 3. ëª¨ë¸ í‰ê°€

```bash
bash scripts/evaluate_model.sh \
  --model_path models/kanana-finetune-rag \
  --dataset_path data/processed/finetune_rag_sample
```

### ê³ ê¸‰ ì‚¬ìš©ë²•

#### ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµ

1. **ë°ì´í„°ì…‹ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±**

```python
from src.data_processing.prepare_finetune_rag_dataset import (
    FinetuneRAGDatasetBuilder,
    RAGExample,
    AnswerType
)
from pathlib import Path

# ì˜ˆì œ ìƒì„±
examples = [
    RAGExample(
        question="ì œì²œì˜ ëŒ€í‘œ ê´€ê´‘ì§€ëŠ”?",
        context="ì œì²œì˜ ëŒ€í‘œ ê´€ê´‘ì§€ë¡œëŠ” ì˜ë¦¼ì§€, ì²­í’í˜¸ë°˜ ì¼€ì´ë¸”ì¹´ ë“±ì´ ìˆìŠµë‹ˆë‹¤.",
        answer="ì œì²œì˜ ëŒ€í‘œ ê´€ê´‘ì§€ë¡œëŠ” ì˜ë¦¼ì§€, ì²­í’í˜¸ë°˜ ì¼€ì´ë¸”ì¹´ ë“±ì´ ìˆìŠµë‹ˆë‹¤.",
        answer_type=AnswerType.ANSWERABLE,
    ),
    # ... ë” ë§ì€ ì˜ˆì œ ì¶”ê°€
]

# ë°ì´í„°ì…‹ ë¹Œë“œ
builder = FinetuneRAGDatasetBuilder(xml_format=True)
dataset = builder.build_dataset(
    examples,
    output_path=Path("data/processed/my_custom_dataset")
)
```

2. **ì„¤ì • íŒŒì¼ ìˆ˜ì •**

`configs/finetune_rag_config.yaml`:
```yaml
dataset:
  train_dataset_path: "data/processed/my_custom_dataset"
```

3. **í•™ìŠµ ì‹¤í–‰**

```bash
python src/training/finetune_rag.py --config configs/finetune_rag_config.yaml
```

#### PEFT Adapterë§Œ ë¡œë“œí•˜ì—¬ í‰ê°€

```bash
python src/evaluation/evaluate_rag_model.py \
  --model_path models/kanana-finetune-rag \
  --base_model_path kakaocorp/kanana-1.5-8b-base \
  --dataset_path data/processed/finetune_rag_dataset \
  --output_path results/eval_results.json
```

#### ì¶”ë¡  í…ŒìŠ¤íŠ¸

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# ëª¨ë¸ ë¡œë“œ
base_model = AutoModelForCausalLM.from_pretrained(
    "kakaocorp/kanana-1.5-8b-base",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
model = PeftModel.from_pretrained(base_model, "models/kanana-finetune-rag")
tokenizer = AutoTokenizer.from_pretrained("models/kanana-finetune-rag")

# ì¶”ë¡ 
question = "ì˜ë¦¼ì§€ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?"
context = "ì˜ë¦¼ì§€ëŠ” ì œì²œì‹œ ì†¡í•™ë©´ì— ìœ„ì¹˜í•œ ì—­ì‚¬ì ì¸ ì €ìˆ˜ì§€ì…ë‹ˆë‹¤."

messages = [
    {"role": "system", "content": "ì œê³µëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”."},
    {"role": "user", "content": f"""<document>
<context>{context}</context>
</document>
<question>{question}</question>
<answer>"""}
]

input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")
outputs = model.generate(input_ids, max_new_tokens=256, temperature=0.7)
answer = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)

print(f"ë‹µë³€: {answer}")
```

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
goodganglabs/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ finetune_rag_config.yaml       # í•™ìŠµ ì„¤ì • íŒŒì¼
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_processing/
â”‚   â”‚   â””â”€â”€ prepare_finetune_rag_dataset.py  # ë°ì´í„°ì…‹ ì¤€ë¹„
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ finetune_rag.py            # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ evaluation/
â”‚       â””â”€â”€ evaluate_rag_model.py      # í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_kanana_finetune_rag.sh   # í•™ìŠµ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ evaluate_model.sh              # í‰ê°€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ prepare_sample_dataset.sh      # ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                           # ì›ë³¸ ë°ì´í„°
â”‚   â””â”€â”€ processed/                     # ì²˜ë¦¬ëœ ë°ì´í„°ì…‹
â”œâ”€â”€ models/                            # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
â”œâ”€â”€ results/                           # í‰ê°€ ê²°ê³¼
â””â”€â”€ docs/
    â””â”€â”€ FINETUNE_RAG_README.md         # ì´ ë¬¸ì„œ
```

## ì„¤ì • íŒŒì¼

### ì£¼ìš” ì„¤ì • í•­ëª©

#### ëª¨ë¸ ì„¤ì •
```yaml
model:
  name: "kakaocorp/kanana-1.5-8b-base"
  torch_dtype: "bfloat16"
  use_flash_attention_2: true
```

#### QLoRA ì„¤ì •
```yaml
qlora:
  enabled: true
  load_in_4bit: true
  lora_r: 16              # LoRA rank (8, 16, 32, 64)
  lora_alpha: 32          # Scaling factor (ë³´í†µ 2x rank)
  lora_dropout: 0.05
  target_modules:         # í•™ìŠµí•  ëª¨ë“ˆ
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
```

#### í•™ìŠµ ì„¤ì •
```yaml
training:
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8  # Effective batch = 16
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"
  optim: "paged_adamw_32bit"
  bf16: true
```

#### ë°ì´í„°ì…‹ ì„¤ì •
```yaml
dataset:
  train_dataset_path: "data/processed/finetune_rag_dataset"
  use_chat_template: true
  xml_format: true
  max_seq_length: 2048
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | íŠœë‹ ê°€ì´ë“œ |
|---------|--------|------------|
| `lora_r` | 16 | 8 (ë¹ ë¦„, ì €í’ˆì§ˆ) â†’ 64 (ëŠë¦¼, ê³ í’ˆì§ˆ) |
| `learning_rate` | 2e-4 | QLoRA: 1e-4 ~ 5e-4, Full FT: 1e-5 ~ 5e-5 |
| `num_train_epochs` | 3 | ì‘ì€ ë°ì´í„°ì…‹: 3-5, í° ë°ì´í„°ì…‹: 1-2 |
| `batch_size` | 2 | GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • |
| `max_seq_length` | 2048 | ì§§ì€ ë¬¸ì„œ: 512-1024, ê¸´ ë¬¸ì„œ: 2048-4096 |

## í‰ê°€ ë° ë¶„ì„

### í‰ê°€ ë©”íŠ¸ë¦­

1. **Overall Accuracy**: ì „ì²´ ì •í™•ë„
2. **Answerable Accuracy**: ë‹µë³€ ê°€ëŠ¥í•œ ì§ˆë¬¸ì˜ ì •í™•ë„
3. **Unanswerable Accuracy (Refusal Rate)**: ë‹µë³€ ë¶ˆê°€ ì§ˆë¬¸ì„ ì˜¬ë°”ë¥´ê²Œ ê±°ë¶€í•œ ë¹„ìœ¨
4. **Hallucination Rate**: í™˜ê° ë°œìƒ ë¹„ìœ¨

### í‰ê°€ ê²°ê³¼ ì˜ˆì‹œ

```json
{
  "metrics": {
    "total_examples": 100,
    "accuracy": 0.85,
    "hallucination_rate": 0.08,
    "answerable_examples": 85,
    "answerable_accuracy": 0.88,
    "unanswerable_examples": 15,
    "unanswerable_accuracy": 0.73,
    "refusal_rate": 0.73
  }
}
```

### ë² ì´ìŠ¤ë¼ì¸ vs Fine-tuned ë¹„êµ

**í‰ê°€ ë°©ë²•:**
1. ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ë¡œ í‰ê°€
2. Fine-tuned ëª¨ë¸ë¡œ í‰ê°€
3. ë©”íŠ¸ë¦­ ë¹„êµ

```bash
# Baseline í‰ê°€
python src/evaluation/evaluate_rag_model.py \
  --model_path kakaocorp/kanana-1.5-8b-base \
  --dataset_path data/processed/finetune_rag_dataset \
  --output_path results/baseline_results.json

# Fine-tuned í‰ê°€
python src/evaluation/evaluate_rag_model.py \
  --model_path models/kanana-finetune-rag \
  --base_model_path kakaocorp/kanana-1.5-8b-base \
  --dataset_path data/processed/finetune_rag_dataset \
  --output_path results/finetuned_results.json
```

### ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ

ë…¼ë¬¸ ê¸°ì¤€ Finetune-RAG ì ìš© ì‹œ:
- **Answerable Accuracy**: +5-10%
- **Refusal Rate**: +15-25%
- **Hallucination Rate**: -10-20%

## ë¹„ìš© ì¶”ì • (RunPod ê¸°ì¤€)

### GPU ê°€ê²©
- RTX 4090: ~$0.50/hour
- A40: ~$0.60/hour

### í•™ìŠµ ì‹œê°„ ì¶”ì •

| ë°ì´í„°ì…‹ í¬ê¸° | Epochs | ì˜ˆìƒ ì‹œê°„ | ì˜ˆìƒ ë¹„ìš© (RTX 4090) |
|-------------|--------|---------|-------------------|
| 100 examples | 3 | 1-2 hours | $0.50-1.00 |
| 150 examples | 3 | 2-3 hours | $1.00-1.50 |
| 200 examples | 3 | 3-4 hours | $1.50-2.00 |

**ì´ ì˜ˆìƒ ë¹„ìš© (Baseline + Fine-tuning + Evaluation):**
- **ìµœì†Œ**: $2-3
- **ê¶Œì¥**: $3-5
- **ìµœëŒ€**: $5-7

**$15 í¬ë ˆë”§ìœ¼ë¡œ ì¶©ë¶„í•œ ì‹¤í—˜ ê°€ëŠ¥!**

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)

**ì¦ìƒ:**
```
torch.cuda.OutOfMemoryError: CUDA out of memory
```

**í•´ê²° ë°©ë²•:**
1. Batch size ì¤„ì´ê¸°:
   ```yaml
   per_device_train_batch_size: 1
   gradient_accumulation_steps: 16
   ```

2. Sequence length ì¤„ì´ê¸°:
   ```yaml
   max_seq_length: 1024
   ```

3. LoRA rank ì¤„ì´ê¸°:
   ```yaml
   lora_r: 8
   ```

### í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼

**í•´ê²° ë°©ë²•:**
1. Flash Attention 2 í™œì„±í™” (ì´ë¯¸ í™œì„±í™”ë¨)
2. Gradient checkpointing ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì¶©ë¶„ ì‹œ):
   ```yaml
   gradient_checkpointing: false
   ```
3. DataLoader workers ì¦ê°€:
   ```yaml
   dataloader_num_workers: 8
   ```

### Lossê°€ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ

**í•´ê²° ë°©ë²•:**
1. Learning rate ì¡°ì •:
   ```yaml
   learning_rate: 1.0e-4  # ë” ë‚®ì€ ê°’
   ```

2. Warmup ë¹„ìœ¨ ì¦ê°€:
   ```yaml
   warmup_ratio: 0.1
   ```

3. ë°ì´í„° í’ˆì§ˆ í™•ì¸:
   - Answerable/Unanswerable ë¹„ìœ¨ í™•ì¸
   - ë°ì´í„° ì¤‘ë³µ ì œê±°
   - ë ˆì´ë¸” ì •í™•ì„± í™•ì¸

## ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- **Finetune-RAG**: [arXiv:2505.10792](https://arxiv.org/pdf/2505.10792)
- **KANANA**: [arXiv:2502.18934](https://arxiv.org/abs/2502.18934)
- **QLoRA**: [arXiv:2305.14314](https://arxiv.org/abs/2305.14314)

### ëª¨ë¸
- [kakaocorp/kanana-1.5-8b-base](https://huggingface.co/kakaocorp/kanana-1.5-8b-base)
- [kakaocorp/kanana-1.5-8b-instruct-2505](https://huggingface.co/kakaocorp/kanana-1.5-8b-instruct-2505)

### ë¼ì´ë¸ŒëŸ¬ë¦¬
- [Transformers](https://huggingface.co/docs/transformers)
- [PEFT](https://huggingface.co/docs/peft)
- [TRL](https://huggingface.co/docs/trl)
- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

### ê¸°íƒ€
- [Allganize Korean RAG Evaluation Dataset](https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-KO)
- [RunPod Documentation](https://docs.runpod.io/)
- [Weights & Biases](https://wandb.ai/)

## ë¼ì´ì„¼ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” Apache 2.0 ë¼ì´ì„¼ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

## ë¬¸ì˜

- **ì´ë©”ì¼**: dasol@goodganglabs.com
- **GitHub Issues**: [goodganglabs/issues](https://github.com/your-username/goodganglabs/issues)

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-18
**ë²„ì „**: 1.0
