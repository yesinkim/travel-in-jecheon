# í•™ìŠµ ì¤‘ í‰ê°€ ê°€ì´ë“œ (Training Evaluation Guide)

## ëª©ì°¨
1. [í‰ê°€ ì§€í‘œ í•´ì„](#í‰ê°€-ì§€í‘œ-í•´ì„)
2. [í•™ìŠµ ê³¡ì„  ë¶„ì„](#í•™ìŠµ-ê³¡ì„ -ë¶„ì„)
3. [ë¬¸ì œ ìƒí™© ëŒ€ì‘](#ë¬¸ì œ-ìƒí™©-ëŒ€ì‘)
4. [ì²´í¬í¬ì¸íŠ¸ ì„ íƒ](#ì²´í¬í¬ì¸íŠ¸-ì„ íƒ)

---

## í‰ê°€ ì§€í‘œ í•´ì„

### ê¸°ë³¸ ì§€í‘œ

| ì§€í‘œ | ì„¤ëª… | ì¢‹ì€ ê°’ | ë‚˜ìœ ì‹ í˜¸ |
|------|------|---------|----------|
| `train_loss` | í•™ìŠµ ë°ì´í„° ì†ì‹¤ | ê°ì†Œ ì¶”ì„¸ | ì¦ê°€ or ì •ì²´ |
| `eval_loss` | ê²€ì¦ ë°ì´í„° ì†ì‹¤ | ê°ì†Œ ì¶”ì„¸ | ì¦ê°€ (overfitting) |
| `perplexity` | ì–¸ì–´ ëª¨ë¸ ì„±ëŠ¥ | ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ | ë†’ê±°ë‚˜ ì¦ê°€ |
| `learning_rate` | í˜„ì¬ í•™ìŠµë¥  | ì ì§„ì  ê°ì†Œ | ë„ˆë¬´ ë¹ ë¥¸ ë³€í™” |

### Loss ê°’ í•´ì„ (Qwen2.5-7B ê¸°ì¤€)

```
ì´ˆê¸° loss (epoch 0):     2.0 - 3.0  (ì •ìƒ)
ì¤‘ê°„ loss (epoch 1-2):   1.0 - 2.0  (í•™ìŠµ ì¤‘)
ìµœì¢… loss (epoch 3):     0.5 - 1.5  (ëª©í‘œ)

âš ï¸ ì£¼ì˜:
- Loss < 0.3: Overfitting ê°€ëŠ¥ì„± (ë„ˆë¬´ ë‚®ìŒ)
- Loss > 3.0: í•™ìŠµ ì•ˆ ë¨ (ì´ˆê¸° ìƒíƒœ)
- Eval loss > Train loss + 0.5: Overfitting
```

### Perplexity í•´ì„

```python
Perplexity = exp(loss)

ì˜ˆì‹œ:
- Loss 2.0 â†’ Perplexity 7.4  (ì´ˆê¸°)
- Loss 1.0 â†’ Perplexity 2.7  (ì¢‹ìŒ)
- Loss 0.5 â†’ Perplexity 1.6  (ë§¤ìš° ì¢‹ìŒ)

ì˜ë¯¸: "ëª¨ë¸ì´ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•  ë•Œ í‰ê· ì ìœ¼ë¡œ Nê°œ í›„ë³´ ì¤‘ ê³ ë¯¼"
ë‚®ì„ìˆ˜ë¡ í™•ì‹ ë„ê°€ ë†’ìŒ.
```

---

## í•™ìŠµ ê³¡ì„  ë¶„ì„

### 1. ì •ìƒ í•™ìŠµ (Ideal)

```
Step    Train Loss    Eval Loss    íŒë‹¨
----    ----------    ---------    ----
10      2.50          2.55         ì •ìƒ ì‹œì‘
50      2.10          2.15         âœ… ì •ìƒ
100     1.70          1.80         âœ… ì •ìƒ
150     1.40          1.50         âœ… ì •ìƒ
200     1.15          1.25         âœ… ì •ìƒ
250     0.95          1.10         âœ… ê³„ì† ì§„í–‰

íŠ¹ì§•:
- ë‘ loss ëª¨ë‘ ê°ì†Œ
- Gapì´ ì¼ì • (0.05 - 0.15)
- ì•ˆì •ì ì¸ ê°ì†Œ ì¶”ì„¸
```

### 2. Overfitting ì‹œì‘

```
Step    Train Loss    Eval Loss    íŒë‹¨
----    ----------    ---------    ----
10      2.50          2.55         ì •ìƒ
50      2.10          2.15         âœ… ì •ìƒ
100     1.70          1.80         âœ… ì •ìƒ
150     1.40          1.55         âš ï¸ gap ì¦ê°€
200     1.15          1.60         âŒ eval ì¦ê°€
250     0.90          1.75         âŒ overfitting!

ëŒ€ì‘:
â†’ Step 100-150 checkpoint ì‚¬ìš©
â†’ Regularization ì¶”ê°€ (dropout, weight decay)
â†’ Early stopping ì ìš©
```

### 3. Underfitting

```
Step    Train Loss    Eval Loss    íŒë‹¨
----    ----------    ---------    ----
10      2.50          2.55         ì •ìƒ
50      2.45          2.50         âš ï¸ ëŠë¦° ê°ì†Œ
100     2.40          2.45         âŒ ê±°ì˜ ë³€í™” ì—†ìŒ
150     2.35          2.42         âŒ underfitting

ëŒ€ì‘:
â†’ Learning rate ì¦ê°€ (2e-5 â†’ 5e-5)
â†’ ë” ë§ì€ epoch
â†’ ëª¨ë¸ í¬ê¸° ì¦ê°€ (3B â†’ 7B)
â†’ Batch size ì¡°ì •
```

### 4. Learning Rate ë¬¸ì œ

**ë„ˆë¬´ ë†’ìŒ:**
```
Step    Train Loss    íŒë‹¨
----    ----------    ----
10      2.50          ì •ìƒ
50      3.10          âŒ ì¦ê°€!
100     2.80          ë¶ˆì•ˆì •
150     3.50          âŒ ë°œì‚°

ëŒ€ì‘: LR 1/10ë¡œ ì¤„ì´ê¸° (2e-5 â†’ 2e-6)
```

**ë„ˆë¬´ ë‚®ìŒ:**
```
Step    Train Loss    íŒë‹¨
----    ----------    ----
10      2.50          ì •ìƒ
50      2.49          ê±°ì˜ ë³€í™” ì—†ìŒ
100     2.48          âŒ ë„ˆë¬´ ëŠë¦¼
150     2.47          ì‹œê°„ ë‚­ë¹„

ëŒ€ì‘: LR 2-5ë°° ì¦ê°€ (2e-5 â†’ 5e-5)
```

---

## ë¬¸ì œ ìƒí™© ëŒ€ì‘

### ìƒí™© 1: Lossê°€ NaN

```python
ë¬¸ì œ: train_loss = NaN

ì›ì¸:
1. Learning rate ë„ˆë¬´ ë†’ìŒ
2. Gradient explosion
3. ë°ì´í„°ì— inf/nan ê°’ í¬í•¨

í•´ê²°:
âœ… Learning rate 1/10ë¡œ ì¤„ì´ê¸°
âœ… Gradient clipping ì¶”ê°€:
   training_args = TrainingArguments(
       max_grad_norm=1.0,  # ì¶”ê°€
   )
âœ… ë°ì´í„° ê²€ì¦:
   dataset.filter(lambda x: x['text'] is not None)
```

### ìƒí™© 2: GPU Out of Memory

```python
ë¬¸ì œ: CUDA out of memory

í•´ê²°:
âœ… Batch size ì¤„ì´ê¸°: 4 â†’ 2 â†’ 1
âœ… Gradient accumulation ì¦ê°€:
   gradient_accumulation_steps=8  # effective batch = 1*8=8
âœ… Gradient checkpointing í™œì„±í™”:
   gradient_checkpointing=True
âœ… Mixed precision ì‚¬ìš©:
   bf16=True  # or fp16=True
âœ… Max length ì¤„ì´ê¸°:
   max_length=512 â†’ 256
```

### ìƒí™© 3: í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼

```python
ë¬¸ì œ: 1 epochì— 5ì‹œê°„ ì´ìƒ

í•´ê²°:
âœ… Batch size ì¦ê°€ (ë©”ëª¨ë¦¬ í—ˆìš© ì‹œ)
âœ… Dataloader workers ì¦ê°€:
   dataloader_num_workers=4
âœ… Pin memory í™œì„±í™”:
   dataloader_pin_memory=True
âœ… ë¶ˆí•„ìš”í•œ evaluation ì¤„ì´ê¸°:
   eval_steps=100  (50 â†’ 100)
âœ… ë¡œê¹… ì¤„ì´ê¸°:
   logging_steps=50  (10 â†’ 50)
```

### ìƒí™© 4: Validation Lossë§Œ ì¦ê°€

```python
ë¬¸ì œ: train_loss ê°ì†Œ, eval_loss ì¦ê°€

ì›ì¸: Overfitting (í›ˆë ¨ ë°ì´í„° ì•”ê¸°)

í•´ê²°:
âœ… Early stopping ì‚¬ìš©:
   load_best_model_at_end=True
âœ… Dropout ì¶”ê°€ (LoRA ì‚¬ìš© ì‹œ):
   lora_dropout=0.1
âœ… Weight decay ì¦ê°€:
   weight_decay=0.01  (0.0 â†’ 0.01)
âœ… Epoch ìˆ˜ ì¤„ì´ê¸°:
   num_epochs=2  (3 â†’ 2)
âœ… ë°ì´í„° ì¦ê°• (if possible)
```

---

## ì²´í¬í¬ì¸íŠ¸ ì„ íƒ

### ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ í™•ì¸

```bash
# í•™ìŠµ í›„ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡
ls -lh results/qwen-7b-jecheon/

ì¶œë ¥ ì˜ˆì‹œ:
checkpoint-50/      # Step 50
checkpoint-100/     # Step 100
checkpoint-150/     # Step 150  â† Best (eval_loss ìµœì €)
checkpoint-200/     # Step 200
final_model/        # ìµœì¢… ëª¨ë¸
```

### Best Checkpoint ì°¾ê¸°

```python
# trainer_state.jsonì—ì„œ best checkpoint í™•ì¸
import json

with open("results/qwen-7b-jecheon/trainer_state.json") as f:
    state = json.load(f)

# Best checkpoint ê²½ë¡œ
best_checkpoint = state["best_model_checkpoint"]
print(f"Best: {best_checkpoint}")

# ê° checkpointì˜ eval_loss
for log in state["log_history"]:
    if "eval_loss" in log:
        print(f"Step {log['step']}: eval_loss = {log['eval_loss']:.4f}")
```

### ìˆ˜ë™ ì„ íƒ ê¸°ì¤€

```python
ì²´í¬í¬ì¸íŠ¸ ì„ íƒ ìš°ì„ ìˆœìœ„:

1. Lowest eval_loss
   â†’ ê°€ì¥ ì¼ë°˜í™” ì„±ëŠ¥ ì¢‹ìŒ

2. Train/Eval loss gap ê°€ì¥ ì‘ìŒ
   â†’ ê°€ì¥ ì•ˆì •ì 

3. Sample ì˜ˆì¸¡ í’ˆì§ˆì´ ê°€ì¥ ì¢‹ìŒ
   â†’ ì‹¤ì œ ì‚¬ìš© ì„±ëŠ¥ ê³ ë ¤

ì˜ˆì‹œ:
checkpoint-100: eval_loss=1.2, gap=0.1, ì˜ˆì¸¡í’ˆì§ˆ=ë³´í†µ
checkpoint-150: eval_loss=1.1, gap=0.2, ì˜ˆì¸¡í’ˆì§ˆ=ì¢‹ìŒ  â† ì„ íƒ
checkpoint-200: eval_loss=1.3, gap=0.4, ì˜ˆì¸¡í’ˆì§ˆ=ë§¤ìš°ì¢‹ìŒ (overfit)
```

---

## ì‹¤ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•™ìŠµ ì‹œì‘ ì „

```bash
âœ… ë°ì´í„°ì…‹ í¬ê¸° í™•ì¸
   - Train: 100+ samples
   - Validation: 10-20 samples

âœ… GPU ë©”ëª¨ë¦¬ í™•ì¸
   nvidia-smi

âœ… ì„¤ì • í™•ì¸
   - Learning rate: 2e-5
   - Batch size: 4 (ë˜ëŠ” ë©”ëª¨ë¦¬ì— ë§ê²Œ)
   - Eval steps: 50
   - Max length: 512

âœ… ë¡œê¹… ì„¤ì •
   - Wandb ë¡œê·¸ì¸ í™•ì¸: wandb login
   - í”„ë¡œì íŠ¸ ì´ë¦„ ì„¤ì •
```

### í•™ìŠµ ì‹œì‘ í›„ (10 steps)

```bash
âœ… Loss ê°ì†Œ ì‹œì‘
   Step 1:  2.5
   Step 10: 2.3 âœ…

âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì•ˆì •
   nvidia-smi | grep python

âœ… No errors in logs
   tail -f nohup.out
```

### ì²« Evaluation (50 steps)

```bash
âœ… Eval loss í™•ì¸
   eval_loss < 3.0 âœ…

âœ… Train/Eval gap í™•ì¸
   |train_loss - eval_loss| < 0.5 âœ…

âœ… Sample ì˜ˆì¸¡ í™•ì¸
   ë‹µë³€ì´ ì˜ë¯¸ ìˆëŠ” ë¬¸ì¥ì¸ê°€? âœ…
```

### ë§¤ Evaluation

```bash
âœ… Loss ì¶”ì„¸
   eval_loss ê³„ì† ê°ì†Œ? âœ…
   train/eval gap ì¦ê°€í•˜ì§€ ì•ŠìŒ? âœ…

âœ… ìƒ˜í”Œ í’ˆì§ˆ
   ë‹µë³€ì´ ì ì  ê°œì„ ë˜ëŠ”ê°€? âœ…

âœ… Checkpoint ì €ì¥
   ìƒˆ checkpoint ìƒì„±ë˜ì—ˆëŠ”ê°€? âœ…
```

### í•™ìŠµ ì¢…ë£Œ í›„

```bash
âœ… Best checkpoint ì‹ë³„
   cat results/*/trainer_state.json | grep best_model

âœ… ìµœì¢… í‰ê°€
   python scripts/evaluate.py --checkpoint results/.../checkpoint-XXX

âœ… ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
   ì§ˆë¬¸ 3-5ê°œë¡œ ì‹¤ì œ ë‹µë³€ í’ˆì§ˆ í™•ì¸

âœ… ëª¨ë¸ ì €ì¥
   ì—…ë¡œë“œ ì „ ìµœì¢… í…ŒìŠ¤íŠ¸
```

---

## Weights & Biases í™œìš©

### ì„¤ì¹˜ ë° ë¡œê·¸ì¸

```bash
pip install wandb
wandb login
# API key ì…ë ¥: https://wandb.ai/authorize
```

### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

```bash
# í•™ìŠµ ì‹œì‘ í›„ ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸
https://wandb.ai/<username>/jecheon-rag-finetuning

í™•ì¸ í•­ëª©:
ğŸ“Š Charts:
   - train/loss
   - eval/loss
   - learning_rate

ğŸ“ˆ ë¹„êµ:
   - ì—¬ëŸ¬ ì‹¤í—˜ ë™ì‹œ ë¹„êµ
   - Hyperparameter ì˜í–¥ ë¶„ì„

ğŸ’¾ Artifacts:
   - Best checkpoint ìë™ ì €ì¥
   - ëª¨ë¸ ë²„ì „ ê´€ë¦¬
```

### ì¤‘ìš” ê·¸ë˜í”„

```python
1. Loss ê·¸ë˜í”„ (ê°€ì¥ ì¤‘ìš”!)
   Yì¶•: loss
   Xì¶•: step
   âœ… ê°ì†Œ ì¶”ì„¸
   âŒ ì¦ê°€ or ì •ì²´

2. Learning Rate Schedule
   Warmup â†’ Constant/Decay í™•ì¸

3. Gradient Norm
   ë„ˆë¬´ í¬ë©´ (>10) â†’ clipping í•„ìš”

4. Train/Eval Loss ë¹„êµ
   ë‘ ì„ ì´ ê°€ê¹Œì›Œì•¼ í•¨
   Gap ë²Œì–´ì§€ë©´ overfitting
```

---

## TensorBoard í™œìš© (ëŒ€ì•ˆ)

```bash
# í•™ìŠµ ì¤‘ ë³„ë„ í„°ë¯¸ë„ì—ì„œ
tensorboard --logdir ./results

# ë¸Œë¼ìš°ì €: http://localhost:6006

íƒ­ë³„ í™•ì¸ ì‚¬í•­:
ğŸ“Š SCALARS:
   - train/loss
   - eval/loss

ğŸ“ˆ GRAPHS:
   - ëª¨ë¸ êµ¬ì¡° ì‹œê°í™”

ğŸ“ HPARAMS:
   - Hyperparameter ë¹„êµ
```

---

## ìš”ì•½: ì¢‹ì€ í•™ìŠµì˜ ì‹ í˜¸

```
âœ… Train loss ê¾¸ì¤€íˆ ê°ì†Œ
âœ… Eval lossë„ í•¨ê»˜ ê°ì†Œ
âœ… Train/Eval gap < 0.5
âœ… ìƒ˜í”Œ ì˜ˆì¸¡ ì ì  ê°œì„ 
âœ… No NaN, no OOM errors
âœ… Perplexity < 3.0 (final)
âœ… Loss ì•ˆì •ì  (ì§„ë™ ì—†ìŒ)

ì´ 7ê°€ì§€ê°€ ëª¨ë‘ ë§Œì¡±ë˜ë©´ ì„±ê³µì ì¸ í•™ìŠµ!
```

---

## ì°¸ê³  ìë£Œ

- [HuggingFace Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
- [Weights & Biases Guide](https://docs.wandb.ai/)
- [Understanding Loss Curves](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)
