# LLM-as-a-Judge í‰ê°€ ê°€ì´ë“œ (Bench-RAG)

ì´ ë¬¸ì„œëŠ” Bench-RAG ìŠ¤íƒ€ì¼ì˜ LLM-as-a-Judge í‰ê°€ ì‹œìŠ¤í…œ ì‚¬ìš©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­ (4ê°€ì§€)

### 1. Accuracy (ì •í™•ì„±)
- **íƒ€ì…:** Boolean (true/false)
- **ì¸¡ì •:** ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€í–ˆëŠ”ì§€ (í™˜ê° ì—¬ë¶€)
- **íŒì •:**
  - `true` = ë¬¸ì„œ ë‚´ìš©ë§Œ ì‚¬ìš©, ì¶”ê°€ ì •ë³´ ì—†ìŒ
  - `false` = ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€í•¨ (í™˜ê° ë°œìƒ)

**ì˜ˆì‹œ:**
```
ë¬¸ì„œ: "ì˜ë¦¼ì§€ëŠ” ì œì²œì‹œ ì†¡í•™ë©´ì— ìœ„ì¹˜í•©ë‹ˆë‹¤."
ì§ˆë¬¸: "ì˜ë¦¼ì§€ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?"

âœ“ ì¢‹ì€ ë‹µë³€: "ì œì²œì‹œ ì†¡í•™ë©´ì— ìœ„ì¹˜í•©ë‹ˆë‹¤." â†’ accuracy: true
âœ— ë‚˜ìœ ë‹µë³€: "ì œì²œì‹œ ì†¡í•™ë©´ì— ìœ„ì¹˜í•˜ë©°, ì…ì¥ë£ŒëŠ” ë¬´ë£Œì…ë‹ˆë‹¤." â†’ accuracy: false
  (ì…ì¥ë£Œ ì •ë³´ëŠ” ë¬¸ì„œì— ì—†ìŒ)
```

### 2. Helpfulness (ìœ ìš©ì„±)
- **íƒ€ì…:** Integer (1-10)
- **ì¸¡ì •:** ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ë„ì›€ì´ ë˜ëŠ”ì§€
- **ì ìˆ˜:**
  - 1-3: ë„ì›€ ì•ˆ ë¨
  - 4-6: ë³´í†µ
  - 7-8: ë„ì›€ ë¨
  - 9-10: ë§¤ìš° ë„ì›€ ë¨

### 3. Relevance (ê´€ë ¨ì„±)
- **íƒ€ì…:** Integer (1-10)
- **ì¸¡ì •:** ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ì í•©í•œ ë‹µë³€ì¸ì§€
- **ì ìˆ˜:**
  - 1-3: ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ìŒ
  - 4-6: ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨
  - 7-8: ê´€ë ¨ì„± ë†’ìŒ
  - 9-10: ì™„ë²½í•˜ê²Œ ë‹µë³€

### 4. Depth (ê¹Šì´)
- **íƒ€ì…:** Integer (1-10)
- **ì¸¡ì •:** ë‹µë³€ì˜ ìƒì„¸ë„ì™€ ê¹Šì´
- **ì ìˆ˜:**
  - 1-3: ë„ˆë¬´ ì§§ê±°ë‚˜ í”¼ìƒì 
  - 4-6: ê¸°ë³¸ì ì¸ ì •ë³´ í¬í•¨
  - 7-8: ìƒì„¸í•œ ì •ë³´
  - 9-10: ë§¤ìš° ìƒì„¸í•˜ê³  í¬ê´„ì 

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ì „ì²´ ì›Œí¬í”Œë¡œìš°

```
ëª¨ë¸ ì¶”ë¡  ê²°ê³¼
    â†“
[1] prepare_predictions_for_judge.py (í˜•ì‹ ë³€í™˜)
    â†“
Judge í˜•ì‹ ë°ì´í„°
    â†“
[2] evaluate_with_llm_judge.py (LLM í‰ê°€)
    â†“
í‰ê°€ ê²°ê³¼ (Accuracy, Helpfulness, Relevance, Depth)
```

### Step 1: ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì¤€ë¹„

ë¨¼ì € ëª¨ë¸ì˜ ì¶”ë¡  ê²°ê³¼ê°€ í•„ìš”í•©ë‹ˆë‹¤:

**í•„ìš”í•œ íŒŒì¼:**
- `test_data.jsonl`: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ (ì§ˆë¬¸, ì •ë‹µ ë¬¸ì„œ, ì •ë‹µ)
- `model_predictions.jsonl`: ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€

**test_data.jsonl í˜•ì‹:**
```json
{
  "question": "ì˜ë¦¼ì§€ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?",
  "answer": "ì œì²œì‹œ ì†¡í•™ë©´ ì˜ë¦¼ëŒ€ë¡œ 47ê¸¸ 7ì— ìœ„ì¹˜í•©ë‹ˆë‹¤.",
  "documents": [
    {
      "doc_id": "doc_001",
      "content": "ì˜ë¦¼ì§€ëŠ” ì œì²œì‹œ ì†¡í•™ë©´ ì˜ë¦¼ëŒ€ë¡œ 47ê¸¸ 7ì— ìœ„ì¹˜...",
      "is_correct": true
    },
    {
      "doc_id": "doc_002",
      "content": "ì²­í’í˜¸ëŠ” ì¶©ì£¼ëŒ ê±´ì„¤ë¡œ...",
      "is_correct": false
    }
  ]
}
```

**model_predictions.jsonl í˜•ì‹:**
```json
{
  "question": "ì˜ë¦¼ì§€ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?",
  "generated_answer": "ì˜ë¦¼ì§€ëŠ” ì œì²œì‹œ ì†¡í•™ë©´ ì˜ë¦¼ëŒ€ë¡œ 47ê¸¸ 7ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤."
}
```

### Step 2: Judge í˜•ì‹ìœ¼ë¡œ ë³€í™˜

```bash
python scripts/prepare_predictions_for_judge.py \
  --dataset data/processed/test_data.jsonl \
  --predictions outputs/baseline_predictions.jsonl \
  --output outputs/baseline_for_judge.jsonl
```

**ì¶œë ¥ í˜•ì‹ (for_judge.jsonl):**
```json
{
  "filename": "ì œì²œì‹œê´€ê´‘ì •ë³´ì±…ì.pdf",
  "content": "ì˜ë¦¼ì§€ëŠ” ì œì²œì‹œ ì†¡í•™ë©´ ì˜ë¦¼ëŒ€ë¡œ 47ê¸¸ 7ì— ìœ„ì¹˜...",
  "question": "ì˜ë¦¼ì§€ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?",
  "response": "ì˜ë¦¼ì§€ëŠ” ì œì²œì‹œ ì†¡í•™ë©´ ì˜ë¦¼ëŒ€ë¡œ 47ê¸¸ 7ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤."
}
```

### Step 3: LLM Judgeë¡œ í‰ê°€

```bash
python scripts/evaluate_with_llm_judge.py \
  --predictions outputs/baseline_for_judge.jsonl \
  --output outputs/baseline_judge_results.jsonl \
  --model gemini-2.0-flash-exp
```

**ì˜µì…˜:**
- `--predictions`: Judge í˜•ì‹ ì˜ˆì¸¡ íŒŒì¼ ê²½ë¡œ
- `--output`: ê²°ê³¼ ì €ì¥ ê²½ë¡œ
- `--model`: ì‚¬ìš©í•  Gemini ëª¨ë¸ (ê¸°ë³¸: gemini-2.0-flash-exp)
- `--limit`: í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¼ë¶€ë§Œ í‰ê°€ (ì˜ˆ: --limit 10)

**í‰ê°€ ì‹œê°„:**
- ìƒ˜í”Œ 1ê°œë‹¹ **4ë²ˆì˜ API í˜¸ì¶œ** (Accuracy, Helpfulness, Relevance, Depth)
- 30ê°œ ìƒ˜í”Œ = 120ë²ˆ API í˜¸ì¶œ â‰ˆ **2-3ë¶„ ì†Œìš”**

### Step 4: ê²°ê³¼ í™•ì¸

**ì½˜ì†” ì¶œë ¥:**
```
============================================================
LLM-AS-A-JUDGE EVALUATION RESULTS (Bench-RAG)
============================================================

ğŸ“Š Aggregate Metrics:
  Accuracy Rate:     85.00%
  Avg Helpfulness:   8.50/10
  Avg Relevance:     9.20/10
  Avg Depth:         7.80/10

ğŸ“ˆ Evaluation Stats:
  Successfully Evaluated: 30
  Failed Evaluations:     0

============================================================
```

**ê²°ê³¼ íŒŒì¼ (baseline_judge_results.jsonl):**
```json
{
  "filename": "ì œì²œì‹œê´€ê´‘ì •ë³´ì±…ì.pdf",
  "question": "ì˜ë¦¼ì§€ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?",
  "response": "ì˜ë¦¼ì§€ëŠ” ì œì²œì‹œ ì†¡í•™ë©´ ì˜ë¦¼ëŒ€ë¡œ 47ê¸¸ 7ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.",
  "accuracy": true,
  "accuracy_explanation": "ì‘ë‹µì€ ì œê³µëœ ì •ë³´ì—ë§Œ ê¸°ë°˜í•˜ì—¬ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ì¶”ê°€ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
  "helpfulness": 9,
  "helpfulness_explanation": "ì‘ë‹µì€ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ì£¼ì†Œë¥¼ ì œê³µí•˜ì—¬ ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.",
  "relevance": 10,
  "relevance_explanation": "ì‘ë‹µì€ ì§ˆë¬¸ì— ì™„ë²½í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.",
  "depth": 7,
  "depth_explanation": "ì‘ë‹µì€ ê°„ê²°í•˜ì§€ë§Œ í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
}
```

**í†µê³„ íŒŒì¼ (baseline_judge_results_aggregate.json):**
```json
{
  "accuracy_rate": 0.85,
  "avg_helpfulness": 8.5,
  "avg_relevance": 9.2,
  "avg_depth": 7.8,
  "num_evaluated": 30,
  "num_failed": 0
}
```

---

## ğŸ“ˆ Baseline vs Fine-tuned ë¹„êµ

### 1. Baseline ëª¨ë¸ í‰ê°€

```bash
# Step 1: í˜•ì‹ ë³€í™˜
python scripts/prepare_predictions_for_judge.py \
  --dataset data/processed/test_data.jsonl \
  --predictions outputs/baseline_predictions.jsonl \
  --output outputs/baseline_for_judge.jsonl

# Step 2: í‰ê°€
python scripts/evaluate_with_llm_judge.py \
  --predictions outputs/baseline_for_judge.jsonl \
  --output outputs/baseline_judge_results.jsonl
```

### 2. Fine-tuned ëª¨ë¸ í‰ê°€

```bash
# Step 1: í˜•ì‹ ë³€í™˜
python scripts/prepare_predictions_for_judge.py \
  --dataset data/processed/test_data.jsonl \
  --predictions outputs/finetuned_predictions.jsonl \
  --output outputs/finetuned_for_judge.jsonl

# Step 2: í‰ê°€
python scripts/evaluate_with_llm_judge.py \
  --predictions outputs/finetuned_for_judge.jsonl \
  --output outputs/finetuned_judge_results.jsonl
```

### 3. ê²°ê³¼ ë¹„êµ

**ë¹„êµ ìŠ¤í¬ë¦½íŠ¸ (ê°„ë‹¨í•œ Python):**
```python
import json

# Load aggregate results
with open('outputs/baseline_judge_results_aggregate.json') as f:
    baseline = json.load(f)

with open('outputs/finetuned_judge_results_aggregate.json') as f:
    finetuned = json.load(f)

# Compare
print("Metric           | Baseline | Fine-tuned | Improvement")
print("-----------------|----------|------------|------------")
print(f"Accuracy Rate    | {baseline['accuracy_rate']:.2%}   | {finetuned['accuracy_rate']:.2%}     | +{(finetuned['accuracy_rate']-baseline['accuracy_rate'])*100:.1f}pp")
print(f"Helpfulness      | {baseline['avg_helpfulness']:.2f}/10  | {finetuned['avg_helpfulness']:.2f}/10    | +{finetuned['avg_helpfulness']-baseline['avg_helpfulness']:.2f}")
print(f"Relevance        | {baseline['avg_relevance']:.2f}/10  | {finetuned['avg_relevance']:.2f}/10    | +{finetuned['avg_relevance']-baseline['avg_relevance']:.2f}")
print(f"Depth            | {baseline['avg_depth']:.2f}/10  | {finetuned['avg_depth']:.2f}/10    | +{finetuned['avg_depth']-baseline['avg_depth']:.2f}")
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Metric           | Baseline | Fine-tuned | Improvement
-----------------|----------|------------|------------
Accuracy Rate    | 75.00%   | 90.00%     | +15.0pp
Helpfulness      | 7.20/10  | 8.80/10    | +1.60
Relevance        | 8.00/10  | 9.30/10    | +1.30
Depth            | 6.50/10  | 8.20/10    | +1.70
```

---

## ğŸ’¡ Tips & Best Practices

### 1. ìƒ˜í”Œë§ ì „ëµ

**í…ŒìŠ¤íŠ¸ìš© (ë¹ ë¥¸ í™•ì¸):**
```bash
# ì²˜ìŒ 10ê°œë§Œ í‰ê°€
python scripts/evaluate_with_llm_judge.py \
  --predictions outputs/for_judge.jsonl \
  --output outputs/test_results.jsonl \
  --limit 10
```

**ìµœì¢… í‰ê°€ (ì „ì²´):**
```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€
python scripts/evaluate_with_llm_judge.py \
  --predictions outputs/for_judge.jsonl \
  --output outputs/final_results.jsonl
```

### 2. API ë¹„ìš© ì ˆê°

**ì¶”ì²œ ëª¨ë¸ ìˆœì„œ (ë¹ ë¦„ â†’ ëŠë¦¼, ì €ë ´ â†’ ë¹„ìŒˆ):**
1. `gemini-2.0-flash-exp` (ê¸°ë³¸, ë¹ ë¥´ê³  ì €ë ´)
2. `gemini-1.5-flash` (ì•ˆì •ì )
3. `gemini-1.5-pro` (ê³ í’ˆì§ˆ, ë¹„ìŒˆ)

**ì˜ˆìƒ ë¹„ìš© (30ê°œ ìƒ˜í”Œ):**
- gemini-2.0-flash: **ë¬´ë£Œ** (í˜„ì¬ ì‹¤í—˜ ëª¨ë¸)
- gemini-1.5-flash: ~$0.01-0.02
- gemini-1.5-pro: ~$0.05-0.10

### 3. ì¬ì‹œë„ ë¡œì§

LLM JudgeëŠ” ìë™ ì¬ì‹œë„ ê¸°ëŠ¥ì´ ë‚´ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
- ì‹¤íŒ¨ ì‹œ 3ë²ˆê¹Œì§€ ì¬ì‹œë„
- ì§€ìˆ˜ ë°±ì˜¤í”„ (2ì´ˆ, 4ì´ˆ, 8ì´ˆ)
- 3ë²ˆ ì‹¤íŒ¨ í›„ í•´ë‹¹ ìƒ˜í”Œì€ ê²°ê³¼ì— `null` ê°’ìœ¼ë¡œ ì €ì¥

### 4. ê²°ê³¼ ë¶„ì„

**ì •ì„±ì  ë¶„ì„:**
```python
import json

# Load detailed results
with open('outputs/judge_results.jsonl') as f:
    results = [json.loads(line) for line in f]

# Find low accuracy cases
low_accuracy = [r for r in results if not r['accuracy']]

print(f"Found {len(low_accuracy)} hallucination cases:")
for case in low_accuracy[:5]:
    print(f"\nQuestion: {case['question']}")
    print(f"Response: {case['response']}")
    print(f"Explanation: {case['accuracy_explanation']}")
```

---

## ğŸ”§ Troubleshooting

### ë¬¸ì œ 1: API Key ì—ëŸ¬
```
ValueError: GOOGLE_API_KEY environment variable not set
```

**í•´ê²°:**
```bash
export GOOGLE_API_KEY="your-api-key-here"
```

ë˜ëŠ” `.env` íŒŒì¼ì— ì¶”ê°€:
```
GOOGLE_API_KEY=your-api-key-here
```

### ë¬¸ì œ 2: JSON íŒŒì‹± ì—ëŸ¬
```
json.decoder.JSONDecodeError: Expecting value
```

**ì›ì¸:** LLMì´ JSONì´ ì•„ë‹Œ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
**í•´ê²°:**
- ë” ì•ˆì •ì ì¸ ëª¨ë¸ ì‚¬ìš© (gemini-1.5-flash)
- ì¬ì‹œë„ ë¡œì§ì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬

### ë¬¸ì œ 3: Rate Limit ì—ëŸ¬
```
429 Resource Exhausted
```

**í•´ê²°:**
- `llm_judge.py`ì˜ `time.sleep(0.5)` ê°’ì„ ëŠ˜ë¦¬ê¸° (ì˜ˆ: 1.0)
- ë˜ëŠ” `--limit` ì˜µì…˜ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°

---

## ğŸ“Š ë¦¬í¬íŠ¸ ì‘ì„± ì˜ˆì‹œ

### Bench-RAG í‰ê°€ ê²°ê³¼ (ë¦¬í¬íŠ¸ í¬í•¨ ë‚´ìš©)

**í‘œ 1: ì •ëŸ‰ì  í‰ê°€ ê²°ê³¼**

| Metric | Baseline | Fine-tuned | Improvement |
|--------|----------|------------|-------------|
| Accuracy Rate | 75.0% | 90.0% | +15.0pp |
| Helpfulness | 7.2/10 | 8.8/10 | +1.6 |
| Relevance | 8.0/10 | 9.3/10 | +1.3 |
| Depth | 6.5/10 | 8.2/10 | +1.7 |

**í‘œ 2: ì •ì„±ì  ë¶„ì„ (3ê°€ì§€ ì˜ˆì‹œ)**

**ì˜ˆì‹œ 1: í™˜ê° ê°ì†Œ**
- **ì§ˆë¬¸:** "ì˜ë¦¼ì§€ ì…ì¥ë£ŒëŠ” ì–¼ë§ˆì¸ê°€ìš”?"
- **Baseline:** "ì˜ë¦¼ì§€ ì…ì¥ë£ŒëŠ” ë¬´ë£Œì…ë‹ˆë‹¤." âŒ (Accuracy: false)
- **Fine-tuned:** "ì œê³µëœ ì •ë³´ì—ëŠ” ì…ì¥ë£Œì— ëŒ€í•œ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤." âœ“ (Accuracy: true)

**ì˜ˆì‹œ 2: ìƒì„¸ë„ ê°œì„ **
- **ì§ˆë¬¸:** "ì²­í’í˜¸ë°˜ ì¼€ì´ë¸”ì¹´ëŠ” ì–¼ë§ˆë‚˜ ê¸¸ì–´ìš”?"
- **Baseline:** "ì²­í’í˜¸ë°˜ ì¼€ì´ë¸”ì¹´ëŠ” 2.3kmì…ë‹ˆë‹¤." (Depth: 5/10)
- **Fine-tuned:** "ì²­í’í˜¸ë°˜ ì¼€ì´ë¸”ì¹´ëŠ” ì™•ë³µ 2.3kmì˜ ê±°ë¦¬ë¥¼ ìš´í–‰í•˜ë©°, ì²­í’í˜¸ì˜ ì•„ë¦„ë‹¤ìš´ ê²½ì¹˜ë¥¼ ê°ìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤." (Depth: 8/10)

**ì˜ˆì‹œ 3: ê´€ë ¨ì„± í–¥ìƒ**
- **ì§ˆë¬¸:** "ì œì²œì—ì„œ ê°€ì„ì— ê°€ê¸° ì¢‹ì€ ê³³ì€?"
- **Baseline:** "ì œì²œì—ëŠ” ì˜ë¦¼ì§€, ì²­í’í˜¸ ë“± ë§ì€ ê´€ê´‘ì§€ê°€ ìˆìŠµë‹ˆë‹¤." (Relevance: 6/10)
- **Fine-tuned:** "ê°€ì„ì—ëŠ” ë°±ìš´ê¶Œ íë§ ì½”ìŠ¤ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤. ë‹¨í’ì´ ì•„ë¦„ë‹¤ìš´ ì›”ì•…ì‚°ê³¼ ì²­í’í˜¸ë°˜ì„ ë‘˜ëŸ¬ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤." (Relevance: 9/10)

---

## ğŸ“ Summary

**LLM-as-a-Judge í‰ê°€ëŠ”:**
- âœ… ìë™ ë©”íŠ¸ë¦­(ROUGE, BERTScore)ë³´ë‹¤ **ì¸ê°„ í‰ê°€ì— ê°€ê¹Œì›€**
- âœ… **í™˜ê°(hallucination) íƒì§€**ì— íŠ¹íˆ íš¨ê³¼ì 
- âœ… **ìƒì„¸í•œ ì„¤ëª…**ì„ ì œê³µí•˜ì—¬ ì •ì„±ì  ë¶„ì„ ê°€ëŠ¥
- âš ï¸ API í˜¸ì¶œ ë¹„ìš© ë°œìƒ (í•˜ì§€ë§Œ gemini-flashëŠ” ì €ë ´)
- âš ï¸ í‰ê°€ ì‹œê°„ì´ ê¸¸ìŒ (30ê°œ = 2-3ë¶„)

**ì¶”ì²œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
1. **ë¹ ë¥¸ ìë™ í‰ê°€:** ROUGE + BERTScore (metrics.py)
2. **ì‹¬ì¸µ í’ˆì§ˆ í‰ê°€:** LLM-as-a-Judge (llm_judge.py)
3. **ë¦¬í¬íŠ¸ ì‘ì„±:** ë‘˜ ë‹¤ ì‚¬ìš© + ì •ì„±ì  ì˜ˆì‹œ ë¶„ì„

---

**ì‘ì„±ì¼:** 2025-11-18
**ë²„ì „:** 1.0
**ì—°ë½ì²˜:** ê³¼ì œ ê´€ë ¨ ë¬¸ì˜ - dasol@goodganglabs.com
