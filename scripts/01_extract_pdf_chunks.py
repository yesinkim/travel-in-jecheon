"""
Document Chunking Script for Jecheon Tourism Dataset

This script extracts meaningful chunks from the Jecheon tourism markdown file
and creates a structured documents.jsonl file.

Output: data/chunks/documents.jsonl
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Any


class JecheonDocumentChunker:
    """Extracts and chunks Jecheon tourism information."""

    CATEGORIES = {
        "transportation": ["ì‹œí‹°íˆ¬ì–´", "ê´€ê´‘íƒì‹œ", "ê´€ê´‘ì£¼ë¯¼ì¦"],
        "tourism": ["ì˜ë¦¼ì§€", "ì²­í’", "ë°°ë¡ ", "ë°•ë‹¬ì¬", "ì˜¥ìˆœë´‰", "ì¼€ì´ë¸”ì¹´"],
        "food": ["ë§›ì§‘", "ì‹ë‹¹", "ìŒì‹"],
        "accommodation": ["ìˆ™ë°•", "ë¦¬ì¡°íŠ¸", "í˜¸í…”", "ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤"],
        "activity": ["íŠ¸ë ˆí‚¹", "ì²´í—˜", "ì¶•ì œ", "ë ˆì €"],
        "culture": ["ë°•ë¬¼ê´€", "ì„±ì§€", "ë¬¸í™”", "ì—­ì‚¬"],
        "course": ["ì½”ìŠ¤", "ì—¬í–‰", "ì¶”ì²œ"],
        "benefit": ["ì¸ì„¼í‹°ë¸Œ", "í• ì¸", "í˜œíƒ", "ê¸°ë¶€"],
    }

    def __init__(self, markdown_path: str):
        """Initialize chunker with markdown file path."""
        self.markdown_path = Path(markdown_path)
        self.chunks = []

    def read_markdown(self) -> str:
        """Read markdown file."""
        with open(self.markdown_path, 'r', encoding='utf-8') as f:
            return f.read()

    def clean_text(self, text: str) -> str:
        """Clean text by removing line numbers and extra whitespace."""
        # Remove line numbers (e.g., "    1â†’")
        text = re.sub(r'^\s*\d+â†’', '', text, flags=re.MULTILINE)
        # Remove multiple blank lines
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        # Strip leading/trailing whitespace
        text = text.strip()
        return text

    def categorize_chunk(self, content: str, title: str) -> str:
        """Determine the category of a chunk based on its content and title."""
        text = (title + " " + content).lower()

        category_scores = {}
        for category, keywords in self.CATEGORIES.items():
            score = sum(1 for keyword in keywords if keyword.lower() in text)
            if score > 0:
                category_scores[category] = score

        if category_scores:
            return max(category_scores.items(), key=lambda x: x[1])[0]
        return "general"

    def extract_chunks(self) -> List[Dict[str, Any]]:
        """Extract meaningful chunks from the markdown content."""
        content = self.read_markdown()
        content = self.clean_text(content)

        # Split by major sections
        sections = []

        # Manual parsing based on content structure
        chunks_data = [
            # Transportation & Services
            {
                "title": "ë””ì§€í„¸ê´€ê´‘ì£¼ë¯¼ì¦",
                "category": "transportation",
                "content": self._extract_section(content, "ë””ì§€í„¸ ê´€ê´‘ì£¼ë¯¼ì¦", "ì œì²œ ì‹œí‹°íˆ¬ì–´"),
                "page": 4,
            },
            {
                "title": "ì œì²œ ì‹œí‹°íˆ¬ì–´",
                "category": "transportation",
                "content": self._extract_section(content, "ì œì²œ ì‹œí‹°íˆ¬ì–´", "ì œì²œ ê´€ê´‘íƒì‹œ"),
                "page": 5,
            },
            {
                "title": "ì œì²œ ê´€ê´‘íƒì‹œ",
                "category": "transportation",
                "content": self._extract_section(content, "ì œì²œ ê´€ê´‘íƒì‹œ", "ë‹¨ì²´ê´€ê´‘ê° ìœ ì¹˜ ì¸ì„¼í‹°ë¸Œ"),
                "page": 6,
            },
            {
                "title": "ë‹¨ì²´ê´€ê´‘ê° ì¸ì„¼í‹°ë¸Œ",
                "category": "benefit",
                "content": self._extract_section(content, "ë‹¨ì²´ê´€ê´‘ê° ìœ ì¹˜ ì¸ì„¼í‹°ë¸Œ", "ê°€ìŠ¤íŠ¸ë¡œ íˆ¬ì–´"),
                "page": 7,
            },
            {
                "title": "ê°€ìŠ¤íŠ¸ë¡œ íˆ¬ì–´",
                "category": "food",
                "content": self._extract_section(content, "ê°€ìŠ¤íŠ¸ë¡œ íˆ¬ì–´", "ëª¨ë°”ì¼ ë°”ë¡œê°€ê¸°"),
                "page": 8,
            },

            # Tourism Sites - Major
            {
                "title": "ì˜ë¦¼ì§€Â·ì˜ë¦¼ì§€ì—­ì‚¬ë°•ë¬¼ê´€",
                "category": "tourism",
                "content": self._extract_section(content, "ì˜ë¦¼ì§€Â·ì˜ë¦¼ì§€ì—­ì‚¬ë°•ë¬¼ê´€", "ë°°ë¡ ì„±ì§€"),
                "page": 12,
                "location": "ì†¡í•™ë©´",
                "address": "ì œì²œì‹œ ì†¡í•™ë©´ ì˜ë¦¼ëŒ€ë¡œ 47ê¸¸ 7",
            },
            {
                "title": "ë°°ë¡ ì„±ì§€",
                "category": "culture",
                "content": self._extract_section(content, "ë°°ë¡ ì„±ì§€", "ë°•ë‹¬ì¬"),
                "page": 12,
                "location": "ë´‰ì–‘ì",
                "address": "ì œì²œì‹œ ë´‰ì–‘ì ë°°ë¡ ì„±ì§€ê¸¸ 296",
            },
            {
                "title": "ë°•ë‹¬ì¬",
                "category": "tourism",
                "content": self._extract_section(content, "ë°•ë‹¬ì¬", "ì œì²œí•œë°©ì—‘ìŠ¤í¬ ê³µì›"),
                "page": 12,
                "location": "ë°±ìš´ë©´",
                "address": "ì œì²œì‹œ ë°±ìš´ë©´ ë°•ë‹¬ë¡œ 231",
            },
            {
                "title": "ì œì²œí•œë°©ì—‘ìŠ¤í¬ ê³µì›",
                "category": "culture",
                "content": self._extract_section(content, "ì œì²œí•œë°©ì—‘ìŠ¤í¬ ê³µì›", "ì˜ë¦¼ì§€ ìˆ˜ë¦¬ê³µì›"),
                "page": 12,
                "address": "ì œì²œì‹œ í•œë°©ì—‘ìŠ¤í¬ë¡œ 19",
            },
            {
                "title": "ì²­í’í˜¸ë°˜ ì¼€ì´ë¸”ì¹´",
                "category": "tourism",
                "content": self._extract_section(content, "ì²­í’í˜¸ë°˜ ì¼€ì´ë¸”ì¹´", "ì²­í’ë¬¸í™”ìœ ì‚°ë‹¨ì§€"),
                "page": 14,
                "location": "ì²­í’ë©´",
                "address": "ì œì²œì‹œ ì²­í’ë©´ ë¬¸í™”ì¬ê¸¸ 166",
            },
            {
                "title": "ì²­í’ë¬¸í™”ìœ ì‚°ë‹¨ì§€",
                "category": "culture",
                "content": self._extract_section(content, "ì²­í’ë¬¸í™”ìœ ì‚°ë‹¨ì§€", "ì²­í’ëœë“œ"),
                "page": 14,
                "location": "ì²­í’ë©´",
                "address": "ì œì²œì‹œ ì²­í’í˜¸ë¡œ 2048",
            },
            {
                "title": "ì²­í’ëœë“œ",
                "category": "activity",
                "content": self._extract_section(content, "ì²­í’ëœë“œ", "ì²­í’í˜¸ ìë“œë½ê¸¸"),
                "page": 14,
                "location": "ì²­í’ë©´",
                "address": "ì œì²œì‹œ ì²­í’ë©´ ì²­í’í˜¸ë¡œ50ê¸¸ 6",
            },
            {
                "title": "ì˜¥ìˆœë´‰ ì¶œë ë‹¤ë¦¬",
                "category": "tourism",
                "content": self._extract_section(content, "ì˜¥ìˆœë´‰ ì¶œë ë‹¤ë¦¬", "ì¶©ì£¼í˜¸ í¬ë£¨ì¦ˆ"),
                "page": 14,
                "location": "ìˆ˜ì‚°ë©´",
                "address": "ì œì²œì‹œ ìˆ˜ì‚°ë©´ ì˜¥ìˆœë´‰ë¡œ342",
            },

            # Activities & Experiences
            {
                "title": "íŠ¸ë ˆí‚¹Â·ê±·ê¸° ì¢‹ì€ ê³³",
                "category": "activity",
                "content": self._extract_section(content, "íŠ¸ë˜í‚¹Â·ê±·ê¸° ì¢‹ì€ê³³", "ì½”ìŠ¤ì—¬í–‰ ì¶”ì²œ"),
                "page": 16,
            },

            # Travel Courses
            {
                "title": "1ì¼ ì½”ìŠ¤",
                "category": "course",
                "content": self._extract_section(content, "1ì¼ ì½”ìŠ¤", "1ë°• 2ì¼ ì½”ìŠ¤"),
                "page": 17,
            },
            {
                "title": "1ë°• 2ì¼ ì½”ìŠ¤",
                "category": "course",
                "content": self._extract_section(content, "1ë°• 2ì¼ ì½”ìŠ¤", "íœ´ì–‘Â·íë§ ì½”ìŠ¤"),
                "page": 17,
            },
            {
                "title": "ìŠ¬ë¡œì‹œí‹° ì½”ìŠ¤",
                "category": "course",
                "content": self._extract_section(content, "#ìŠ¬ë¡œì‹œí‹° ì½”ìŠ¤", "#ë¶ë¶€Â·ì„œë¶€ê¶Œ ì½”ìŠ¤"),
                "page": 17,
            },
            {
                "title": "ë¶ë¶€Â·ì„œë¶€ê¶Œ ì½”ìŠ¤",
                "category": "course",
                "content": self._extract_section(content, "#ë¶ë¶€Â·ì„œë¶€ê¶Œ ì½”ìŠ¤", "ë¬¸í™”Â·ì—­ì‚¬ ì½”ìŠ¤"),
                "page": 17,
            },

            # Food
            {
                "title": "ì œì²œë§›ì§‘ ì†Œê°œ",
                "category": "food",
                "content": self._extract_section(content, "ì œì²œë§›ì§‘", "ì£¼ìš” ìˆ™ë°•ì‹œì„¤"),
                "page": 20,
            },

            # Accommodation
            {
                "title": "ì£¼ìš” ìˆ™ë°•ì‹œì„¤",
                "category": "accommodation",
                "content": self._extract_section(content, "ì£¼ìš” ìˆ™ë°•ì‹œì„¤", "ê³ í–¥ì‚¬ë‘ ê¸°ë¶€ì œ"),
                "page": 22,
            },

            # Benefits & Tips
            {
                "title": "ê³ í–¥ì‚¬ë‘ ê¸°ë¶€ì œ",
                "category": "benefit",
                "content": self._extract_section(content, "ê³ í–¥ì‚¬ë‘ ê¸°ë¶€ì œ", "ì•Œì•„ë‘ë©´ ë„ì›€ë˜ëŠ” ê¿€íŒ"),
                "page": 24,
            },
            {
                "title": "ì•Œì•„ë‘ë©´ ì¢‹ì€ ê¿€íŒ",
                "category": "benefit",
                "content": self._extract_section(content, "ì•Œì•„ë‘ë©´ ë„ì›€ë˜ëŠ” ê¿€íŒ", "Travel in Jecheon"),
                "page": 26,
            },
        ]

        # Create document chunks
        for idx, chunk_data in enumerate(chunks_data, start=1):
            if chunk_data["content"] and len(chunk_data["content"].strip()) > 50:
                doc_id = f"doc_{idx:03d}"
                self.chunks.append({
                    "doc_id": doc_id,
                    "title": chunk_data["title"],
                    "category": chunk_data["category"],
                    "content": chunk_data["content"].strip(),
                    "metadata": {
                        "page": chunk_data.get("page", 0),
                        "location": chunk_data.get("location", ""),
                        "address": chunk_data.get("address", ""),
                    },
                    "filename": f"{doc_id}_{chunk_data['title']}.txt",
                })

        return self.chunks

    def _extract_section(self, content: str, start_marker: str, end_marker: str) -> str:
        """Extract content between two markers."""
        try:
            start_idx = content.find(start_marker)
            if start_idx == -1:
                return ""

            end_idx = content.find(end_marker, start_idx + len(start_marker))
            if end_idx == -1:
                # Take rest of content if no end marker
                section = content[start_idx:]
            else:
                section = content[start_idx:end_idx]

            return section.strip()
        except Exception as e:
            print(f"Error extracting section {start_marker}: {e}")
            return ""

    def save_to_jsonl(self, output_path: str):
        """Save chunks to JSONL file."""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            for chunk in self.chunks:
                f.write(json.dumps(chunk, ensure_ascii=False) + '\n')

        print(f"âœ… Saved {len(self.chunks)} chunks to {output_path}")

    def print_summary(self):
        """Print summary of extracted chunks."""
        print(f"\nğŸ“Š Extraction Summary:")
        print(f"Total chunks: {len(self.chunks)}")

        # Category distribution
        category_counts = {}
        for chunk in self.chunks:
            cat = chunk["category"]
            category_counts[cat] = category_counts.get(cat, 0) + 1

        print("\nğŸ“‚ Category Distribution:")
        for cat, count in sorted(category_counts.items(), key=lambda x: -x[1]):
            print(f"  - {cat}: {count}")

        print("\nğŸ“ Sample Chunks:")
        for chunk in self.chunks[:3]:
            print(f"\n  [{chunk['doc_id']}] {chunk['title']} ({chunk['category']})")
            print(f"  Content preview: {chunk['content'][:100]}...")


def main():
    """Main execution function."""
    print("ğŸš€ Starting Jecheon Tourism Document Chunking...")

    # Paths
    markdown_path = "/home/user/goodganglabs/data/processed/ì œì²œì‹œê´€ê´‘ì •ë³´ì±…ì.md"
    output_path = "/home/user/goodganglabs/data/chunks/documents.jsonl"

    # Initialize chunker
    chunker = JecheonDocumentChunker(markdown_path)

    # Extract chunks
    chunks = chunker.extract_chunks()

    # Save to JSONL
    chunker.save_to_jsonl(output_path)

    # Print summary
    chunker.print_summary()

    print("\nâœ… Document chunking completed!")


if __name__ == "__main__":
    main()
