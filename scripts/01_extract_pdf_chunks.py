"""
Document Chunking Script for Jecheon Tourism Dataset (Optimized for RAG)

This script extracts meaningful chunks from the Jecheon tourism markdown file
with optimal size for RAG fine-tuning (300-2000 chars).

Key features:
- Minimum chunk size: 300 chars (sufficient context)
- Maximum chunk size: 2000 chars (avoid noise)
- Context-aware extraction (preserve semantic boundaries)

Output: data/chunks/documents.jsonl
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Any


class JecheonDocumentChunker:
    """Extracts and chunks Jecheon tourism information optimized for RAG."""

    # RAG-optimized chunk sizes
    MIN_CHUNK_SIZE = 300   # chars (~75 tokens)
    MAX_CHUNK_SIZE = 2000  # chars (~500 tokens)

    CATEGORIES = {
        "transportation": ["ì‹œí‹°íˆ¬ì–´", "ê´€ê´‘íƒì‹œ", "ê´€ê´‘ì£¼ë¯¼ì¦", "êµí†µ"],
        "tourism": ["ì˜ë¦¼ì§€", "ì²­í’", "ë°°ë¡ ", "ë°•ë‹¬ì¬", "ì˜¥ìˆœë´‰", "ì¼€ì´ë¸”ì¹´", "ê´€ê´‘ì§€", "ëª…ì†Œ"],
        "food": ["ë§›ì§‘", "ì‹ë‹¹", "ìŒì‹", "ë¨¹ê±°ë¦¬", "ê°€ìŠ¤íŠ¸ë¡œ"],
        "accommodation": ["ìˆ™ë°•", "ë¦¬ì¡°íŠ¸", "í˜¸í…”", "ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤"],
        "activity": ["íŠ¸ë ˆí‚¹", "ì²´í—˜", "ì¶•ì œ", "ë ˆì €", "ê±·ê¸°"],
        "culture": ["ë°•ë¬¼ê´€", "ì„±ì§€", "ë¬¸í™”", "ì—­ì‚¬", "ìœ ì‚°"],
        "course": ["ì½”ìŠ¤", "ì—¬í–‰", "ì¶”ì²œ"],
        "benefit": ["ì¸ì„¼í‹°ë¸Œ", "í• ì¸", "í˜œíƒ", "ê¸°ë¶€"],
    }

    def __init__(self, markdown_path: str):
        """Initialize chunker with markdown file path."""
        self.markdown_path = Path(markdown_path)
        self.chunks = []
        self.full_content = ""

    def read_markdown(self) -> str:
        """Read markdown file."""
        with open(self.markdown_path, 'r', encoding='utf-8') as f:
            return f.read()

    def clean_text(self, text: str) -> str:
        """Clean text by removing line numbers and extra whitespace."""
        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            # Remove line numbers (e.g., "    1â†’")
            cleaned_line = re.sub(r'^\s*\d+â†’', '', line)
            cleaned_lines.append(cleaned_line)

        # Join and clean up
        text = '\n'.join(cleaned_lines)
        # Remove multiple blank lines
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        return text.strip()

    def categorize_chunk(self, content: str, title: str) -> str:
        """Determine the category of a chunk based on its content and title."""
        text = (title + " " + content).lower()

        category_scores = {}
        for category, keywords in self.CATEGORIES.items():
            score = sum(1 for keyword in keywords if keyword.lower() in text)
            if score > 0:
                category_scores[category] = score

        if category_scores:
            return max(category_scores.items(), key=lambda x: x[1])[0]
        return "general"

    def extract_between_patterns(self, content: str, start: str, end: str,
                                 include_start: bool = True) -> str:
        """Extract content between two patterns."""
        try:
            start_idx = content.find(start)
            if start_idx == -1:
                return ""

            if not include_start:
                start_idx += len(start)

            end_idx = content.find(end, start_idx + 1)
            if end_idx == -1:
                return content[start_idx:].strip()

            return content[start_idx:end_idx].strip()
        except Exception as e:
            return ""

    def ensure_min_size(self, content: str, title: str, context_before: str = "",
                       context_after: str = "") -> str:
        """
        Ensure chunk meets minimum size by adding context.
        If too small, add surrounding context.
        """
        if len(content) >= self.MIN_CHUNK_SIZE:
            return content

        # Add context to meet minimum size
        enhanced_content = content

        # Add context before if needed
        if context_before and len(enhanced_content) < self.MIN_CHUNK_SIZE:
            additional = context_before[-200:]  # Last 200 chars
            enhanced_content = f"{additional}\n\n{enhanced_content}"

        # Add context after if needed
        if context_after and len(enhanced_content) < self.MIN_CHUNK_SIZE:
            additional = context_after[:200]  # First 200 chars
            enhanced_content = f"{enhanced_content}\n\n{additional}"

        return enhanced_content

    def truncate_max_size(self, content: str) -> str:
        """Truncate content if it exceeds maximum size."""
        if len(content) <= self.MAX_CHUNK_SIZE:
            return content

        # Find last sentence boundary within limit
        truncated = content[:self.MAX_CHUNK_SIZE]
        last_period = max(truncated.rfind('.\n'), truncated.rfind('ã€‚\n'))

        if last_period > self.MIN_CHUNK_SIZE:
            return content[:last_period + 2].strip()

        return truncated.strip()

    def extract_tourist_site_enhanced(self, content: str, title: str,
                                      pattern: str) -> str:
        """Extract tourist site with enhanced context."""
        match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
        if not match:
            return ""

        extracted = match.group(0).strip()

        # If too small, try to get more context
        if len(extracted) < self.MIN_CHUNK_SIZE:
            # Find position in full content
            pos = content.find(extracted)
            if pos != -1:
                # Get surrounding context
                start = max(0, pos - 200)
                end = min(len(content), pos + len(extracted) + 200)
                extended = content[start:end].strip()

                if len(extended) >= self.MIN_CHUNK_SIZE:
                    extracted = extended

        return self.truncate_max_size(extracted)

    def extract_chunks(self) -> List[Dict[str, Any]]:
        """Extract meaningful chunks from the markdown content."""
        content = self.read_markdown()
        content = self.clean_text(content)
        self.full_content = content

        chunks_data = []

        # 1. Transportation & Services
        chunks_data.extend([
            {
                "title": "ë””ì§€í„¸ê´€ê´‘ì£¼ë¯¼ì¦",
                "category": "transportation",
                "content": self.extract_between_patterns(content, "ë””ì§€í„¸ ê´€ê´‘ì£¼ë¯¼ì¦", "ì œì²œ ì‹œí‹°íˆ¬ì–´"),
                "page": 4,
            },
            {
                "title": "ì œì²œ ì‹œí‹°íˆ¬ì–´",
                "category": "transportation",
                "content": self.extract_between_patterns(content, "ì œì²œ ì‹œí‹°íˆ¬ì–´", "ì œì²œ ê´€ê´‘íƒì‹œ"),
                "page": 5,
            },
            {
                "title": "ì œì²œ ê´€ê´‘íƒì‹œ",
                "category": "transportation",
                "content": self.extract_between_patterns(content, "ì œì²œ ê´€ê´‘íƒì‹œ\nì œì²œ í† ë°•ì´", "ë‹¨ì²´ê´€ê´‘ê° ìœ ì¹˜ ì¸ì„¼í‹°ë¸Œ"),
                "page": 6,
            },
            {
                "title": "ë‹¨ì²´ê´€ê´‘ê° ì¸ì„¼í‹°ë¸Œ",
                "category": "benefit",
                "content": self.extract_between_patterns(content, "ë‹¨ì²´ê´€ê´‘ê° ìœ ì¹˜ ì¸ì„¼í‹°ë¸Œ", "ê°€ìŠ¤íŠ¸ë¡œ íˆ¬ì–´"),
                "page": 7,
            },
            {
                "title": "ê°€ìŠ¤íŠ¸ë¡œ íˆ¬ì–´",
                "category": "food",
                "content": self.extract_between_patterns(content, "ê°€ìŠ¤íŠ¸ë¡œ íˆ¬ì–´", "ëª¨ë°”ì¼ ë°”ë¡œê°€ê¸°"),
                "page": 8,
            },
        ])

        # 2. Tourist Sites (with enhanced context)
        tourist_sites = [
            {
                "title": "ì˜ë¦¼ì§€Â·ì˜ë¦¼ì§€ì—­ì‚¬ë°•ë¬¼ê´€",
                "category": "tourism",
                "pattern": r"ì˜ë¦¼ì§€(?:ëŠ”|Â·ì˜ë¦¼ì§€ì—­ì‚¬ë°•ë¬¼ê´€).*?(?:ì œì²œì‹œ\s*(?:ì†¡í•™ë©´|ì†”ë§¤ë¡œ)[^\n]+)(?:\n[^\n]+){0,5}",
                "page": 12,
                "location": "ì†¡í•™ë©´",
                "address": "ì œì²œì‹œ ì†¡í•™ë©´ ì˜ë¦¼ëŒ€ë¡œ 47ê¸¸ 7",
            },
            {
                "title": "ë°°ë¡ ì„±ì§€",
                "category": "culture",
                "pattern": r"ë°°ë¡ ì„±ì§€.*?ì œì²œì‹œ\s*ë´‰ì–‘ì[^\n]+(?:\n[^\n]+){0,5}",
                "page": 12,
                "location": "ë´‰ì–‘ì",
                "address": "ì œì²œì‹œ ë´‰ì–‘ì ë°°ë¡ ì„±ì§€ê¸¸ 296",
            },
            {
                "title": "ë°•ë‹¬ì¬",
                "category": "tourism",
                "pattern": r"ë°•ë‹¬ì¬.*?ì œì²œì‹œ\s*ë°±ìš´ë©´[^\n]+(?:\n[^\n]+){0,5}",
                "page": 12,
                "location": "ë°±ìš´ë©´",
                "address": "ì œì²œì‹œ ë°±ìš´ë©´ ë°•ë‹¬ë¡œ 231",
            },
            {
                "title": "ì²­í’í˜¸ë°˜ ì¼€ì´ë¸”ì¹´",
                "category": "tourism",
                "pattern": r"ë¹„ë´‰ì‚°ì˜?\s*(?:ê´€ê´‘|í’ê²½).*?ì²­í’í˜¸ë°˜?\s*ì¼€ì´ë¸”ì¹´.*?ì œì²œì‹œ\s*ì²­í’ë©´[^\n]+(?:\n[^\n]+){0,3}",
                "page": 14,
                "location": "ì²­í’ë©´",
                "address": "ì œì²œì‹œ ì²­í’ë©´ ë¬¸í™”ì¬ê¸¸ 166",
            },
            {
                "title": "ì²­í’ë¬¸í™”ìœ ì‚°ë‹¨ì§€",
                "category": "culture",
                "pattern": r"ì²­í’ë¬¸í™”ìœ ì‚°ë‹¨ì§€.*?ì œì²œì‹œ\s*ì²­í’í˜¸ë¡œ[^\n]+(?:\n[^\n]+){0,3}",
                "page": 14,
                "location": "ì²­í’ë©´",
                "address": "ì œì²œì‹œ ì²­í’í˜¸ë¡œ 2048",
            },
            {
                "title": "ì²­í’ëœë“œ",
                "category": "activity",
                "pattern": r"ì²­í’ëœë“œ.*?ì œì²œì‹œ\s*ì²­í’ë©´[^\n]+(?:\n[^\n]+){0,3}",
                "page": 14,
                "location": "ì²­í’ë©´",
                "address": "ì œì²œì‹œ ì²­í’ë©´ ì²­í’í˜¸ë¡œ50ê¸¸ 6",
            },
            {
                "title": "ì˜¥ìˆœë´‰ ì¶œë ë‹¤ë¦¬",
                "category": "tourism",
                "pattern": r"(?:ëª…ìŠ¹.*?)?ì˜¥ìˆœë´‰\s*ì¶œë ë‹¤ë¦¬.*?ì œì²œì‹œ\s*ìˆ˜ì‚°ë©´[^\n]+(?:\n[^\n]+){0,3}",
                "page": 14,
                "location": "ìˆ˜ì‚°ë©´",
                "address": "ì œì²œì‹œ ìˆ˜ì‚°ë©´ ì˜¥ìˆœë´‰ë¡œ342",
            },
            {
                "title": "êµ­ë¦½ ì œì²œ ì¹˜ìœ ì˜ ìˆ²",
                "category": "activity",
                "pattern": r"êµ­ë¦½\s*ì œì²œ\s*ì¹˜ìœ ì˜\s*ìˆ².*?ì œì²œì‹œ\s*ì²­í’ë©´[^\n]+(?:\n[^\n]+){0,3}",
                "page": 14,
                "location": "ì²­í’ë©´",
                "address": "ì œì²œì‹œ ì²­í’ë©´ í•™í˜„ì†Œì•¼ë¡œ 590",
            },
            {
                "title": "ì²­í’í˜¸ ìë“œë½ê¸¸",
                "category": "activity",
                "pattern": r"ì²­í’í˜¸\s*ìë“œë½ê¸¸.*?ì œì²œì‹œ\s*ìˆ˜ì‚°ë©´[^\n]+(?:\n[^\n]+){0,3}",
                "page": 14,
                "location": "ìˆ˜ì‚°ë©´",
                "address": "ì œì²œì‹œ ìˆ˜ì‚°ë©´ ì˜¥ìˆœë´‰ë¡œ 6ê¸¸ 3",
            },
        ]

        for site in tourist_sites:
            extracted = self.extract_tourist_site_enhanced(
                content, site["title"], site["pattern"]
            )
            if extracted:
                chunks_data.append({
                    "title": site["title"],
                    "category": site["category"],
                    "content": extracted,
                    "page": site["page"],
                    "location": site.get("location", ""),
                    "address": site.get("address", ""),
                })

        # 3. Additional sites
        additional_sites = [
            ("ì‚¼í•œì˜ ì´ˆë¡ê¸¸", "activity", r"ì‚¼í•œì˜\s*ì´ˆë¡ê¸¸.*?(?:ì œì²œì‹œ\s*ì„±ë´‰ë¡œ|ê±´ê°•ê³¼\s*ì‹¬ë¦¬ì¹˜ìœ )[^\n]*(?:\n[^\n]+){0,5}"),
            ("ì œì²œì˜ ì¶•ì œ", "activity", r"ì œì²œì˜\s*ì¶•ì œ.*?(?:ì²­í’í˜¸\s*ë²šê½ƒì¶•ì œ|ë´‰ì–‘ë°•ë‹¬ì½©ì¶•ì œ).*?(?:\n[^\n]+){0,8}"),
        ]

        for title, category, pattern in additional_sites:
            extracted = self.extract_tourist_site_enhanced(content, title, pattern)
            if extracted:
                chunks_data.append({
                    "title": title,
                    "category": category,
                    "content": extracted,
                    "page": 12,
                })

        # 4. Trekking & Activities
        trekking_content = self.extract_between_patterns(content, "íŠ¸ë˜í‚¹Â·ê±·ê¸° ì¢‹ì€ê³³", "ì½”ìŠ¤ì—¬í–‰ ì¶”ì²œ")
        if trekking_content:
            chunks_data.append({
                "title": "íŠ¸ë ˆí‚¹Â·ê±·ê¸° ì¢‹ì€ ê³³",
                "category": "activity",
                "content": trekking_content,
                "page": 16,
            })

        # 5. Travel Courses
        chunks_data.extend([
            {
                "title": "ì œì²œ 1ì¼ ì½”ìŠ¤",
                "category": "course",
                "content": self.extract_between_patterns(content, "1ì¼ ì½”ìŠ¤", "1ë°• 2ì¼ ì½”ìŠ¤"),
                "page": 17,
            },
            {
                "title": "ì œì²œ 1ë°• 2ì¼ ì½”ìŠ¤",
                "category": "course",
                "content": self.extract_between_patterns(content, "1ë°• 2ì¼ ì½”ìŠ¤", "íœ´ì–‘Â·íë§ ì½”ìŠ¤"),
                "page": 17,
            },
            {
                "title": "ìŠ¬ë¡œì‹œí‹° íë§ ì½”ìŠ¤",
                "category": "course",
                "content": self.extract_between_patterns(content, "#ìŠ¬ë¡œì‹œí‹° ì½”ìŠ¤", "#ë¶ë¶€Â·ì„œë¶€ê¶Œ ì½”ìŠ¤"),
                "page": 17,
            },
            {
                "title": "ë°±ìš´ê¶Œ íë§ ì½”ìŠ¤",
                "category": "course",
                "content": self.extract_between_patterns(content, "#ë¶ë¶€Â·ì„œë¶€ê¶Œ ì½”ìŠ¤", "ë¬¸í™”Â·ì—­ì‚¬ ì½”ìŠ¤"),
                "page": 17,
            },
            {
                "title": "ë¶ˆêµ ìˆœë¡€ ì½”ìŠ¤",
                "category": "course",
                "content": self.extract_between_patterns(content, "#ë¶ˆêµ ì½”ìŠ¤", "#ì²œì£¼êµ ì½”ìŠ¤"),
                "page": 18,
            },
            {
                "title": "ì²œì£¼êµ ìˆœë¡€ ì½”ìŠ¤",
                "category": "course",
                "content": self.extract_between_patterns(content, "#ì²œì£¼êµ ì½”ìŠ¤", "#ê¸°ë…êµ ì½”ìŠ¤"),
                "page": 18,
            },
        ])

        # 6. Food & Restaurants
        chunks_data.extend([
            {
                "title": "ì œì²œ ë§›ì§‘ ì•ˆë‚´",
                "category": "food",
                "content": self.extract_between_patterns(content, "ì œì²œë§›ì§‘", "ë¶ë¶€ê¶Œ ("),
                "page": 20,
            },
            {
                "title": "ë¶ë¶€ê¶Œ ë§›ì§‘",
                "category": "food",
                "content": self.extract_between_patterns(content, "ë¶ë¶€ê¶Œ (9)", "ë‚¨ë¶€ê¶Œ ("),
                "page": 21,
            },
            {
                "title": "ì²­í’ê¶Œ ë§›ì§‘",
                "category": "food",
                "content": self.extract_between_patterns(content, "ì²­í’ê¶Œ (11)", "ì£¼ìš” ìˆ™ë°•ì‹œì„¤"),
                "page": 21,
            },
        ])

        # 7. Accommodation
        accommodations = [
            ("í¬ë ˆìŠ¤íŠ¸ ë¦¬ì†œ", "ì œì²œì‹œ ë°±ìš´ë©´ ê¸ˆë´‰ë¡œ 365", "043, 649, 6000"),
            ("ì²­í’ë¦¬ì¡°íŠ¸", "ì œì²œì‹œ ì²­í’ë©´ ì²­í’í˜¸ë¡œ 1798", "043, 640, 7000"),
            ("ESë¦¬ì¡°íŠ¸", "ì œì²œì‹œ ìˆ˜ì‚°ë©´ ì˜¥ìˆœë´‰ë¡œ 1248", "043, 648, 0480"),
            ("ì„œìš¸ê´€ê´‘í˜¸í…”", "ì œì²œì‹œ ì˜ë¦¼ëŒ€ë¡œ13ê¸¸ 10", "043, 651, 8000"),
        ]

        for name, address, phone in accommodations:
            # Find accommodation with context
            pattern = f"{name}.*?{phone}"
            match = re.search(pattern, content, re.DOTALL)
            if match:
                extracted = match.group(0).strip()
                # Add description if too short
                if len(extracted) < self.MIN_CHUNK_SIZE:
                    # Add accommodation type description
                    desc = f"{name}ì€(ëŠ”) ì œì²œì˜ ì£¼ìš” ìˆ™ë°•ì‹œì„¤ì…ë‹ˆë‹¤.\nì£¼ì†Œ: {address}\nì—°ë½ì²˜: {phone}"
                    extracted = f"{desc}\n\n{extracted}"

                chunks_data.append({
                    "title": name,
                    "category": "accommodation",
                    "content": self.truncate_max_size(extracted),
                    "page": 22,
                    "address": address,
                })

        # 8. Benefits & Tips
        chunks_data.extend([
            {
                "title": "ê³ í–¥ì‚¬ë‘ ê¸°ë¶€ì œ",
                "category": "benefit",
                "content": self.extract_between_patterns(content, "ê³ í–¥ì‚¬ë‘ ê¸°ë¶€ì œ", "ì•Œì•„ë‘ë©´ ë„ì›€ë˜ëŠ” ê¿€íŒ"),
                "page": 24,
            },
            {
                "title": "ì•Œì•„ë‘ë©´ ì¢‹ì€ ì •ë³´",
                "category": "benefit",
                "content": self.extract_between_patterns(content, "ì•Œì•„ë‘ë©´ ë„ì›€ë˜ëŠ” ê¿€íŒ", "Travel in Jecheon"),
                "page": 26,
            },
        ])

        # Create document chunks with size validation
        doc_id = 1
        for chunk_data in chunks_data:
            content_text = chunk_data.get("content", "")
            if not content_text or len(content_text.strip()) < 50:
                continue

            # Ensure minimum size
            title = chunk_data["title"]
            if len(content_text) < self.MIN_CHUNK_SIZE:
                # Add title as context if needed
                enhanced = f"# {title}\n\n{content_text}"
                if len(enhanced) < self.MIN_CHUNK_SIZE and chunk_data.get("address"):
                    enhanced += f"\n\nìœ„ì¹˜: {chunk_data['address']}"
                content_text = enhanced

            # Truncate if too large
            content_text = self.truncate_max_size(content_text)

            self.chunks.append({
                "doc_id": f"doc_{doc_id:03d}",
                "title": chunk_data["title"],
                "category": chunk_data["category"],
                "content": content_text.strip(),
                "metadata": {
                    "page": chunk_data.get("page", 0),
                    "location": chunk_data.get("location", ""),
                    "address": chunk_data.get("address", ""),
                },
                "filename": f"doc_{doc_id:03d}_{chunk_data['title']}.txt",
            })
            doc_id += 1

        return self.chunks

    def save_to_jsonl(self, output_path: str):
        """Save chunks to JSONL file."""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            for chunk in self.chunks:
                f.write(json.dumps(chunk, ensure_ascii=False) + '\n')

        print(f"âœ… Saved {len(self.chunks)} chunks to {output_path}")

    def print_summary(self):
        """Print summary of extracted chunks."""
        print(f"\nğŸ“Š Extraction Summary:")
        print(f"Total chunks: {len(self.chunks)}")

        # Size statistics
        sizes = [len(chunk["content"]) for chunk in self.chunks]
        if sizes:
            import statistics
            print(f"\nğŸ“ Chunk Size Statistics:")
            print(f"  Average: {statistics.mean(sizes):.0f} chars")
            print(f"  Median: {statistics.median(sizes):.0f} chars")
            print(f"  Min: {min(sizes)} chars")
            print(f"  Max: {max(sizes)} chars")

            # Size distribution
            print(f"\nğŸ“Š Size Distribution:")
            print(f"  < 300 chars: {sum(1 for s in sizes if s < 300)} âš ï¸")
            print(f"  300-1000 chars: {sum(1 for s in sizes if 300 <= s < 1000)} âœ…")
            print(f"  1000-2000 chars: {sum(1 for s in sizes if 1000 <= s < 2000)} âœ…")
            print(f"  > 2000 chars: {sum(1 for s in sizes if s >= 2000)} âš ï¸")

        # Category distribution
        category_counts = {}
        for chunk in self.chunks:
            cat = chunk["category"]
            category_counts[cat] = category_counts.get(cat, 0) + 1

        print("\nğŸ“‚ Category Distribution:")
        for cat, count in sorted(category_counts.items(), key=lambda x: -x[1]):
            print(f"  - {cat}: {count}")

        print("\nğŸ“ Sample Chunks:")
        for chunk in self.chunks[:5]:
            addr = chunk['metadata'].get('address', '')
            addr_str = f" | {addr}" if addr else ""
            size = len(chunk['content'])
            print(f"  [{chunk['doc_id']}] {chunk['title']} ({size} chars){addr_str}")


def main():
    """Main execution function."""
    print("ğŸš€ Starting Jecheon Tourism Document Chunking (RAG-Optimized)...")

    # Paths (using OCR-corrected markdown)
    markdown_path = "/home/user/goodganglabs/data/processed/ì œì²œì‹œê´€ê´‘ì •ë³´ì±…ì_corrected.md"
    output_path = "/home/user/goodganglabs/data/chunks/documents.jsonl"

    # Initialize chunker
    chunker = JecheonDocumentChunker(markdown_path)

    # Extract chunks
    chunks = chunker.extract_chunks()

    # Save to JSONL
    chunker.save_to_jsonl(output_path)

    # Print summary
    chunker.print_summary()

    print("\nâœ… Document chunking completed!")


if __name__ == "__main__":
    main()
