"""
Document Chunking Script for Jecheon Tourism Dataset (Enhanced Version)

This script extracts meaningful chunks from the Jecheon tourism markdown file
with more granular segmentation. Overlapping is allowed for better coverage.

Output: data/chunks/documents.jsonl
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Any


class JecheonDocumentChunker:
    """Extracts and chunks Jecheon tourism information with fine granularity."""

    CATEGORIES = {
        "transportation": ["ì‹œí‹°íˆ¬ì–´", "ê´€ê´‘íƒì‹œ", "ê´€ê´‘ì£¼ë¯¼ì¦", "êµí†µ"],
        "tourism": ["ì˜ë¦¼ì§€", "ì²­í’", "ë°°ë¡ ", "ë°•ë‹¬ì¬", "ì˜¥ìˆœë´‰", "ì¼€ì´ë¸”ì¹´", "ê´€ê´‘ì§€", "ëª…ì†Œ"],
        "food": ["ë§›ì§‘", "ì‹ë‹¹", "ìŒì‹", "ë¨¹ê±°ë¦¬", "ê°€ìŠ¤íŠ¸ë¡œ"],
        "accommodation": ["ìˆ™ë°•", "ë¦¬ì¡°íŠ¸", "í˜¸í…”", "ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤"],
        "activity": ["íŠ¸ë ˆí‚¹", "ì²´í—˜", "ì¶•ì œ", "ë ˆì €", "ê±·ê¸°"],
        "culture": ["ë°•ë¬¼ê´€", "ì„±ì§€", "ë¬¸í™”", "ì—­ì‚¬", "ìœ ì‚°"],
        "course": ["ì½”ìŠ¤", "ì—¬í–‰", "ì¶”ì²œ"],
        "benefit": ["ì¸ì„¼í‹°ë¸Œ", "í• ì¸", "í˜œíƒ", "ê¸°ë¶€"],
    }

    def __init__(self, markdown_path: str):
        """Initialize chunker with markdown file path."""
        self.markdown_path = Path(markdown_path)
        self.chunks = []

    def read_markdown(self) -> str:
        """Read markdown file."""
        with open(self.markdown_path, 'r', encoding='utf-8') as f:
            return f.read()

    def clean_text(self, text: str) -> str:
        """Clean text by removing line numbers and extra whitespace."""
        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            # Remove line numbers (e.g., "    1â†’")
            cleaned_line = re.sub(r'^\s*\d+â†’', '', line)
            cleaned_lines.append(cleaned_line)

        # Join and clean up
        text = '\n'.join(cleaned_lines)
        # Remove multiple blank lines
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        return text.strip()

    def categorize_chunk(self, content: str, title: str) -> str:
        """Determine the category of a chunk based on its content and title."""
        text = (title + " " + content).lower()

        category_scores = {}
        for category, keywords in self.CATEGORIES.items():
            score = sum(1 for keyword in keywords if keyword.lower() in text)
            if score > 0:
                category_scores[category] = score

        if category_scores:
            return max(category_scores.items(), key=lambda x: x[1])[0]
        return "general"

    def extract_between_patterns(self, content: str, start: str, end: str,
                                 include_start: bool = True) -> str:
        """Extract content between two patterns."""
        try:
            start_idx = content.find(start)
            if start_idx == -1:
                return ""

            if not include_start:
                start_idx += len(start)

            end_idx = content.find(end, start_idx + 1)
            if end_idx == -1:
                return content[start_idx:].strip()

            return content[start_idx:end_idx].strip()
        except Exception as e:
            return ""

    def extract_tourist_sites(self, content: str) -> List[Dict[str, Any]]:
        """Extract individual tourist sites as separate chunks."""
        sites = []

        # Pattern: Site name followed by address
        site_patterns = [
            # Major sites with full details
            (r"ì˜ë¦¼ì§€Â·ì˜ë¦¼ì§€ì—­ì‚¬ë°•ë¬¼ê´€.*?(?=ë°°ë¡ ì„±ì§€|$)", "ì˜ë¦¼ì§€Â·ì˜ë¦¼ì§€ì—­ì‚¬ë°•ë¬¼ê´€", "tourism", 12, "ì†¡í•™ë©´", "ì œì²œì‹œ ì†¡í•™ë©´ ì˜ë¦¼ëŒ€ë¡œ 47ê¸¸ 7"),
            (r"ë°°ë¡ ì„±ì§€.*?(?=ë°•ë‹¬ì¬|$)", "ë°°ë¡ ì„±ì§€", "culture", 12, "ë´‰ì–‘ì", "ì œì²œì‹œ ë´‰ì–‘ì ë°°ë¡ ì„±ì§€ê¸¸ 296"),
            (r"ë°•ë‹¬ì¬(?:ëŠ”|\.)[^ì œ]*?ì œì²œì‹œ[^\n]+(?:\n[^\nì œ]+){0,3}", "ë°•ë‹¬ì¬", "tourism", 12, "ë°±ìš´ë©´", "ì œì²œì‹œ ë°±ìš´ë©´ ë°•ë‹¬ë¡œ 231"),
            (r"ì œì²œí•œë°©ì—‘ìŠ¤í¬\s*ê³µì›.*?(?=ì˜ë¦¼ì§€|$)", "ì œì²œí•œë°©ì—‘ìŠ¤í¬ ê³µì›", "culture", 12, "", "ì œì²œì‹œ í•œë°©ì—‘ìŠ¤í¬ë¡œ 19"),
            (r"ì²­í’í˜¸ë°˜\s*ì¼€ì´ë¸”ì¹´.*?ì œì²œì‹œ\s*ì²­í’ë©´[^\n]+", "ì²­í’í˜¸ë°˜ ì¼€ì´ë¸”ì¹´", "tourism", 14, "ì²­í’ë©´", "ì œì²œì‹œ ì²­í’ë©´ ë¬¸í™”ì¬ê¸¸ 166"),
            (r"ì²­í’ë¬¸í™”ìœ ì‚°ë‹¨ì§€.*?ì œì²œì‹œ\s*ì²­í’í˜¸ë¡œ[^\n]+", "ì²­í’ë¬¸í™”ìœ ì‚°ë‹¨ì§€", "culture", 14, "ì²­í’ë©´", "ì œì²œì‹œ ì²­í’í˜¸ë¡œ 2048"),
            (r"ì²­í’ëœë“œ.*?ì œì²œì‹œ\s*ì²­í’ë©´[^\n]+", "ì²­í’ëœë“œ", "activity", 14, "ì²­í’ë©´", "ì œì²œì‹œ ì²­í’ë©´ ì²­í’í˜¸ë¡œ50ê¸¸ 6"),
            (r"ì˜¥ìˆœë´‰\s*ì¶œë ë‹¤ë¦¬.*?ì œì²œì‹œ\s*ìˆ˜ì‚°ë©´[^\n]+", "ì˜¥ìˆœë´‰ ì¶œë ë‹¤ë¦¬", "tourism", 14, "ìˆ˜ì‚°ë©´", "ì œì²œì‹œ ìˆ˜ì‚°ë©´ ì˜¥ìˆœë´‰ë¡œ342"),
            (r"êµ­ë¦½\s*ì œì²œ\s*ì¹˜ìœ ì˜\s*ìˆ².*?ì œì²œì‹œ\s*ì²­í’ë©´[^\n]+", "êµ­ë¦½ ì œì²œ ì¹˜ìœ ì˜ ìˆ²", "activity", 14, "ì²­í’ë©´", "ì œì²œì‹œ ì²­í’ë©´ í•™í˜„ì†Œì•¼ë¡œ 590"),
            (r"ì²­í’í˜¸\s*ìë“œë½ê¸¸.*?ì œì²œì‹œ\s*ìˆ˜ì‚°ë©´[^\n]+", "ì²­í’í˜¸ ìë“œë½ê¸¸", "activity", 14, "ìˆ˜ì‚°ë©´", "ì œì²œì‹œ ìˆ˜ì‚°ë©´ ì˜¥ìˆœë´‰ë¡œ 6ê¸¸ 3"),
        ]

        for pattern, title, category, page, location, address in site_patterns:
            match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
            if match:
                sites.append({
                    "title": title,
                    "category": category,
                    "content": match.group(0).strip(),
                    "page": page,
                    "location": location,
                    "address": address,
                })

        # Additional sites from secondary sections
        additional_sites = [
            ("ì˜ë¦¼ì§€ ìˆ˜ë¦¬ê³µì›", "activity", r"ì˜ë¦¼ì§€\s*ìˆ˜ë¦¬ê³µì›.*?(?:ìš´ì˜ê¸°ê°„|ì œì²œì‹œ\s*ëª¨ì‚°ë™)[^\n]*(?:\n[^\n]+){0,5}"),
            ("ì‚¼í•œì˜ ì´ˆë¡ê¸¸", "activity", r"ì‚¼í•œì˜\s*ì´ˆë¡ê¸¸.*?(?:ì œì²œì‹œ\s*ì„±ë´‰ë¡œ|km\s*ê¸¸ì´)[^\n]*(?:\n[^\n]+){0,3}"),
            ("êµë™ë¯¼í™”ë§ˆì„", "culture", r"êµë™ë¯¼í™”ë§ˆì„.*?ì œì²œì‹œ\s*ìš©ë‘ë¡œ[^\n]+(?:\n[^\n]+){0,2}"),
            ("ëª¨ì‚°ë¹„í–‰ì¥", "tourism", r"ëª¨ì‚°ë¹„í–‰ì¥.*?ì œì²œì‹œ\s*ê³ ì•”ë™[^\n]+(?:\n[^\n]+){0,2}"),
            ("ì•„ì—´ëŒ€ ìŠ¤ë§ˆíŠ¸ì˜¨ì‹¤", "activity", r"ì•„ì—´ëŒ€\s*ìŠ¤ë§ˆíŠ¸ì˜¨ì‹¤.*?ì œì²œì‹œ\s*ë´‰ì–‘ì[^\n]+(?:\n[^\n]+){0,3}"),
            ("í•œêµ­ì°¨ë¬¸í™”ë°•ë¬¼ê´€", "culture", r"í•œêµ­ì°¨ë¬¸í™”ë°•ë¬¼ê´€.*?ì œì²œì‹œ\s*ê¸ˆí•™ë¡œ[^\n]+(?:\n[^\n]+){0,2}"),
            ("ë²Œìƒˆê½ƒëŒê³¼í•™ê´€", "culture", r"ë²Œìƒˆê½ƒëŒê³¼í•™ê´€.*?ì œì²œì‹œ\s*ë´‰ì–‘ì[^\n]+(?:\n[^\n]+){0,3}"),
        ]

        for title, category, pattern in additional_sites:
            match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
            if match:
                # Extract address from content
                address_match = re.search(r'ì œì²œì‹œ[^\n]+', match.group(0))
                address = address_match.group(0) if address_match else ""

                sites.append({
                    "title": title,
                    "category": category,
                    "content": match.group(0).strip(),
                    "page": 12,  # Default page
                    "location": "",
                    "address": address,
                })

        return sites

    def extract_chunks(self) -> List[Dict[str, Any]]:
        """Extract meaningful chunks from the markdown content."""
        content = self.read_markdown()
        content = self.clean_text(content)

        chunks_data = []

        # 1. Transportation & Services
        chunks_data.extend([
            {
                "title": "ë””ì§€í„¸ê´€ê´‘ì£¼ë¯¼ì¦",
                "category": "transportation",
                "content": self.extract_between_patterns(content, "ë””ì§€í„¸ ê´€ê´‘ì£¼ë¯¼ì¦", "ì œì²œ ì‹œí‹°íˆ¬ì–´"),
                "page": 4,
            },
            {
                "title": "ì œì²œ ì‹œí‹°íˆ¬ì–´",
                "category": "transportation",
                "content": self.extract_between_patterns(content, "ì œì²œ ì‹œí‹°íˆ¬ì–´", "ì œì²œ ê´€ê´‘íƒì‹œ"),
                "page": 5,
            },
            {
                "title": "ì œì²œ ê´€ê´‘íƒì‹œ",
                "category": "transportation",
                "content": self.extract_between_patterns(content, "ì œì²œ ê´€ê´‘íƒì‹œ\nì œì²œ í† ë°•ì´", "ë‹¨ì²´ê´€ê´‘ê° ìœ ì¹˜ ì¸ì„¼í‹°ë¸Œ"),
                "page": 6,
            },
            {
                "title": "ë‹¨ì²´ê´€ê´‘ê° ì¸ì„¼í‹°ë¸Œ",
                "category": "benefit",
                "content": self.extract_between_patterns(content, "ë‹¨ì²´ê´€ê´‘ê° ìœ ì¹˜ ì¸ì„¼í‹°ë¸Œ", "ê°€ìŠ¤íŠ¸ë¡œ íˆ¬ì–´"),
                "page": 7,
            },
            {
                "title": "ê°€ìŠ¤íŠ¸ë¡œ íˆ¬ì–´",
                "category": "food",
                "content": self.extract_between_patterns(content, "ê°€ìŠ¤íŠ¸ë¡œ íˆ¬ì–´", "ëª¨ë°”ì¼ ë°”ë¡œê°€ê¸°"),
                "page": 8,
            },
        ])

        # 2. Extract tourist sites (granular)
        tourist_sites = self.extract_tourist_sites(content)
        for site in tourist_sites:
            chunks_data.append(site)

        # 3. Trekking & Activities
        chunks_data.append({
            "title": "íŠ¸ë ˆí‚¹Â·ê±·ê¸° ì¢‹ì€ ê³³",
            "category": "activity",
            "content": self.extract_between_patterns(content, "íŠ¸ë˜í‚¹Â·ê±·ê¸° ì¢‹ì€ê³³", "ì½”ìŠ¤ì—¬í–‰ ì¶”ì²œ"),
            "page": 16,
        })

        # 4. Travel Courses (each course separately)
        chunks_data.extend([
            {
                "title": "ì œì²œ 1ì¼ ì½”ìŠ¤",
                "category": "course",
                "content": self.extract_between_patterns(content, "1ì¼ ì½”ìŠ¤", "1ë°• 2ì¼ ì½”ìŠ¤"),
                "page": 17,
            },
            {
                "title": "ì œì²œ 1ë°• 2ì¼ ì½”ìŠ¤",
                "category": "course",
                "content": self.extract_between_patterns(content, "1ë°• 2ì¼ ì½”ìŠ¤", "íœ´ì–‘Â·íë§ ì½”ìŠ¤"),
                "page": 17,
            },
            {
                "title": "ìŠ¬ë¡œì‹œí‹° íë§ ì½”ìŠ¤",
                "category": "course",
                "content": self.extract_between_patterns(content, "#ìŠ¬ë¡œì‹œí‹° ì½”ìŠ¤", "#ë¶ë¶€Â·ì„œë¶€ê¶Œ ì½”ìŠ¤"),
                "page": 17,
            },
            {
                "title": "ë°±ìš´ê¶Œ íë§ ì½”ìŠ¤",
                "category": "course",
                "content": self.extract_between_patterns(content, "#ë¶ë¶€Â·ì„œë¶€ê¶Œ ì½”ìŠ¤", "ë¬¸í™”Â·ì—­ì‚¬ ì½”ìŠ¤"),
                "page": 17,
            },
            {
                "title": "ë¬¸í™”Â·ì—­ì‚¬ ì½”ìŠ¤",
                "category": "course",
                "content": self.extract_between_patterns(content, "ë¬¸í™”Â·ì—­ì‚¬ ì½”ìŠ¤", "ì¢…êµì—¬í–‰ ì½”ìŠ¤"),
                "page": 18,
            },
            {
                "title": "ë¶ˆêµ ìˆœë¡€ ì½”ìŠ¤",
                "category": "course",
                "content": self.extract_between_patterns(content, "#ë¶ˆêµ ì½”ìŠ¤", "#ì²œì£¼êµ ì½”ìŠ¤"),
                "page": 18,
            },
            {
                "title": "ì²œì£¼êµ ìˆœë¡€ ì½”ìŠ¤",
                "category": "course",
                "content": self.extract_between_patterns(content, "#ì²œì£¼êµ ì½”ìŠ¤", "#ê¸°ë…êµ ì½”ìŠ¤"),
                "page": 18,
            },
            {
                "title": "ê¸°ë…êµ ìˆœë¡€ ì½”ìŠ¤",
                "category": "course",
                "content": self.extract_between_patterns(content, "#ê¸°ë…êµ ì½”ìŠ¤", "#ìœ êµÂ·ì˜ë³‘ë¬¸í™” ì½”ìŠ¤"),
                "page": 18,
            },
        ])

        # 5. Food & Restaurants
        chunks_data.extend([
            {
                "title": "ì œì²œ ë§›ì§‘ ë¸Œëœë“œ (ì•½ì±„ë½Â·ì˜ë¦¼ì§€ì—ì½”ë‹‰)",
                "category": "food",
                "content": self.extract_between_patterns(content, "ì œì²œë§›ì§‘", "ì‹œë‚´ê¶Œ ("),
                "page": 20,
            },
            {
                "title": "ë¶ë¶€ê¶Œ ë§›ì§‘",
                "category": "food",
                "content": self.extract_between_patterns(content, "ë¶ë¶€ê¶Œ (9)", "ë‚¨ë¶€ê¶Œ ("),
                "page": 21,
            },
            {
                "title": "ì²­í’ê¶Œ ë§›ì§‘",
                "category": "food",
                "content": self.extract_between_patterns(content, "ì²­í’ê¶Œ (11)", "ì£¼ìš” ìˆ™ë°•ì‹œì„¤"),
                "page": 21,
            },
        ])

        # 6. Accommodation (individual facilities)
        accommodations = [
            ("í¬ë ˆìŠ¤íŠ¸ ë¦¬ì†œ", "ì œì²œì‹œ ë°±ìš´ë©´ ê¸ˆë´‰ë¡œ 365", "043, 649, 6000"),
            ("ì²­í’ë¦¬ì¡°íŠ¸", "ì œì²œì‹œ ì²­í’ë©´ ì²­í’í˜¸ë¡œ 1798", "043, 640, 7000"),
            ("ESë¦¬ì¡°íŠ¸", "ì œì²œì‹œ ìˆ˜ì‚°ë©´ ì˜¥ìˆœë´‰ë¡œ 1248", "043, 648, 0480"),
            ("ì„œìš¸ê´€ê´‘í˜¸í…”", "ì œì²œì‹œ ì˜ë¦¼ëŒ€ë¡œ13ê¸¸ 10", "043, 651, 8000"),
        ]

        for name, address, phone in accommodations:
            pattern = f"{name}.*?{address}.*?{phone}"
            match = re.search(pattern, content, re.DOTALL)
            if match:
                chunks_data.append({
                    "title": name,
                    "category": "accommodation",
                    "content": match.group(0).strip(),
                    "page": 22,
                    "address": address,
                })

        # 7. Benefits & Tips
        chunks_data.extend([
            {
                "title": "ê³ í–¥ì‚¬ë‘ ê¸°ë¶€ì œ",
                "category": "benefit",
                "content": self.extract_between_patterns(content, "ê³ í–¥ì‚¬ë‘ ê¸°ë¶€ì œ", "ì•Œì•„ë‘ë©´ ë„ì›€ë˜ëŠ” ê¿€íŒ"),
                "page": 24,
            },
            {
                "title": "ì²­í’í˜¸ ìˆ˜ê²½ë¶„ìˆ˜ ìš´ì˜ì‹œê°„",
                "category": "benefit",
                "content": self.extract_between_patterns(content, "ì²­í’í˜¸ì¡°ê²½ë¶„ìˆ˜", "ì˜ë¦¼ì§€ë¯¸ë””ì–´íŒŒì‚¬ë“œ"),
                "page": 26,
            },
            {
                "title": "ì˜ë¦¼ì§€ ë¯¸ë””ì–´íŒŒì‚¬ë“œ ìš´ì˜ì‹œê°„",
                "category": "benefit",
                "content": self.extract_between_patterns(content, "ì˜ë¦¼ì§€ë¯¸ë””ì–´íŒŒì‚¬ë“œ", "ìœ¡ì‚¼ë¥™ ê´€ê´‘ë‹¨ì§€"),
                "page": 26,
            },
        ])

        # 8. Festivals
        festivals_content = self.extract_between_patterns(content, "ì œì²œì˜ ì¶•ì œ", "ë¯¸ë¦¬ë³´ëŠ” ì—¬í–‰ì§€")
        if festivals_content:
            chunks_data.append({
                "title": "ì œì²œì˜ ì¶•ì œ",
                "category": "activity",
                "content": festivals_content,
                "page": 11,
            })

        # Create document chunks with proper doc_ids
        doc_id = 1
        for chunk_data in chunks_data:
            if chunk_data.get("content") and len(chunk_data["content"].strip()) > 30:
                self.chunks.append({
                    "doc_id": f"doc_{doc_id:03d}",
                    "title": chunk_data["title"],
                    "category": chunk_data["category"],
                    "content": chunk_data["content"].strip(),
                    "metadata": {
                        "page": chunk_data.get("page", 0),
                        "location": chunk_data.get("location", ""),
                        "address": chunk_data.get("address", ""),
                    },
                    "filename": f"doc_{doc_id:03d}_{chunk_data['title']}.txt",
                })
                doc_id += 1

        return self.chunks

    def save_to_jsonl(self, output_path: str):
        """Save chunks to JSONL file."""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            for chunk in self.chunks:
                f.write(json.dumps(chunk, ensure_ascii=False) + '\n')

        print(f"âœ… Saved {len(self.chunks)} chunks to {output_path}")

    def print_summary(self):
        """Print summary of extracted chunks."""
        print(f"\nğŸ“Š Extraction Summary:")
        print(f"Total chunks: {len(self.chunks)}")

        # Category distribution
        category_counts = {}
        for chunk in self.chunks:
            cat = chunk["category"]
            category_counts[cat] = category_counts.get(cat, 0) + 1

        print("\nğŸ“‚ Category Distribution:")
        for cat, count in sorted(category_counts.items(), key=lambda x: -x[1]):
            print(f"  - {cat}: {count}")

        print("\nğŸ“ All Chunks:")
        for chunk in self.chunks:
            addr = chunk['metadata'].get('address', '')
            addr_str = f" | {addr}" if addr else ""
            print(f"  [{chunk['doc_id']}] {chunk['title']} ({chunk['category']}){addr_str}")


def main():
    """Main execution function."""
    print("ğŸš€ Starting Jecheon Tourism Document Chunking (Enhanced)...")

    # Paths
    markdown_path = "/home/user/goodganglabs/data/processed/ì œì²œì‹œê´€ê´‘ì •ë³´ì±…ì.md"
    output_path = "/home/user/goodganglabs/data/chunks/documents.jsonl"

    # Initialize chunker
    chunker = JecheonDocumentChunker(markdown_path)

    # Extract chunks
    chunks = chunker.extract_chunks()

    # Save to JSONL
    chunker.save_to_jsonl(output_path)

    # Print summary
    chunker.print_summary()

    print("\nâœ… Document chunking completed!")


if __name__ == "__main__":
    main()
