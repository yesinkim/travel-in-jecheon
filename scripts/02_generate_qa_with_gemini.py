"""
Q&A Generation Script using OpenAI gemini API

This script generates question-answer pairs from document chunks using OpenAI gemini API.
Generates diverse question types: factual, descriptive, recommendation, comparison, no-answer.

Input: data/chunks/documents.jsonl
Output: data/chunks/qa_pairs.jsonl
"""

import json
import os
from pathlib import Path
from typing import List, Dict, Any
# from openai import OpenAI
import google.generativeai as genai
from tqdm import tqdm
import time
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()


class QAGenerator:
    """Generates Q&A pairs using OpenAI gemini API."""

    # Question type distribution (target percentages)
    QUESTION_DISTRIBUTION = {
        "factual": 0.40,  # 40%
        "descriptive": 0.30,  # 30%
        "recommendation": 0.15,  # 15%
        "comparison": 0.10,  # 10%
        "no_answer": 0.05,  # 5%
    }

    # Questions per chunk by category (target: ~140-150 Q&A for 8B model training)
    QUESTIONS_PER_CHUNK = {
        "tourism": 8,  # Major tourist sites (most important)
        "transportation": 8,
        "food": 8,
        "accommodation": 8,
        "activity": 8,
        "culture": 8,
        "course": 7,
        "benefit": 7,
        "general": 7,
    }

    def __init__(self, api_key: str = None):
        """Initialize QA generator with OpenAI API key."""
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY environment variable not set")

            # response = self.client.chat.completions.create(
            #     model="gemini",
            #     messages=[
            #         {"role": "system", "content": "ÎãπÏã†ÏùÄ ÌïúÍµ≠ Í¥ÄÍ¥ë Ï†ïÎ≥¥ Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ± Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. Ï£ºÏñ¥ÏßÑ Î¨∏ÏÑúÎ•º Î∞îÌÉïÏúºÎ°ú Í≥†ÌíàÏßàÏùò ÏßàÎ¨∏-ÎãµÎ≥Ä ÏåçÏùÑ JSON ÌòïÏãùÏúºÎ°ú ÏÉùÏÑ±Ìï©ÎãàÎã§."},
            #         {"role": "user", "content": prompt}
            #     ],
            #     max_tokens=4096,
            #     temperature=0.7,
            #     response_format={"type": "json_object"}

        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel(
            model_name="gemini-2.5-pro",
            generation_config={"temperature": 0.7, "max_output_tokens": 4096, "response_mime_type": "application/json"},
        )
        self.qa_pairs = []

    def load_documents(self, documents_path: str) -> List[Dict[str, Any]]:
        """Load document chunks from JSONL file."""
        documents = []
        with open(documents_path, 'r', encoding='utf-8') as f:
            for line in f:
                documents.append(json.loads(line))
        return documents

    def create_qa_generation_prompt(
        self,
        doc_content: str,
        doc_title: str,
        doc_category: str,
        num_questions: int
    ) -> str:
        """Create prompt for Claude to generate Q&A pairs."""

        prompt = f"""ÎãπÏã†ÏùÄ ÌïúÍµ≠ Í¥ÄÍ¥ë Ï†ïÎ≥¥ Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ± Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. Ï£ºÏñ¥ÏßÑ Î¨∏ÏÑúÎ•º Î∞îÌÉïÏúºÎ°ú Í≥†ÌíàÏßàÏùò ÏßàÎ¨∏-ÎãµÎ≥Ä ÏåçÏùÑ JSON ÌòïÏãùÏúºÎ°ú ÏÉùÏÑ±Ìï©ÎãàÎã§.
        Ï†úÏ≤úÏãú Í¥ÄÍ¥ë Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú RAG(Retrieval-Augmented Generation) ÌïôÏäµÏö© Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÎßåÎì§Ïñ¥Ï£ºÏÑ∏Ïöî.

Ï£ºÏñ¥ÏßÑ Î¨∏ÏÑúÏóêÏÑú {num_questions}Í∞úÏùò ÏßàÎ¨∏-ÎãµÎ≥Ä ÏåçÏùÑ ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.

[Î¨∏ÏÑú Ï†ïÎ≥¥]
Ï†úÎ™©: {doc_title}
Ïπ¥ÌÖåÍ≥†Î¶¨: {doc_category}

[Î¨∏ÏÑú ÎÇ¥Ïö©]
{doc_content}

[ÏÉùÏÑ± Í∑úÏπô]
1. ÏßàÎ¨∏ Ïú†Ìòï Î∂ÑÌè¨:
   - ÏÇ¨Ïã§ ÏßàÎ¨∏ (factual): ~40% - Ï£ºÏÜå, ÏãúÍ∞Ñ, Í∞ÄÍ≤© Îì± Î™ÖÌôïÌïú ÏÇ¨Ïã§
   - ÏÑ§Î™Ö ÏßàÎ¨∏ (descriptive): ~30% - "Ïñ¥Îñ§ Í≥≥Ïù∏Í∞ÄÏöî?", "Î¨¥ÏóáÏùÑ Ìï† Ïàò ÏûàÎÇòÏöî?"
   - Ï∂îÏ≤ú ÏßàÎ¨∏ (recommendation): ~15% - "Ï∂îÏ≤úÌï¥Ï£ºÏÑ∏Ïöî", "Ïñ¥ÎîîÍ∞Ä Ï¢ãÎÇòÏöî?"
   - ÎπÑÍµê ÏßàÎ¨∏ (comparison): ~10% - "Ï∞®Ïù¥Ï†êÏùÄ?", "Ïñ¥Îñ§ Í≤ÉÏù¥ ÎÇòÏùÄÍ∞ÄÏöî?"
   - Ï†ïÎ≥¥ ÏóÜÏùå (no_answer): ~5% - Î¨∏ÏÑúÏóê ÎãµÏù¥ ÏóÜÎäî ÏßàÎ¨∏

2. ÎãµÎ≥Ä ÏûëÏÑ± ÏõêÏπô:
   - Î∞òÎìúÏãú Ï£ºÏñ¥ÏßÑ Î¨∏ÏÑú ÎÇ¥Ïö©Îßå ÏÇ¨Ïö©
   - Î¨∏ÏÑúÏóê ÏóÜÎäî Ï†ïÎ≥¥Îäî Ï∂îÏ∏°ÌïòÏßÄ Îßê Í≤É
   - Ï†ïÎ≥¥Í∞Ä ÏóÜÏúºÎ©¥ "Ï†úÍ≥µÎêú Í¥ÄÍ¥ë Ï†ïÎ≥¥ÏóêÎäî [Ï£ºÏ†ú]Ïóê ÎåÄÌïú ÎÇ¥Ïö©Ïù¥ ÏóÜÏäµÎãàÎã§" ÌòïÏãùÏúºÎ°ú ÎãµÎ≥Ä

   **Ï§ëÏöî: Ï∂îÏ≤ú/ÏÑ§Î™Ö ÏßàÎ¨∏ ÎãµÎ≥Ä Ïãú**
   - Íµ¨Ï≤¥Ï†ÅÏù∏ ÎÇ¥Ïö©ÏùÑ Ìè¨Ìï®Ìï¥Ïïº Ìï® (ÏúÑÏπò, ÏΩîÏä§, Ïû•ÏÜåÎ™Ö, ÌäπÏßï Îì±)
   - Îã®ÏàúÌûà Ìï†Ïù∏Ïù¥ÎÇò Í∞ÄÍ≤© Ï†ïÎ≥¥Îßå Ïñ∏Í∏âÌïòÏßÄ Îßê Í≤É
   - Ïòà: "ÏãúÌã∞Ìà¨Ïñ¥ Ï∂îÏ≤úÌï¥Ï£ºÏÑ∏Ïöî" ‚Üí Ïñ¥Îñ§ ÏΩîÏä§Î•º Í∞ÄÎäîÏßÄ, Ïñ¥Îñ§ Ïû•ÏÜåÎ•º Î∞©Î¨∏ÌïòÎäîÏßÄ ÏÑ§Î™Ö
   - Ïòà: "ÎßõÏßë Ï∂îÏ≤úÌï¥Ï£ºÏÑ∏Ïöî" ‚Üí Ïñ¥Îñ§ ÏùåÏãùÏ†êÏù¥ ÏûàÎäîÏßÄ, Ïñ¥Îñ§ Î©îÎâ¥Í∞Ä ÏûàÎäîÏßÄ ÏÑ§Î™Ö

3. ÏßàÎ¨∏ ÌíàÏßà:
   - ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌïúÍµ≠Ïñ¥ Íµ¨Ïñ¥Ï≤¥
   - Ïã§Ï†ú Í¥ÄÍ¥ëÍ∞ùÏù¥ Î¨ºÏñ¥Î≥º Î≤ïÌïú ÏßàÎ¨∏
   - ÎÑàÎ¨¥ ÏâΩÍ±∞ÎÇò ÎÑàÎ¨¥ Ïñ¥Î†µÏßÄ ÏïäÍ≤å
   - Îã§ÏñëÌïú ÌëúÌòÑ ÏÇ¨Ïö© (Î∞òÎ≥µ ÌîºÌïòÍ∏∞)

[Ï∂úÎ†• ÌòïÏãù]
JSON Î∞∞Ïó¥Î°ú Î∞òÌôòÌïòÎêò, Í∞Å ÏöîÏÜåÎäî Îã§Ïùå ÌòïÏãù:
{{
  "question": "ÏßàÎ¨∏ ÎÇ¥Ïö©",
  "answer": "ÎãµÎ≥Ä ÎÇ¥Ïö©",
  "question_type": "factual|descriptive|recommendation|comparison|no_answer",
  "difficulty": "easy|medium|hard"
}}

ÏòàÏãú:
[
  {{
    "question": "ÏùòÎ¶ºÏßÄÎäî Ïñ¥ÎîîÏóê ÏûàÎÇòÏöî?",
    "answer": "ÏùòÎ¶ºÏßÄÎäî Ï†úÏ≤úÏãú ÏÜ°ÌïôÎ©¥ ÏùòÎ¶ºÎåÄÎ°ú 47Í∏∏ 7Ïóê ÏúÑÏπòÌï¥ ÏûàÏäµÎãàÎã§.",
    "question_type": "factual",
    "difficulty": "easy"
  }},
  {{
    "question": "ÏùòÎ¶ºÏßÄÎäî Ïñ¥Îñ§ Í≥≥Ïù∏Í∞ÄÏöî?",
    "answer": "ÏùòÎ¶ºÏßÄÎäî Ï†úÏ≤ú 10Í≤Ω Ï§ë ÌïòÎÇòÎ°ú, Í≥†ÎåÄ ÏàòÎ¶¨ÏãúÏÑ§Ïùò ÏõêÌòïÏùÑ Í∞ÑÏßÅÌïú Ïó≠ÏÇ¨Ï†Å Ïû•ÏÜåÏûÖÎãàÎã§. ÏÇºÌïúÏãúÎåÄÏóê Ï∂ïÏ°∞Îêú Í≤ÉÏúºÎ°ú ÏïåÎ†§Ï†∏ ÏûàÏúºÎ©∞ ÌïúÍµ≠Í¥ÄÍ¥ë 100ÏÑ†Ïóê ÏÑ†Ï†ïÎêú Ïù∏Í∏∞ Î™ÖÏÜåÏûÖÎãàÎã§.",
    "question_type": "descriptive",
    "difficulty": "easy"
  }},
  {{
    "question": "ÏùòÎ¶ºÏßÄ Ï£ºÏ∞®ÏöîÍ∏àÏùÄ ÏñºÎßàÏù∏Í∞ÄÏöî?",
    "answer": "Ï£ÑÏÜ°Ìï©ÎãàÎã§. Ï†úÍ≥µÎêú Í¥ÄÍ¥ë Ï†ïÎ≥¥ÏóêÎäî ÏùòÎ¶ºÏßÄ Ï£ºÏ∞®ÏöîÍ∏àÏóê ÎåÄÌïú ÎÇ¥Ïö©Ïù¥ ÏóÜÏäµÎãàÎã§.",
    "question_type": "no_answer",
    "difficulty": "medium"
  }}
]

Ïù¥Ï†ú ÏúÑ Î¨∏ÏÑúÎ•º Î∞îÌÉïÏúºÎ°ú {num_questions}Í∞úÏùò ÏßàÎ¨∏-ÎãµÎ≥Ä ÏåçÏùÑ JSON ÌòïÏãùÏúºÎ°ú ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.

Î∞òÎìúÏãú Îã§Ïùå ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌïòÏÑ∏Ïöî:
{{
  "qa_pairs": [
    {{
      "question": "ÏßàÎ¨∏ ÎÇ¥Ïö©",
      "answer": "ÎãµÎ≥Ä ÎÇ¥Ïö©",
      "question_type": "factual|descriptive|recommendation|comparison|no_answer",
      "difficulty": "easy|medium|hard"
    }}
  ]
}}"""

        return prompt

    def generate_qa_for_document(
        self,
        doc: Dict[str, Any],
        num_questions: int = 7
    ) -> List[Dict[str, Any]]:
        """Generate Q&A pairs for a single document using OpenAI GPT-4o."""

        prompt = self.create_qa_generation_prompt(
            doc_content=doc["content"],
            doc_title=doc["title"],
            doc_category=doc["category"],
            num_questions=num_questions
        )

        try:

            response = self.model.generate_content(prompt)

            # Parse JSON response
            response_data = json.loads(response.text)

            # Handle different possible JSON structures
            if "qa_pairs" in response_data:
                qa_list = response_data["qa_pairs"]
            elif "questions" in response_data:
                qa_list = response_data["questions"]
            elif isinstance(response_data, list):
                qa_list = response_data
            else:
                # Try to find the first list value
                for value in response_data.values():
                    if isinstance(value, list):
                        qa_list = value
                        break
                else:
                    qa_list = []

            # Add document metadata to each Q&A
            for qa in qa_list:
                qa["doc_id"] = doc["doc_id"]
                qa["doc_title"] = doc["title"]
                qa["doc_category"] = doc["category"]
                qa["doc_content"] = doc["content"]

            return qa_list

        except Exception as e:
            print(f"\n‚ùå Error generating Q&A for {doc['doc_id']}: {e}")
            return []

    def generate_all_qa(
        self,
        documents: List[Dict[str, Any]],
        target_total: int = 140
    ) -> List[Dict[str, Any]]:
        """Generate Q&A pairs for all documents."""

        print(f"\nü§ñ Generating Q&A pairs using Gemini 2.5pro API...")
        print(f"Target: {target_total} Q&A pairs from {len(documents)} documents\n")

        all_qa = []

        for doc in tqdm(documents, desc="Generating Q&A"):
            # Determine number of questions for this document
            num_questions = self.QUESTIONS_PER_CHUNK.get(
                doc["category"],
                self.QUESTIONS_PER_CHUNK["general"]
            )

            # Generate Q&A pairs
            qa_list = self.generate_qa_for_document(doc, num_questions)

            all_qa.extend(qa_list)

            # Rate limiting (respect Claude API limits)
            time.sleep(1)

            # Show progress
            print(f"  [{doc['doc_id']}] {doc['title']}: Generated {len(qa_list)} Q&A pairs")

        self.qa_pairs = all_qa
        return all_qa

    def save_to_jsonl(self, output_path: str):
        """Save Q&A pairs to JSONL file."""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            for qa in self.qa_pairs:
                f.write(json.dumps(qa, ensure_ascii=False) + '\n')

        print(f"\n‚úÖ Saved {len(self.qa_pairs)} Q&A pairs to {output_path}")

    def print_summary(self):
        """Print summary of generated Q&A pairs."""
        print(f"\nüìä Q&A Generation Summary:")
        print(f"Total Q&A pairs: {len(self.qa_pairs)}")

        # Question type distribution
        type_counts = {}
        for qa in self.qa_pairs:
            q_type = qa.get("question_type", "unknown")
            type_counts[q_type] = type_counts.get(q_type, 0) + 1

        print("\nüìÇ Question Type Distribution:")
        for q_type, count in sorted(type_counts.items(), key=lambda x: -x[1]):
            percentage = (count / len(self.qa_pairs)) * 100
            print(f"  - {q_type}: {count} ({percentage:.1f}%)")

        # Category distribution
        category_counts = {}
        for qa in self.qa_pairs:
            cat = qa.get("doc_category", "unknown")
            category_counts[cat] = category_counts.get(cat, 0) + 1

        print("\nüìÇ Category Distribution:")
        for cat, count in sorted(category_counts.items(), key=lambda x: -x[1]):
            print(f"  - {cat}: {count}")

        # Difficulty distribution
        difficulty_counts = {}
        for qa in self.qa_pairs:
            diff = qa.get("difficulty", "unknown")
            difficulty_counts[diff] = difficulty_counts.get(diff, 0) + 1

        print("\nüìÇ Difficulty Distribution:")
        for diff, count in sorted(difficulty_counts.items(), key=lambda x: -x[1]):
            percentage = (count / len(self.qa_pairs)) * 100
            print(f"  - {diff}: {count} ({percentage:.1f}%)")

        print("\nüìù Sample Q&A Pairs:")
        for i, qa in enumerate(self.qa_pairs[:3], 1):
            print(f"\n  Example {i}:")
            print(f"  Q: {qa['question']}")
            print(f"  A: {qa['answer'][:100]}...")
            print(f"  Type: {qa['question_type']} | Difficulty: {qa['difficulty']}")


def main():
    """Main execution function."""
    print("üöÄ Starting Q&A Generation with OpenAI gemini API...")

    # Check for API key
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("\n‚ùå Error: GOOGLE_API_KEY environment variable not set!")
        print("Please set it using: export GOOGLE_API_KEY='your-api-key'")
        return

    # Paths
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    documents_path = os.path.join(project_root, "data", "chunks", "documents.jsonl")
    output_path = os.path.join(project_root, "data", "chunks", "qa_pairs.jsonl")

    # Initialize generator
    generator = QAGenerator(api_key=api_key)

    # Load documents
    documents = generator.load_documents(documents_path)
    print(f"‚úÖ Loaded {len(documents)} documents")

    # Generate Q&A pairs
    qa_pairs = generator.generate_all_qa(documents, target_total=140)

    # Save to JSONL
    generator.save_to_jsonl(output_path)

    # Print summary
    generator.print_summary()

    print("\n‚úÖ Q&A generation completed!")


if __name__ == "__main__":
    main()
