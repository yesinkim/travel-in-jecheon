#!/usr/bin/env python3
"""
Model Comparison Script
ëª¨ë“  ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Usage:
    python test_models.py --models Qwen2.5-7B EXAONE-3.5-7.8B
    python test_models.py --all  # ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import List, Dict
import pandas as pd
from datetime import datetime
import argparse
import json
import os
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data" / "processed"
RESULTS_DIR = PROJECT_ROOT / "results"

# ë””ë ‰í† ë¦¬ ìƒì„±
DATA_DIR.mkdir(parents=True, exist_ok=True)
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

# ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
SIMPLE_QUESTIONS = [
    "ì œì²œì€ ì–´ëŠ ë„ì— ì†Œì†ë˜ì–´ ìˆë‚˜ìš”?",
    "K-popì„ ëŒ€í‘œí•˜ëŠ” ì•„í‹°ìŠ¤íŠ¸ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
    "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?",
    "ì„¸ì¢…ëŒ€ì™•ì€ ì–´ë–¤ ì—…ì ì„ ë‚¨ê²¼ë‚˜ìš”?",
    "ì‚¼êµ­ ì‹œëŒ€ì˜ ì„¸ ë‚˜ë¼ëŠ” ê°ê° ì–´ë–¤ íŠ¹ì§•ì´ ìˆì—ˆë‚˜ìš”?"
]

# ëª¨ë¸ ì„¤ì •
MODEL_CONFIGS = {
    "Kanana-2.1B": {
        "name": "kakaocorp/kanana-nano-2.1b-instruct",
        "system_prompt": "You are a helpful AI assistant developed by Kakao.",
        "dtype": torch.bfloat16,
        "trust_remote_code": True,
        "device_map": None,
        "use_eos_token_id": False
    },
    "EXAONE-3.5-7.8B": {
        "name": "LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct",
        "system_prompt": "You are EXAONE model from LG AI Research, a helpful assistant.",
        "dtype": torch.bfloat16,
        "trust_remote_code": True,
        "device_map": "auto",
        "use_eos_token_id": True
    },
    "Qwen2.5-7B": {
        "name": "Qwen/Qwen2.5-7B-Instruct",
        "system_prompt": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.",
        "dtype": "auto",
        "trust_remote_code": None,
        "device_map": "auto",
        "use_eos_token_id": False
    },
    "EXAONE-4.0-1.2B": {
        "name": "LGAI-EXAONE/EXAONE-4.0-1.2B",
        "system_prompt": None,
        "dtype": "bfloat16",
        "trust_remote_code": None,
        "device_map": "auto",
        "use_eos_token_id": False
    }
}


def test_model_on_questions(
    model_key: str,
    questions: List[str],
    max_new_tokens: int = 256,
    device: str = "cuda"
) -> Dict[str, List[str]]:
    """
    íŠ¹ì • ëª¨ë¸ë¡œ ì—¬ëŸ¬ ì§ˆë¬¸ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ê²°ê³¼ ë°˜í™˜

    Args:
        model_key: MODEL_CONFIGSì˜ í‚¤
        questions: í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        max_new_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤

    Returns:
        ì§ˆë¬¸ê³¼ ë‹µë³€ì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
    """
    config = MODEL_CONFIGS[model_key]
    print(f"\n{'='*60}")
    print(f"Testing {model_key} ({config['name']})")
    print(f"{'='*60}\n")

    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("Loading model...")

    # ëª¨ë¸ ë¡œë“œ íŒŒë¼ë¯¸í„° êµ¬ì„±
    model_kwargs = {
        "torch_dtype": config['dtype']
    }

    # trust_remote_code ì„¤ì • (Noneì´ ì•„ë‹ ë•Œë§Œ ì¶”ê°€)
    if config['trust_remote_code'] is not None:
        model_kwargs["trust_remote_code"] = config['trust_remote_code']

    # device_map ì„¤ì • (KananaëŠ” None, ë‚˜ë¨¸ì§€ëŠ” "auto")
    if config['device_map'] is not None:
        model_kwargs["device_map"] = config['device_map']

    model = AutoModelForCausalLM.from_pretrained(
        config['name'],
        **model_kwargs
    )

    # KananaëŠ” .to("cuda") ì‚¬ìš©
    if config['device_map'] is None:
        model = model.to(device)

    tokenizer = AutoTokenizer.from_pretrained(config['name'])
    model.eval()

    results = {
        "model": model_key,
        "questions": [],
        "answers": [],
        "timestamps": []
    }

    # ê° ì§ˆë¬¸ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
    for i, question in enumerate(questions, 1):
        print(f"\n[Question {i}/{len(questions)}]: {question}")

        # ë©”ì‹œì§€ êµ¬ì„±
        if config['system_prompt']:
            messages = [
                {"role": "system", "content": config['system_prompt']},
                {"role": "user", "content": question}
            ]
        else:
            messages = [{"role": "user", "content": question}]

        # ì…ë ¥ ì¤€ë¹„
        input_ids = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt"
        )

        # ì ì ˆí•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        if config['device_map'] is None:
            input_ids = input_ids.to(device)
        elif "EXAONE" in model_key:
            input_ids = input_ids.to(device)
        else:
            input_ids = input_ids.to(model.device)

        # ìƒì„± íŒŒë¼ë¯¸í„° êµ¬ì„±
        generate_kwargs = {
            "max_new_tokens": max_new_tokens,
            "do_sample": False
        }

        # EXAONE-3.5-7.8BëŠ” eos_token_id ì‚¬ìš©
        if config['use_eos_token_id']:
            generate_kwargs["eos_token_id"] = tokenizer.eos_token_id

        # ìƒì„±
        start_time = datetime.now()
        with torch.no_grad():
            output = model.generate(input_ids, **generate_kwargs)
        end_time = datetime.now()

        # ë””ì½”ë”©
        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

        # ê²°ê³¼ ì €ì¥
        results["questions"].append(question)
        results["answers"].append(generated_text)
        results["timestamps"].append((end_time - start_time).total_seconds())

        print(f"[Answer]: {generated_text[:200]}...")
        print(f"[Time]: {results['timestamps'][-1]:.2f}s")

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del model
    del tokenizer
    torch.cuda.empty_cache()

    print(f"\n{model_key} í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

    # ëª¨ë¸ë³„ ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (JSON)
    save_intermediate_result(results, model_key)

    return results


def save_intermediate_result(result: Dict, model_key: str):
    """ëª¨ë¸ë³„ ì¤‘ê°„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = DATA_DIR / f"{model_key}_results_{timestamp}.json"

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ ì¤‘ê°„ ê²°ê³¼ ì €ì¥: {filename}")


def create_comparison_df(results_list: List[Dict]) -> pd.DataFrame:
    """ê²°ê³¼ë¥¼ ë¹„êµ ê°€ëŠ¥í•œ DataFrameìœ¼ë¡œ ë³€í™˜"""
    comparison_data = []

    for result in results_list:
        model_name = result['model']
        for i, (question, answer, time) in enumerate(zip(
            result['questions'],
            result['answers'],
            result['timestamps']
        )):
            comparison_data.append({
                'Question #': i + 1,
                'Question': question,
                'Model': model_name,
                'Answer': answer,
                'Time (s)': round(time, 2)
            })

    return pd.DataFrame(comparison_data)


def save_comparison_results(df: pd.DataFrame, timestamp: str):
    """ë¹„êµ ê²°ê³¼ë¥¼ ì—¬ëŸ¬ í˜•ì‹ìœ¼ë¡œ ì €ì¥"""

    # 1. CSV ì €ì¥ (ì „ì²´ ê²°ê³¼)
    csv_path = DATA_DIR / f"model_comparison_results_{timestamp}.csv"
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ CSV ì €ì¥: {csv_path}")

    # 2. Pivot í…Œì´ë¸” ì €ì¥ (ëª¨ë¸ë³„ ë‹µë³€ ë¹„êµ)
    pivot_df = df.pivot_table(
        index=['Question #', 'Question'],
        columns='Model',
        values='Answer',
        aggfunc='first'
    ).reset_index()

    pivot_path = DATA_DIR / f"model_comparison_pivot_{timestamp}.csv"
    pivot_df.to_csv(pivot_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ Pivot í…Œì´ë¸” ì €ì¥: {pivot_path}")

    # 3. í‰ê·  ì‘ë‹µ ì‹œê°„ ì €ì¥
    avg_times = df.groupby('Model')['Time (s)'].mean().sort_values()

    stats_df = pd.DataFrame({
        'Model': avg_times.index,
        'Average Time (s)': avg_times.values,
        'Total Questions': [len(df[df['Model'] == model]) for model in avg_times.index]
    })

    stats_path = DATA_DIR / f"model_statistics_{timestamp}.csv"
    stats_df.to_csv(stats_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ í†µê³„ ì €ì¥: {stats_path}")

    return pivot_df, stats_df


def print_comparison_summary(df: pd.DataFrame):
    """ë¹„êµ ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥"""
    print("\n" + "="*80)
    print("ëª¨ë¸ë³„ ë‹µë³€ ë¹„êµ")
    print("="*80)

    for question_num in df['Question #'].unique():
        question_data = df[df['Question #'] == question_num]
        question_text = question_data['Question'].iloc[0]

        print(f"\n\nğŸ“Œ ì§ˆë¬¸ {question_num}: {question_text}")
        print("-" * 80)

        for _, row in question_data.iterrows():
            print(f"\nğŸ¤– {row['Model']} ({row['Time (s)']}ì´ˆ):")
            print(f"{row['Answer']}\n")

    # í‰ê·  ì‘ë‹µ ì‹œê°„ ë¹„êµ
    print("\n" + "="*80)
    print("ëª¨ë¸ë³„ í‰ê·  ì‘ë‹µ ì‹œê°„")
    print("="*80)
    avg_times = df.groupby('Model')['Time (s)'].mean().sort_values()
    print(avg_times)


def create_visualization(df: pd.DataFrame, timestamp: str):
    """ê²°ê³¼ ì‹œê°í™”"""

    # í•œê¸€ í°íŠ¸ ì„¤ì •
    plt.rcParams['font.family'] = 'AppleGothic'
    plt.rcParams['axes.unicode_minus'] = False

    # ëª¨ë¸ë³„ í‰ê·  ì‘ë‹µ ì‹œê°„ ë§‰ëŒ€ ê·¸ë˜í”„
    fig, ax = plt.subplots(figsize=(10, 6))

    avg_times = df.groupby('Model')['Time (s)'].mean().sort_values()
    avg_times.plot(kind='barh', ax=ax, color='steelblue')

    ax.set_xlabel('í‰ê·  ì‘ë‹µ ì‹œê°„ (ì´ˆ)', fontsize=12)
    ax.set_ylabel('ëª¨ë¸', fontsize=12)
    ax.set_title('ëª¨ë¸ë³„ í‰ê·  ì‘ë‹µ ì‹œê°„ ë¹„êµ', fontsize=14, fontweight='bold')
    ax.grid(axis='x', alpha=0.3)

    plt.tight_layout()

    # ì €ì¥
    plot_path = RESULTS_DIR / f'model_response_time_comparison_{timestamp}.png'
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š ê·¸ë˜í”„ ì €ì¥: {plot_path}")

    plt.close()


def main():
    parser = argparse.ArgumentParser(description='ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument(
        '--models',
        nargs='+',
        choices=list(MODEL_CONFIGS.keys()),
        help='í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ì„ íƒ'
    )
    parser.add_argument(
        '--all',
        action='store_true',
        help='ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸'
    )
    parser.add_argument(
        '--max-tokens',
        type=int,
        default=256,
        help='ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 256)'
    )
    parser.add_argument(
        '--device',
        type=str,
        default='cuda',
        choices=['cuda', 'cpu'],
        help='ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (ê¸°ë³¸: cuda)'
    )

    args = parser.parse_args()

    # CUDA ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        args.device = 'cpu'

    # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ê²°ì •
    if args.all:
        models_to_test = list(MODEL_CONFIGS.keys())
    elif args.models:
        models_to_test = args.models
    else:
        # ê¸°ë³¸ê°’: Qwen2.5-7Bë§Œ í…ŒìŠ¤íŠ¸
        models_to_test = ["Qwen2.5-7B"]
        print("âš ï¸  ëª¨ë¸ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Qwen2.5-7Bë§Œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
        print("   ë‹¤ë¥¸ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ --models ë˜ëŠ” --all ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n")

    print("="*60)
    print("ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*60)
    print(f"í…ŒìŠ¤íŠ¸í•  ëª¨ë¸: {', '.join(models_to_test)}")
    print(f"ì§ˆë¬¸ ê°œìˆ˜: {len(SIMPLE_QUESTIONS)}")
    print(f"ìµœëŒ€ í† í°: {args.max_tokens}")
    print(f"ë””ë°”ì´ìŠ¤: {args.device}")
    print("="*60)

    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (ëª¨ë“  ê²°ê³¼ì— ë™ì¼í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš©)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸
    all_results = []

    for model_key in models_to_test:
        try:
            result = test_model_on_questions(
                model_key=model_key,
                questions=SIMPLE_QUESTIONS,
                max_new_tokens=args.max_tokens,
                device=args.device
            )
            all_results.append(result)
        except Exception as e:
            print(f"\nâŒ Error testing {model_key}: {str(e)}")
            import traceback
            traceback.print_exc()
            continue

    if not all_results:
        print("\nâŒ í…ŒìŠ¤íŠ¸ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nâœ… ì´ {len(all_results)}ê°œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

    # DataFrame ìƒì„±
    df_comparison = create_comparison_df(all_results)

    # ê²°ê³¼ ì¶œë ¥
    print_comparison_summary(df_comparison)

    # ê²°ê³¼ ì €ì¥
    print("\n" + "="*60)
    print("ê²°ê³¼ ì €ì¥ ì¤‘...")
    print("="*60)

    pivot_df, stats_df = save_comparison_results(df_comparison, timestamp)

    # ì‹œê°í™” ìƒì„±
    try:
        create_visualization(df_comparison, timestamp)
    except Exception as e:
        print(f"âš ï¸  ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {str(e)}")

    # ìµœì¢… í†µê³„ ì¶œë ¥
    print("\n" + "="*60)
    print("ìµœì¢… í†µê³„")
    print("="*60)
    print(stats_df.to_string(index=False))

    print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print(f"\nê²°ê³¼ íŒŒì¼ ìœ„ì¹˜:")
    print(f"  - ë°ì´í„°: {DATA_DIR}")
    print(f"  - ê·¸ë˜í”„: {RESULTS_DIR}")


if __name__ == "__main__":
    main()
